{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GWASTutorial","text":"<p>This Github page aims to provide a hands-on tutorial on common analysis in Complex Trait Genomics. This tutorial is designed for the course <code>Fundamental Exercise II</code> provided by The Laboratory of Complex Trait Genomics at the University of Tokyo. For more information, please see About.</p> <p>This tutorial covers the minimum skills and knowledge required to perform a typical genome-wide association study (GWAS). The contents are categorized into the following groups. Additionally, for absolute beginners, we also prepared a section on command lines in Linux.</p> <p>If you have any questions or suggestions, please feel free to let us know in the Issue section of this repository.</p> <p></p>"},{"location":"#contents","title":"Contents","text":""},{"location":"#command-lines","title":"Command lines","text":"<ul> <li>Linux command line basics (optional) : For those who haven't used the command line, we will first introduce the basics of the Linux system and commonly used commands.</li> </ul>"},{"location":"#pre-gwas","title":"Pre-GWAS","text":"<ul> <li>Data formats : Before any analysis, the first thing is always to get familiar with your data. In this section, we will introduce some basic formats used to store sequence, genotype and dosage data.</li> <li>Data QC : Usually the raw genotype data is \"dirty\". This means that there are usually errors, invalid or missing values. In this section, we will learn how to perform quality control for the raw genotype data using PLINK. </li> <li>Principal component analysis (PCA) : In this section, we will cover how to perform Principal Component Analysis (PCA) to analyze the population structure.  </li> </ul>"},{"location":"#gwas","title":"GWAS","text":"<ul> <li>Association tests: After QC, we will perform the very first association tests for a simulated binary trait (case-control trait) with a logistic regression model using PLINK.</li> <li>Visualization: To visualize the summary statistics generated from association tests, we will use a python package called gwaslab to create Manhattan plots, Quantitle-Quantile plots and Regional plots.</li> </ul>"},{"location":"#post-gwas","title":"Post-GWAS","text":"<p>In these sections, we will briefly introduce the Post-GWAS analyses, which will dig deeper into the GWAS summary statistics.  </p> <ul> <li>Variant Annotation by ANNOVAR/VEP</li> <li>Heritability Concepts</li> <li>SNP-Heritability estimation by GCTA-GREML</li> <li>LD score regression (univariate, cross-trait and partitioned) by LDSC</li> <li>Gene / Gene-set analysis by MAGMA</li> <li>Fine-mapping by SUSIE</li> <li>Polygenic risk scores</li> </ul>"},{"location":"#others","title":"Others","text":"<ul> <li>Recommended reading</li> </ul>"},{"location":"01_Dataset/","title":"Sample Dataset","text":"<p>504 EAS individuals from 1000 Genome Project Phase 3 version 5</p> <ul> <li>Url: http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/</li> </ul>"},{"location":"01_Dataset/#genotype-data-processing","title":"Genotype Data Processing","text":"<ul> <li>Selected only autosomal variants</li> <li>Split multi-allelic variants</li> <li>Variants were normalized</li> <li>Remove duplicated variants</li> <li>Selected only SNP (ATCG)</li> <li>Selected only SNPs with MAF&gt;0.05</li> <li>Randomly selected 20% of SNPs (<code>plink --thin 0.2</code>)</li> <li>Converted to plink bed format and merged to a single file  </li> </ul>"},{"location":"01_Dataset/#download","title":"Download","text":"<p>Simply run <code>download_sampledata.sh</code> and the dataset will be downloaded and decompressed.</p> <p>Sample dataset is currently hosted on Dropbox which may not be accessible for users in certain regions.</p> <pre><code>./download_sampledata.sh\n</code></pre> <p>or you can manually download it from this link.</p> <p>And you will get the following files: <pre><code>1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bed\n1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim\n1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam\n</code></pre></p>"},{"location":"01_Dataset/#phenotype-simulation","title":"Phenotype Simulation","text":"<p>Phenotypes were simply simulated using GCTA with the 1KG EAS dataset (without thinning). <pre><code>gcta64  \\\n--bfile 1KG.EAS.auto.snp.norm.nodup.split.maf005 \\ \n--simu-cc 200 304 \\\n--simu-causal-loci causal_10.snplist  \\\n--simu-hsq 0.8  \\\n--simu-k 0.4 \\\n--simu-rep 1 \\\n--out 1kgeas_binary\n</code></pre></p> <pre><code>$ cat causal_10.snplist\n3:176520196:C:T 3\n1:217437563:C:T 3\n9:36591968:T:G 3\n6:29898352:T:C 3\n2:55620927:G:A 3\n13:92117183:G:A 3\n14:78760515:T:C 3\n11:102442005:T:G 3\n11:56317673:T:A 3\n7:139979401:G:C 3\n</code></pre>"},{"location":"01_Dataset/#reference","title":"Reference","text":"<ul> <li>1000 Genomes Project Consortium. (2015). A global reference for human genetic variation. Nature, 526(7571), 68.</li> <li>Yang, J., Lee, S. H., Goddard, M. E., &amp; Visscher, P. M. (2011). GCTA: a tool for genome-wide complex trait analysis. The American Journal of Human Genetics, 88(1), 76-82.</li> </ul>"},{"location":"02_Linux_basics/","title":"Introduction","text":"<p>This section is intended to provide a minimum introduction of the command line in Linux system for handling genomic data.  (If you are alreay familiar with Linux commands, it is completely ok to skip this section.)</p> <p>If you are a beginner with no background in programming, it would be helpful if you could learn some basic commands first before any analysis.  In this section, we will introduce the most basic commands which enable you to handle genomic files in the terminal using command lines in a linux system. </p> <p>For Mac users</p> <p>This tutorial will probably work with no problems. Just simply open your terminal and follow the tutorial.</p> <p>For Windows users</p> <p>You can simply insall WSL to get a linux environment. Please check here for how to install WSL.</p>"},{"location":"02_Linux_basics/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Linux introduction<ul> <li>Linux kernel and distributions</li> <li>GUI and CUI</li> <li><code>man</code></li> </ul> </li> <li>Overview and checking the manual pages</li> <li>Handling directories<ul> <li>Absolute path and relative path</li> </ul> </li> <li>Manipulating files</li> <li>Symbolic link</li> <li>Archiving and Compression</li> <li>Checking files</li> <li>Editing files in terminal</li> <li>Permissions</li> <li>Other useful commands</li> <li>Bash scripts</li> <li>Advanced text editing</li> <li>Git and Github</li> <li>Downloading</li> </ul>"},{"location":"02_Linux_basics/#linux-system-introduction","title":"Linux System Introduction","text":""},{"location":"02_Linux_basics/#what-is-linux","title":"What is Linux?","text":"Term Description Linux refers to a family of open-source Unix-like operating systems based on the Linux kernel. Linux kernel a free and open-source Unix-like operating system kernel, which controls the software and hardware of the computer. Linux distributions refer to\u00a0operating systems\u00a0made from a software collection that is based upon the\u00a0Linux kernel. <p>Main functions of the Linux kernel</p> <ul> <li>System memory management </li> <li>Software process management</li> <li>Hardware device drivers  </li> <li>File system management</li> </ul> <p>Some of the most common linux distributions</p> <p></p> <ul> <li>Unbuntu : https://ubuntu.com/</li> <li>CentOS : https://www.centos.org/</li> <li>Fedora : https://getfedora.org/</li> <li>Linux Mint : https://linuxmint.com/</li> </ul> <p>Linux and Linus</p> <p>Linux is named after Linus Benedict Torvalds, who is a legendary Finnish software engineer who lead the development of the Linux kernel. He also developped the amazing version control software - Git.</p> <p>Reference: https://en.wikipedia.org/wiki/Linux</p>"},{"location":"02_Linux_basics/#how-do-we-interact-with-computers","title":"How do we interact with computers?","text":"<ul> <li>Graphical User Interface (GUI): allows users to interact with computers through graphical icons </li> <li>Character User Interface (CUI): allows users to interact with computers through command lines</li> </ul> <p>GUI and CUI</p> <p></p> <p>Shell</p> <ul> <li>A Shell provided the actual interface for you to interact with the Linux system. When you type commands in a shell, it will collect and execute the commands. </li> <li><code>$</code> is the prompt for bash shell, which indicate that you can type commands after the <code>$</code> sign.</li> <li>Different shells might use other signs for the prompt. For example, the defaul zsh in Mac uses <code>%</code> , and C shell uses <code>&gt;</code> as the prompt sign.</li> <li>There are multiple available shells which differ in their features. For a typical linux system, the default shell is <code>bash</code>.  </li> </ul>"},{"location":"02_Linux_basics/#a-general-comparison-between-cui-and-gui","title":"A general comparison between CUI and GUI","text":"GUI CUI Interaction Graphics Command line Precision LOW HIGH Speed LOW HIGH Memory required HIGH LOW Ease of operation Easier DIFFICULT Flexibility MORE flexible LESS flexible <p>Tip</p> <p>The reason why we want to use CUI for large-scale data analysis is that CUI is better in term of precision, memory usage and processing speed.</p>"},{"location":"02_Linux_basics/#overview-of-the-basic-commands-in-linux","title":"Overview of the basic commands in Linux","text":"<p>Unlike clicking and dragging files in Windows or MacOS, in Linux, we usually handle files by typing commands in the terminal.</p> <p></p> <p>Here is a list of the basic commands we are going to cover in this brief tutorial:</p> <p>Basic Linux commands</p> Function group Commands Description Directories <code>pwd</code>, <code>ls</code>, <code>mkdir</code>, <code>rmdir</code> Commands for checking, creating and removing directories Files <code>touch</code>,<code>cp</code>,<code>mv</code>,<code>rm</code> Commands for creating, copying, moving and removing files Checking files <code>cat</code>,<code>zcat</code>,<code>head</code>,<code>tail</code>,<code>less</code>,<code>more</code>,<code>wc</code> Commands for inspecting files Archiving and compression <code>tar</code>,<code>gzip</code>,<code>gunzip</code>,<code>zip</code>,<code>unzip</code> Commands for Archiving and Compressing files Manipulating text <code>sort</code>,<code>uniq</code>,<code>cut</code>,<code>join</code>,<code>tr</code> Commands for manipulating text files Modifying permission <code>chmod</code>,<code>chown</code>, <code>chgrp</code> Commands for changing the permissions of files and directories Links <code>ln</code> Commands for creating symbolic and hard links Pipe, redirect and others pipe, <code>&gt;</code>,<code>&gt;&gt;</code>,<code>*</code>,<code>.</code>,<code>..</code> A group of miscellaneous commands Advance text editing <code>awk</code>, <code>sed</code> Commands for more complicated text manipulation and editing"},{"location":"02_Linux_basics/#how-to-check-the-usage-of-a-command-using-man","title":"How to check the usage of a command using <code>man</code>:","text":"<p>The first command we might want to learn is <code>man</code>, which shows the manual for a certain command. When you forget how to use a command, you can always use <code>man</code> to check.</p> <p><code>man</code> : Check the manual of a command (e.g., <code>man chmod</code>) or <code>--help</code> option (e.g., <code>chmod --help</code>)</p> <p>For example, we want to check the usage of <code>pwd</code>:</p> <p>Use <code>man</code> to get the manual for commands</p> <p><pre><code>$ man pwd\n</code></pre> Then you will see the manual of <code>pwd</code> in your terminal. <pre><code>PWD(1)                                              User     Commands                                              PWD(1)\n\nNAME\n       pwd - print name of current/working directory\n\nSYNOPSIS\n       pwd [OPTION]...\n\nDESCRIPTION\n       Print the full filename of the current working directory.\n....\n</code></pre></p> <p>Explain shell</p> <p>Or you can use this wonderful website to get explanations for your commands.</p> <p>URL : https://explainshell.com/</p> <p></p>"},{"location":"02_Linux_basics/#commands","title":"Commands","text":""},{"location":"02_Linux_basics/#directories","title":"Directories","text":"<p>The first set of commands are: <code>pwd</code> , <code>cd</code> , <code>ls</code>, <code>mkdir</code> and <code>rmdir</code>, which are related to directories (like the folders in a Windows system).</p>"},{"location":"02_Linux_basics/#pwd","title":"<code>pwd</code>","text":"<p><code>pwd</code> : Print working directory, which means print the path of the current directory (working directory)</p> <p>Use <code>pwd</code> to print the current directory you are in</p> <pre><code>$ pwd\n/home/he/work/GWASTutorial/02_Linux_basics\n</code></pre> <p>This command prints the absolute path.</p> <p>An example of Linux file system and file paths</p> <p></p> Type Description Example Absolute path path starting from root (the orange path) <code>/home/User3/GWASTutorial/02_Linux_basics/README.md</code> Relative path ath starting from the current directory (the blue path) <code>./GWASTutorial/02_Linux_basics/README.md</code> <p>Tip: use <code>readlink</code> to obtain the absolute path of a file</p> <p>To get the absolute path of a file, you can use <code>readlink -f [filename]</code>.</p> <pre><code>$ readlink -f README.md \n/home/he/work/GWASTutorial/02_Linux_basics/README.md\n</code></pre>"},{"location":"02_Linux_basics/#cd","title":"<code>cd</code>","text":"<p><code>cd</code>: Change the current working directory </p> <p>Use <code>cd</code> to change directory to <code>02_Linux_basics</code> and then print the current directory</p> <pre><code>$ cd 02_Linux_basics\n$ pwd\n/home/he/work/GWASTutorial/02_Linux_basics\n</code></pre>"},{"location":"02_Linux_basics/#ls","title":"<code>ls</code> :","text":"<p><code>ls</code> : List the files in the directory</p> <p>Some frequently used options for <code>ls</code> :</p> <ul> <li><code>-l</code>: in a list-like format</li> <li><code>-h</code>: convert file size into a human readable format (KB,MB,GB...)</li> <li><code>-a</code>: list all files (including hidden files, namly those files a period at the beginning of the filename)</li> </ul> <p>Simply list the files and directories in the current directory</p> <pre><code>$ ls\nREADME.md  sumstats.txt\n</code></pre> <p>List the files and directories with options <code>-lha</code></p> <pre><code>$ ls -lha\ndrwxr-xr-x   4 he  staff   128B Dec 23 14:07 .\ndrwxr-xr-x  17 he  staff   544B Dec 23 12:13 ..\n-rw-r--r--   1 he  staff     0B Oct 17 11:24 README.md\n-rw-r--r--   1 he  staff    31M Dec 23 14:07 sumstats.txt\n</code></pre> <p>Tip: use <code>tree</code> to visualize the structure of a directory</p> <p>You can use <code>tree</code> command to visualize the structure of a directory.</p> <pre><code>$ tree ./02_Linux_basics/\n./02_Linux_basics/\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 sumstats.txt\n\n0 directories, 2 files\n</code></pre>"},{"location":"02_Linux_basics/#mkdir-rmdir","title":"<code>mkdir</code> &amp; <code>rmdir</code> :","text":"<ul> <li><code>mkdir</code> : Create a new empty directory</li> <li><code>rmdir</code>: Delete an empty directory</li> </ul> <p>Make a directory and delete it</p> <pre><code>$ mkdir new_directory\n$ ls\nnew_directory  README.md  sumstats.txt\n$ rmdir new_directory/\n$ ls\nREADME.md  sumstats.txt\n</code></pre>"},{"location":"02_Linux_basics/#manipulating-files","title":"Manipulating files","text":"<p>This set of commands includes: <code>touch</code>, <code>mv</code> , <code>rm</code> and <code>cp</code></p>"},{"location":"02_Linux_basics/#touch","title":"<code>touch</code>","text":"<p><code>touch</code> command is used to create a new empty file.</p> <p>Create an empty text file called <code>newfile.txt</code> in this directory</p> <pre><code>$ ls -l\ntotal 64048\n-rw-r--r--  1 he  staff         0 Oct 17 11:24 README.md\n-rw-r--r--  1 he  staff  32790417 Dec 23 14:07 sumstats.txt\n\ntouch newfile.txt\n\n$ touch newfile.txt\n$ ls -l\ntotal 64048\n-rw-r--r--  1 he  staff         0 Oct 17 11:24 README.md\n-rw-r--r--  1 he  staff         0 Dec 23 14:14 newfile.txt\n-rw-r--r--  1 he  staff  32790417 Dec 23 14:07 sumstats.txt\n</code></pre>"},{"location":"02_Linux_basics/#mv","title":"<code>mv</code>","text":"<p><code>mv</code> has two functions:</p> <ul> <li>(1) move files to another paths</li> <li>(2) rename files</li> </ul> <p>The following command will create a new directoru called <code>new_directory</code>, and move <code>sumstats.txt</code> into that directory. Just like draggig a file in to a folder in window system.</p> <p>Move a file to a different directory</p> <pre><code># make a new directory\n$ mkdir new_directory\n\n#move sumstats to the new directory\n$ mv sumstats.txt new_directory/\n\n# list the item in new_directory\n$ ls new_directory/\nsumstats.txt\n</code></pre> <p>Now, let's move it back to the current directory and rename it to <code>sumstats_new.txt</code>.</p> <p>Rename a file using <code>mv</code></p> <p><pre><code>$ mv ./new_directory/sumstats.txt ./\n</code></pre> Note: <code>./</code> means the current directory You can also use <code>mv</code> to rename a file: <pre><code>#rename\n$mv sumstats.txt sumstats_new.txt </code></pre></p>"},{"location":"02_Linux_basics/#rm","title":"<code>rm</code>","text":"<p><code>rm</code> : Remove files or diretories</p> <p>Remove a file and a directory</p> <pre><code># remove a file\n$rm file\n\n#remove files in a directory (recursive mode)\n$rm -r directory/\n</code></pre> <p>There is no trash can in Linux command-line interface</p> <p>If you delete a file with <code>rm</code> , it will be very difficult to restore it. Please be careful wehn using <code>rm</code>. </p>"},{"location":"02_Linux_basics/#cp","title":"<code>cp</code>","text":"<p><code>cp</code> command is used to copy files or diretories.</p> <p>Copy a file and a directory</p> <pre><code>#cp files\n$cp file1 file2\n\n# copy directory\n$cp -r directory1/ directory2/\n</code></pre>"},{"location":"02_Linux_basics/#links","title":"Links","text":"<p>Symbolic link is like a shortcut on window system, which is a special type of file that points to another file.</p> <p>It is very useful when you want to organize your tool box or working space.</p> <p>You can use <code>ln -s pathA pathB</code> to create such a link. </p> <p>Create a symbolic link for plink</p> <p>Let`s create a symbolic link for plink first. <pre><code># /home/he/tools/plink/plink is the orinial file\n# /home/he/tools/bin is the path for the symbolic link \nln -s /home/he/tools/plink/plink /home/he/tools/bin\n</code></pre></p> <p>And then check the link.</p> <pre><code>cd /home/he/tools/bin\nls -lha\nlrwxr-xr-x  1 he  staff    27B Aug 30 11:30 plink -&gt; /home/he/tools/plink/plink\n</code></pre>"},{"location":"02_Linux_basics/#archiving-and-compression","title":"Archiving and Compression","text":"<p>Results for millions of variants are usually very large, sometimes &gt;10GB, or consists of multiple files. </p> <p>To save space and make it easier to transfer, we need to archive and compress these files.</p> <ul> <li>Archiving: combine multiple files in a single file.</li> <li>Compression: make the file size smaller without losing any infomation by convertying the file to binary forms.</li> </ul> <p>Archiving and Compression</p> <p></p> <p>Commoly used commands for archiving and compression:</p> Extensions Create Extract Functions <code>file.gz</code> <code>gzip</code> <code>gunzip</code> compress <code>files.tar</code> <code>tar -cvf</code> <code>tar -xvf</code> archive <code>files.tar.gz</code> or <code>files.tgz</code> <code>tar -czvf</code> <code>tar -xvzf</code> archive and compress <code>file.zip</code> <code>zip</code> <code>unzip</code> archive and compress <p>Compress and decompress a file using <code>gzip</code> and <code>gunzip</code></p> <pre><code>$ ls -lh\n-rw-r--r--  1 he  staff    31M Dec 23 14:07 sumstats.txt\n\n$ gzip sumstats.txt\n$ ls -lh\n-rw-r--r--  1 he  staff   9.9M Dec 23 14:07 sumstats.txt.gz\n\n$ gunzip sumstats.txt.gz\n$ ls -lh\n-rw-r--r--   1 he  staff    31M Dec 23 14:07 sumstats.txt\n</code></pre>"},{"location":"02_Linux_basics/#read-and-check-files","title":"Read and check files","text":"<p>We have a group of handy commands to check part of or the entire file, including <code>cat</code>, <code>zcat</code>, <code>less</code>, <code>head</code>, <code>tail</code>, <code>wc</code></p>"},{"location":"02_Linux_basics/#cat","title":"<code>cat</code>","text":"<p><code>cat</code> command can print the contents of files or concatenate the files.</p> <p>Create and then <code>cat</code> the file <code>a_text_file.txt</code> </p> <pre><code>$ ls -lha &gt; a_text_file.txt\n$ cat a_text_file.txt \ntotal 32M\ndrwxr-x---  2 he staff 4.0K Apr  2 00:37 .\ndrwxr-x--- 29 he staff 4.0K Apr  1 22:20 ..\n-rw-r-----  1 he staff    0 Apr  2 00:37 a_text_file.txt\n-rw-r-----  1 he staff 5.0K Apr  1 22:20 README.md\n-rw-r-----  1 he staff  32M Mar 30 18:17 sumstats.txt\n</code></pre> <p>Warning</p> <p>Be careful not to <code>cat</code> a text file with a huge number of lines. You can try to <code>cat sumstats.txt</code> and see what happends.</p> <p>By the way, <code>&gt; a_text_file.txt</code> here means redirect the output to file <code>a_text_file.txt</code>.</p>"},{"location":"02_Linux_basics/#zcat","title":"<code>zcat</code>","text":"<p><code>zcat</code> is similar to <code>cat</code>, but can only applied to compressed files.</p> <p><code>cat</code> and <code>zcat</code> a gzipped text file</p> <pre><code>$ gzip a_text_file.txt \n$ cat a_text_file.txt.gz                                                         TGba_text_file.    txt\u044f\n@\u0231\u00bbO\ud8ac\udc19v\u0602\ud85e\udca9\u00bc\ud9c3\udce0bq}\udb06\udca4\\\ueee0\u00a4n\u0662\u00aa\uda40\udc2cn\u00bb\u06a1\u01ed\n                          w5J_\u00bd\ud88d\ude27P\u07c9=\u00ffK\n(\u05a3\u0530\u00a7\u04a4\u0176a\u0786                              \u00acM\u00adR\udbb5\udc8am\u00b3\u00fee\u00b8\u00a4\u00bc\u05cdSd\ufff1\u07f2\ub4e4\u00aa\u00adv\n       \u5a41                                                                                                               resize: unknown character, exiting.\n\n$ zcat a_text_file.txt.gz \ntotal 32M\ndrwxr-x---  2 he staff 4.0K Apr  2 00:37 .\ndrwxr-x--- 29 he staff 4.0K Apr  1 22:20 ..\n-rw-r-----  1 he staff    0 Apr  2 00:37 a_text_file.txt\n-rw-r-----  1 he staff 5.0K Apr  1 22:20 README.md\n-rw-r-----  1 he staff  32M Mar 30 18:17 sumstats.txt\n</code></pre> <p>gzcat</p> <p>Use <code>gzcat</code> instead of <code>zcat</code> if your device is running MacOS. </p>"},{"location":"02_Linux_basics/#head","title":"<code>head</code>","text":"<p><code>head</code>: Print the first 10 lines.</p> <p><code>-n</code>: option to change the number of lines.</p> <p>Check the first 10 lines and only the first line of the file <code>sumstats.txt</code></p> <pre><code>$ head sumstats.txt \nCHROM   POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   319 17  2   1   1   ADD 10000   1.04326 0.0495816   0.854176    0.393008    .\n1   319 22  1   2   2   ADD 10000   1.03347 0.0493972   0.666451    0.505123    .\n1   418 23  1   2   2   ADD 10000   1.02668 0.0498185   0.528492    0.597158    .\n1   537 30  1   2   2   ADD 10000   1.01341 0.0498496   0.267238    0.789286    .\n1   546 31  2   1   1   ADD 10000   1.02051 0.0336786   0.60284 0.546615    .\n1   575 33  2   1   1   ADD 10000   1.09795 0.0818305   1.14199 0.25346 .\n1   752 44  2   1   1   ADD 10000   1.02038 0.0494069   0.408395    0.682984    .\n1   913 50  2   1   1   ADD 10000   1.07852 0.0493585   1.53144 0.12566 .\n1   1356    77  2   1   1   ADD 10000   0.947521    0.0339805   -1.5864 0.112649    .\n\n$ head -n 1 sumstats.txt \nCHROM   POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n</code></pre>"},{"location":"02_Linux_basics/#tail","title":"<code>tail</code>","text":"<p>Similar to <code>head</code>, you can use <code>tail</code> ro check the last 10 lines. <code>-n</code> works in the same way.</p> <p>Check the last 10 lines of the file <code>sumstats.txt</code></p> <pre><code>$ tail sumstats.txt 22  99996057    9959945 2   1   1   ADD 10000   1.03234 0.0335547   0.948413    0.342919.\n22  99996465    9959971 2   1   1   ADD 10000   1.04755 0.0337187   1.37769 0.1683  .\n22  99997041    9960013 2   1   1   ADD 10000   1.01942 0.0937548   0.205195    0.837419.\n22  99997608    9960051 2   1   1   ADD 10000   0.969928    0.0397711   -0.767722   0.    442652    .\n22  99997629    9960055 2   1   1   ADD 10000   0.986949    0.0395305   -0.332315   0.    739652    .\n22  99997742    9960061 2   1   1   ADD 10000   0.990829    0.0396614   -0.232298   0.    816307    .\n22  99998121    9960086 2   1   1   ADD 10000   1.04448 0.0335879   1.29555 0.19513 .\n22  99998455    9960106 2   1   1   ADD 10000   0.880953    0.152754    -0.829771   0.    406668    .\n22  99999208    9960146 2   1   1   ADD 10000   0.944604    0.065187    -0.874248   0.    381983    .\n22  99999382    9960164 2   1   1   ADD 10000   0.970509    0.033978    -0.881014   0.37831 .\n</code></pre>"},{"location":"02_Linux_basics/#wc","title":"<code>wc</code>","text":"<p><code>wc</code>: short for word count, which count the lines, words, and characters in a file.</p> <p>For example, </p> <p>Count the lines, words, and characters in <code>sumstats.txt</code></p> <p><pre><code>$ wc sumstats.txt 445933  5797129 32790417 sumstats.txt\n</code></pre> This means that <code>sumstats.txt</code> has 445933 lines, 5797129 words, and 32790417 characters. </p>"},{"location":"02_Linux_basics/#edit-files","title":"Edit files","text":"<p>Vim is a handy text editor in command line.</p> <pre><code>vim README.md\n</code></pre> <p>Vim - text editor</p> <p></p> <p>Press <code>i</code> to enter insert mode, and then you can edit the file as you want. When finished, just pres <code>Esc</code> to escape insert mode, and then press <code>shift + :</code> , then <code>wq</code> to quit and also save the file.</p> <p>Vim is a little bit hard to learn for beginners, but when you get familiar with it, it will be a mighty and convenient tool. For more detailed tutorials on Vim, you can check: https://github.com/iggredible/Learn-Vim</p> <p>Other common command line text editors</p> <ul> <li>nano</li> <li>emacs</li> </ul>"},{"location":"02_Linux_basics/#permission","title":"Permission","text":"<p>The permissions of a file or directory are represented as a 10-character string (1+3+3+3) :</p> <p>For example, this represents a directory(the initial d) which is readable, writable and executable for the owner(the first 3: rwx), users in the same group(the 3 characters in the middle: rwx) and others (last 3 characters: rwx).</p> <p><code>drwxrwxrwx</code></p> <p>-&gt; <code>d (directory or file) rwx (permissions for owner) rwx (permissions for users in the same group) rwx (permissions for other users)</code></p> Notation Description <code>r</code> readable <code>w</code> writable <code>x</code> executable <code>d</code> directory <code>-</code> file <p>Command for checking the permissions of files in the current directory: <code>ls -l</code></p> <p>Command for changing permissions: <code>chmod</code>, <code>chown</code>, <code>chgrp</code></p> <p>Syntax: <pre><code>chmod [3-digit Binary notation] [path]\n</code></pre></p> Number notation Permission 3-digit Binary notation 7 <code>rwx</code> 111 6 <code>rw-</code> 110 5 <code>r-x</code> 101 4 <code>r--</code> 100 3 <code>-wx</code> 011 2 <code>-w-</code> 010 1 <code>--x</code> 001 0 <code>---</code> 000 <p>Change the permissions of the file <code>README.md</code> to <code>660</code></p> <pre><code># there is a readme file in the directory, and its permissions are -rw-r----- \n$ ls -lh\ntotal 4.0K\n-rw-r----- 1 he staff 2.1K Feb 24 01:16 README.md\n\n# let's change the permissions to 660, which is a numeric notation of -rw-rw---- based on the     table above\n$ chmod 660 README.md # chack again, and it was changed.\n$ ls -lh\ntotal 4.0K\n-rw-rw---- 1 he staff 2.1K Feb 24 01:16 README.md\n</code></pre> <p>Note</p> <p>These commands are very important because we use genome data, which could raise severe ethical and privacy issues if there is data leak. </p> <p>Warning</p> <p>Please always be cautious when handling human genomic data.</p>"},{"location":"02_Linux_basics/#others","title":"Others","text":"<p>There are a group of very handy and flexible commands which will greatly improve your efficiency. These include <code>|</code> , <code>&gt;</code>, <code>&gt;&gt;</code>,<code>*</code>,<code>.</code>,<code>..</code>,<code>~</code>,and <code>-</code>.</p>"},{"location":"02_Linux_basics/#pipe","title":"<code>|</code>  (pipe)","text":"<p>Pipe basically is used to pass the output of the previous command to the next command as input, instead of printing is in terminal. Using pipe you can do very complicated manipulations of the files.</p> <p>An example of Pipe</p> <p><pre><code>cat sumstats.txt | sort | uniq | wc\n</code></pre> This means (1) print sumstats, (2) sort the output, (3) then keep the unique lines and finally (4) count the lines and words.</p>"},{"location":"02_Linux_basics/#_1","title":"<code>&gt;</code>","text":"<p><code>&gt;</code> redirects output to a new file (if the file already exist, it will be overwritten)</p> <p>Redirects the output of <code>cat sumstats.txt | sort | uniq | wc</code> to <code>count.txt</code></p> <pre><code>cat sumstats.txt | sort | uniq | wc &gt; count.txt\n</code></pre>"},{"location":"02_Linux_basics/#_2","title":"<code>&gt;&gt;</code>","text":"<p><code>&gt;&gt;</code> redirects output to a file by appending to the end of the file (if the file already exist, it will not be overwritten)</p> <p>Redirects the output of <code>cat sumstats.txt | sort | uniq | wc</code> to <code>count.txt</code> by appending</p> <pre><code>cat sumstats.txt | sort | uniq | wc &gt;&gt; count.txt\n</code></pre> <p>Other useful commands include : </p> Command Description Example Code Example code meaning <code>*</code> represent zero or more characters - - <code>?</code> represent a single character - - <code>.</code> the current directory - - <code>..</code> the parent directory of the current directory. <code>cd ..</code> change to the parent directory of the current directory <code>~</code> the home directory <code>cd ~</code> change to the curent user's home directory <code>-</code> the last directory you are working in. <code>cd -</code> change to the last directory you are working in. <p>Wildcards</p> <p>The asterisk <code>*</code> and the question mark  <code>?</code> are called wildcard characters or wildcards in Linux, which are special symbols that can represent other normal characters. Wildcards are especially useful when handling multiple files with similar pattern in their names. </p> <p>Warning</p> <p>Be extremely careful when you use rm and *. It is disastrous when you mistakenly type <code>rm *</code></p>"},{"location":"02_Linux_basics/#bash-scripts","title":"Bash scripts","text":"<p>If you have a lot of commands to run, or if you want to automate some complex manipulations, bash scripts are a good way to address this issue.</p> <p>We can use vim to create a bash script called <code>hello.sh</code></p> <p>A simple example of bash scripts:</p> <p>Example</p> hello.sh<pre><code>#!/bin/bash\necho \"Hello, world1\"\necho \"Hello, world2\"\n</code></pre> <p><code>#!</code> is called shebang, which tells the system which interpreter to use to excute the shell script.</p> <p>Then use <code>chmod</code> to give it permission to execute.</p> <pre><code>chmod +x hello.sh </code></pre> <p>Now we can run the srcipt by <code>./hello.sh</code>:</p> <pre><code>./hello.sh\n\"Hello, world1\" \"Hello, world2\" </code></pre>"},{"location":"02_Linux_basics/#advanced-text-editing","title":"Advanced text editing","text":"<p>(optional: awk, sed, cut, sort, join, uniq)</p> <ul> <li><code>cut</code> : cutting out columns from files.</li> <li><code>sort</code>: sorting the lines of a file.</li> <li><code>uniq</code>: filter the duplicated lines in a file.</li> <li><code>join</code>: join two tabular files based on specified keys.</li> </ul> <p>Advanced commands:</p> <ul> <li><code>awk</code> : https://cloufield.github.io/GWASTutorial/60_awk/</li> <li><code>sed</code> : https://cloufield.github.io/GWASTutorial/61_sed/</li> </ul>"},{"location":"02_Linux_basics/#git-and-github","title":"Git and Github","text":"<p>Git is a powerful version control software and github is a platform where you can share your codes.</p> <p>Currently you just need to learn <code>git clone</code>, which simply downloads an existing repository.</p> <p><code>git clone https://github.com/Cloufield/GWASTutorial.git</code></p> <p>You can also check here for more information.</p> <p>Quote</p> <ul> <li>Git Reference: https://git-scm.com/doc</li> <li>Cheatsheet: https://training.github.com/downloads/github-git-cheat-sheet/</li> </ul>"},{"location":"02_Linux_basics/#download","title":"Download","text":"<p>We can use <code>wget [option] [url]</code> command to download files to local machine.</p> <p><code>-O</code> option specify the file name you want to change for the downloaded file. </p> <p>Use wget to download the hg19 reference genome from UCSC</p> <pre><code># Download hg19 reference genome from UCSC\nwget https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz\n\n# Download hg19 reference genome from UCSC and rename it to  my_refgenome.fa.gz\nwget -O my_refgenome.fa.gz https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz\n</code></pre>"},{"location":"02_Linux_basics/#exercise","title":"Exercise","text":"<p>The questions are generated by Microsoft Bing!</p> <p>What is the command to list all files and directories in your current working directory?</p> <ul> <li>A) <code>ls</code></li> <li>B) <code>cd</code></li> <li>C) <code>pwd</code></li> <li>D) <code>mkdir</code></li> </ul> <p>What is the command to create a new directory named \u201ctest\u201d?</p> <ul> <li>A) <code>cd test</code></li> <li>B) <code>pwd test</code></li> <li>C) <code>mkdir test</code></li> <li>D) <code>ls test</code></li> </ul> <p>What is the command to copy a file named \u201cdata.txt\u201d from your current working directory to another directory named \u201cbackup\u201d?</p> <ul> <li>A) <code>cp data.txt backup/</code></li> <li>B) <code>mv data.txt backup/</code></li> <li>C) <code>rm data.txt backup/</code></li> <li>D) <code>cat data.txt backup/</code></li> </ul> <p>What is the command to display the first 10 lines of a file named \u201cresults.csv\u201d?</p> <ul> <li>A) <code>head results.csv</code></li> <li>B) <code>tail results.csv</code></li> <li>C) <code>less results.csv</code></li> <li>D) <code>more results.csv</code></li> </ul> <p>What is the command to count the number of lines, words, and characters in a file named \u201creport.txt\u201d?</p> <ul> <li>A) <code>wc report.txt</code></li> <li>B) <code>count report.txt</code></li> <li>C) <code>size report.txt</code></li> <li>D) <code>stat report.txt</code></li> </ul> <p>What is the command to search for a pattern in a file named \u201clog.txt\u201d and print only the matching lines?</p> <ul> <li>A) <code>grep pattern log.txt</code></li> <li>B) <code>find pattern log.txt</code></li> <li>C) <code>locate pattern log.txt</code></li> <li>D) <code>search pattern log.txt</code></li> </ul> <p>What is the command to sort the contents of a file named \u201cnames.txt\u201d in alphabetical order and save the output to a new file named \u201csorted_names.txt\u201d?</p> <ul> <li>A) <code>sort names.txt &gt; sorted_names.txt</code></li> <li>B) <code>sort names.txt &lt; sorted_names.txt</code></li> <li>C) <code>sort names.txt &gt;&gt; sorted_names.txt</code></li> <li>D) <code>sort names.txt &lt;&lt; sorted_names.txt</code></li> </ul> <p>What is the command to display the difference between two files named \u201cold_version.py\u201d and \u201cnew_version.py\u201d?</p> <ul> <li>A) <code>diff old_version.py new_version.py</code></li> <li>B) <code>cmp old_version.py new_version.py</code></li> <li>C) <code>diffy old_version.py new_version.py</code></li> <li>D) <code>compare old_version.py new_version.py</code></li> </ul> <p>What is the command to change the permissions of a file named \u201cscript.sh\u201d to make it executable by everyone?</p> <ul> <li>A) <code>chmod +x script.sh</code></li> <li>B) <code>chmod 777 script.sh</code></li> <li>C) <code>chmod ugo+x script.sh</code></li> <li>D) <code>All of the above</code></li> </ul> <p>What is the command to run a program named \u201cprogram.exe\u201d in the background and redirect its output to a file named \u201coutput.log\u201d?</p> <ul> <li>A) <code>program.exe &amp; &gt; output.log</code></li> <li>B) <code>program.exe &gt; output.log &amp;</code></li> <li>C) <code>program.exe &lt; output.log &amp;</code></li> <li>D) <code>program.exe &amp; &lt; output.log</code></li> </ul>"},{"location":"03_Data_formats/","title":"Data format","text":"<p>This section lists some of the most commonly used formats in complex trait genomic analysis. </p>"},{"location":"03_Data_formats/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Data formats for general purpose<ul> <li>txt</li> <li>tsv</li> <li>csv</li> </ul> </li> <li>Data formats in bioinformatics<ul> <li>Sequence<ul> <li>FASTA</li> <li>FASTQ</li> </ul> </li> <li>Alignment<ul> <li>SAM/BAM</li> </ul> </li> <li>Variant and genotype<ul> <li>VCF/BCF</li> <li>ped/map</li> <li>bed/fam/bim</li> </ul> </li> <li>Imputation dosage<ul> <li>bgen</li> <li>pgen</li> </ul> </li> </ul> </li> </ul>"},{"location":"03_Data_formats/#data-formats-for-general-purposes","title":"Data formats for general purposes","text":""},{"location":"03_Data_formats/#txt","title":"txt","text":"<p>Simple text file</p> <p><code>.txt</code></p> <p><pre><code>cat sample_text.txt \nLorem ipsum dolor sit amet, consectetur adipiscing elit. In ut sem congue, tristique tortor et, ullamcorper elit. Nulla elementum, erat ac fringilla mattis, nisi tellus euismod dui, interdum laoreet orci velit vel leo. Vestibulum neque mi, pharetra in tempor id, malesuada at ipsum. Duis tellus enim, suscipit sit amet vestibulum in, ultricies vitae erat. Proin consequat id quam sed sodales. Ut a magna non tellus dictum aliquet vitae nec mi. Suspendisse potenti. Vestibulum mauris sem, viverra ac metus sed, scelerisque ornare arcu. Vivamus consequat, libero vitae aliquet tempor, lorem leo mattis arcu, et viverra erat ligula sit amet tortor. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Praesent ut massa ac tortor lobortis placerat. Pellentesque aliquam tortor augue, at rutrum magna molestie et. Etiam congue nulla in venenatis congue. Nunc ac felis pharetra, cursus leo et, finibus eros.\n</code></pre> Random texts are generated using - https://www.lipsum.com/</p>"},{"location":"03_Data_formats/#tsv","title":"tsv","text":"<p>Tab-separated values Tabular data format </p> <p><code>.tsv</code></p> <pre><code>head sample_data.tsv\n#CHROM  POS ID  REF ALT A1  FIRTH?  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1:13273:G:C G   C   C   N   ADD 503 0.750168    0.280794    -1.02373    0.305961    .\n1   14599   1:14599:T:A T   A   A   N   ADD 503 1.80972 0.231595    2.56124 0.0104299   .\n1   14604   1:14604:A:G A   G   G   N   ADD 503 1.80972 0.231595    2.56124 0.0104299   .\n1   14930   1:14930:A:G A   G   G   N   ADD 503 1.70139 0.240245    2.21209 0.0269602   .\n1   69897   1:69897:T:C T   C   T   N   ADD 503 1.58002 0.194774    2.34855 0.0188466   .\n1   86331   1:86331:A:G A   G   G   N   ADD 503 1.47006 0.236102    1.63193 0.102694    .\n1   91581   1:91581:G:A G   A   A   N   ADD 503 0.924422    0.122991    -0.638963   0.522847    .\n1   122872  1:122872:T:G    T   G   G   N   ADD 503 1.07113 0.180776    0.380121    0.703856    .\n1   135163  1:135163:C:T    C   T   T   N   ADD 503 0.711822    0.23908 -1.42182    0.155079    .\n</code></pre>"},{"location":"03_Data_formats/#csv","title":"csv","text":"<p>Comma-separated values Tabular data format </p> <p><code>.csv</code></p> <pre><code>head sample_data.csv \n#CHROM,POS,ID,REF,ALT,A1,FIRTH?,TEST,OBS_CT,OR,LOG(OR)_SE,Z_STAT,P,ERRCODE\n1,13273,1:13273:G:C,G,C,C,N,ADD,503,0.750168,0.280794,-1.02373,0.305961,.\n1,14599,1:14599:T:A,T,A,A,N,ADD,503,1.80972,0.231595,2.56124,0.0104299,.\n1,14604,1:14604:A:G,A,G,G,N,ADD,503,1.80972,0.231595,2.56124,0.0104299,.\n1,14930,1:14930:A:G,A,G,G,N,ADD,503,1.70139,0.240245,2.21209,0.0269602,.\n1,69897,1:69897:T:C,T,C,T,N,ADD,503,1.58002,0.194774,2.34855,0.0188466,.\n1,86331,1:86331:A:G,A,G,G,N,ADD,503,1.47006,0.236102,1.63193,0.102694,.\n1,91581,1:91581:G:A,G,A,A,N,ADD,503,0.924422,0.122991,-0.638963,0.522847,.\n1,122872,1:122872:T:G,T,G,G,N,ADD,503,1.07113,0.180776,0.380121,0.703856,.\n1,135163,1:135163:C:T,C,T,T,N,ADD,503,0.711822,0.23908,-1.42182,0.155079,.\n</code></pre>"},{"location":"03_Data_formats/#data-formats-in-bioinformatics","title":"Data formats in bioinformatics","text":"<p>A typical workflow for generating genotype data for genome-wide association analysis.</p> <p></p>"},{"location":"03_Data_formats/#sequence","title":"Sequence","text":""},{"location":"03_Data_formats/#fasta","title":"fasta","text":"<p>text-based format for representing either nucleotide sequences or amino acid (protein) sequences</p> <p><code>.fa</code> or <code>.fasta</code></p> <pre><code>&gt;SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n</code></pre>"},{"location":"03_Data_formats/#fastq","title":"fastq","text":"<p>text-based format for storing both a nucleotide sequence and its corresponding quality scores</p> <p><code>.fastq</code></p> <p><pre><code>@SEQ_ID\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n</code></pre> Reference: https://en.wikipedia.org/wiki/FASTQ_format</p>"},{"location":"03_Data_formats/#alingment","title":"Alingment","text":""},{"location":"03_Data_formats/#sambam","title":"SAM/BAM","text":"<p>Sequence Alignment/Map Format is a TAB-delimited text file format consisting of a header section and an alignment section.</p> <p><code>.sam</code></p> <p><pre><code>@HD VN:1.6 SO:coordinate\n@SQ SN:ref LN:45\nr001 99 ref 7 30 8M2I4M1D3M = 37 39 TTAGATAAAGGATACTG *\nr002 0 ref 9 30 3S6M1P1I4M * 0 0 AAAAGATAAGGATA *\nr003 0 ref 9 30 5S6M * 0 0 GCCTAAGCTAA * SA:Z:ref,29,-,6H5M,17,0;\nr004 0 ref 16 30 6M14N5M * 0 0 ATAGCTTCAGC *\nr003 2064 ref 29 17 6H5M * 0 0 TAGGC * SA:Z:ref,9,+,5S6M,30,1;\nr001 147 ref 37 30 9M = 7 -39 CAGCGGCAT * NM:i:1\n</code></pre> Reference : https://samtools.github.io/hts-specs/SAMv1.pdf</p>"},{"location":"03_Data_formats/#variant-and-genotype","title":"Variant and genotype","text":""},{"location":"03_Data_formats/#vcf-vcfgz-vcfgztbi","title":"vcf / vcf.gz / vcf.gz.tbi","text":"<p>VCF is a text file format consisting of meta-information lines, a header line, and then data lines. Each data line contains information about a variant in the genome (and the genotype information on samples for each variant). </p> <p><code>.vcf</code></p> <p><pre><code>##fileformat=VCFv4.2\n##fileDate=20090805\n##source=myImputationProgramV3.1\n##reference=file:///seq/references/1000GenomesPilot-NCBI36.fasta\n##contig=&lt;ID=20,length=62435964,assembly=B36,md5=f126cdf8a6e0c7f379d618ff66beb2da,species=\"Homo sapiens\",taxonomy=x&gt;\n##phasing=partial\n##INFO=&lt;ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"&gt;\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"&gt;\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\"&gt;\n##INFO=&lt;ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"&gt;\n##INFO=&lt;ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"&gt;\n##INFO=&lt;ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"&gt;\n##FILTER=&lt;ID=q10,Description=\"Quality below 10\"&gt;\n##FILTER=&lt;ID=s50,Description=\"Less than 50% of samples have data\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n##FORMAT=&lt;ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"&gt;\n#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA00001 NA00002 NA00003\n20 14370 rs6054257 G A 29 PASS NS=3;DP=14;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:51,51 1|0:48:8:51,51 1/1:43:5:.,.\n20 17330 . T A 3 q10 NS=3;DP=11;AF=0.017 GT:GQ:DP:HQ 0|0:49:3:58,50 0|1:3:5:65,3 0/0:41:3\n20 1110696 rs6040355 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4\n20 1230237 . T . 47 PASS NS=3;DP=13;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:51,51 0/0:61:2\n20 1234567 microsat1 GTC G,GTCT 50 PASS NS=3;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3\n</code></pre> Reference : https://samtools.github.io/hts-specs/VCFv4.2.pdf </p>"},{"location":"03_Data_formats/#plink-format","title":"PLINK format","text":"<p>The figure shows how genotypes are stored in files.</p> <p>We have 3 parts of information:</p> <ol> <li>Individual information</li> <li>Variant information</li> <li>Genotype matrix</li> </ol> <p>And there are different ways (format sets) to represent this information in PLINK1.9 and PLINK2:</p> <ol> <li>ped / map</li> <li>fam / bim / bed</li> <li>psam / pvar / pgen</li> </ol> <p></p>"},{"location":"03_Data_formats/#ped-map","title":"ped / map","text":"<p><code>.ped</code> (PLINK/MERLIN/Haploview text pedigree + genotype table)</p> <p>Original standard text format for sample pedigree information and genotype calls.Contains no header line, and one line per sample with 2V+6 fields where V is the number of variants. The first six fields are the same as those in a .fam file. The seventh and eighth fields are allele calls for the first variant in the .map file ('0' = no call); the 9th and 10th are allele calls for the second variant; and so on.</p> <p><code>.ped</code></p> <pre><code># check the first 16 rows and 16 columns of the ped file\ncut -d \" \" -f 1-16 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.ped | head\n0 HG00403 0 0 0 -9 G G T T A A G A C C\n0 HG00404 0 0 0 -9 G G T T A A G A T C\n0 HG00406 0 0 0 -9 G G T T A A G A T C\n0 HG00407 0 0 0 -9 G G T T A A A A C C\n0 HG00409 0 0 0 -9 G G T T A A G A C C\n0 HG00410 0 0 0 -9 G G T T A A G A C C\n0 HG00419 0 0 0 -9 G G T T A A A A T C\n0 HG00421 0 0 0 -9 G G T T A A G A C C\n0 HG00422 0 0 0 -9 G G T T A A G A C C\n0 HG00428 0 0 0 -9 G G T T A A G A C C\n0 HG00436 0 0 0 -9 G G A T G A A A C C\n0 HG00437 0 0 0 -9 C G T T A A G A C C\n0 HG00442 0 0 0 -9 G G T T A A G A C C\n0 HG00443 0 0 0 -9 G G T T A A G A C C\n0 HG00445 0 0 0 -9 G G T T A A G A C C\n0 HG00446 0 0 0 -9 C G T T A A G A T C\n</code></pre> <p><code>.map</code> (PLINK text fileset variant information file)</p> <p>Variant information file accompanying a .ped text pedigree + genotype table. A text file with no header line, and one line per variant with the following 3-4 fields:</p> <ul> <li>Chromosome code. PLINK 1.9 also permits contig names here, but most older programs do not.</li> <li>Variant identifier</li> <li>Position in morgans or centimorgans (optional; also safe to use dummy value of '0')</li> <li>Base-pair coordinate</li> </ul> <p><code>.map</code></p> <pre><code>head 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.map\n1       1:13273:G:C     0       13273\n1       1:14599:T:A     0       14599\n1       1:14604:A:G     0       14604\n1       1:14930:A:G     0       14930\n1       1:69897:T:C     0       69897\n1       1:86331:A:G     0       86331\n1       1:91581:G:A     0       91581\n1       1:122872:T:G    0       122872\n1       1:135163:C:T    0       135163\n1       1:233473:C:G    0       233473\n</code></pre> <p>Reference: https://www.cog-genomics.org/plink/1.9/formats</p>"},{"location":"03_Data_formats/#bed-fam-bim","title":"bed / fam /bim","text":"<p>bed/fam/bim formats are the binary implementation of ped/map formats. bed/bim/fam files contain the same information as ped/map but are much smaller in size.</p> <pre><code>-rw-r----- 1 yunye yunye 135M Dec 23 11:45 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bed\n-rw-r----- 1 yunye yunye  36M Dec 23 11:46 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim\n-rw-r----- 1 yunye yunye 9.4K Dec 23 11:46 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam\n-rw-r--r-- 1 yunye yunye  32M Dec 27 17:51 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.map\n-rw-r--r-- 1 yunye yunye 2.2G Dec 27 17:51 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.ped\n</code></pre> <p><code>.fam</code></p> <pre><code>head 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam\n0 HG00403 0 0 0 -9\n0 HG00404 0 0 0 -9\n0 HG00406 0 0 0 -9\n0 HG00407 0 0 0 -9\n0 HG00409 0 0 0 -9\n0 HG00410 0 0 0 -9\n0 HG00419 0 0 0 -9\n0 HG00421 0 0 0 -9\n0 HG00422 0 0 0 -9\n0 HG00428 0 0 0 -9\n</code></pre> <p><code>.bim</code></p> <pre><code>head 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim\n1       1:13273:G:C     0       13273   C       G\n1       1:14599:T:A     0       14599   A       T\n1       1:14604:A:G     0       14604   G       A\n1       1:14930:A:G     0       14930   G       A\n1       1:69897:T:C     0       69897   C       T\n1       1:86331:A:G     0       86331   G       A\n1       1:91581:G:A     0       91581   A       G\n1       1:122872:T:G    0       122872  G       T\n1       1:135163:C:T    0       135163  T       C\n1       1:233473:C:G    0       233473  G       C\n</code></pre> <p><code>.bed</code></p> <p>\"Primary representation of genotype calls at biallelic variants The first three bytes should be 0x6c, 0x1b, and 0x01 in that order. The rest of the file is a sequence of V blocks of N/4 (rounded up) bytes each, where V is the number of variants and N is the number of samples. The first block corresponds to the first marker in the .bim file, etc.\" <pre><code>hexdump -C 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bed | head\n00000000  6c 1b 01 ff ff bf bf ff  ff ff ef fb ff ff ff fe  |l...............|\n00000010  ff ff ff ff fb ff bb ff  ff fb af ff ff fe fb ff  |................|\n00000020  ff ff ff fe ff ff ff ff  ff bf ff ff ef ff ff ef  |................|\n00000030  bb ff ff ff ff ff ff ff  fa ff ff ff ff ff ff ff  |................|\n00000040  ff ff ff fb ff ff ff ff  ff ff ff ff ff ff ff ef  |................|\n00000050  ff ff ff fb fe ef fe ff  ff ff ff eb ff ff fe fe  |................|\n00000060  ff ff fe ff bf ff fa fb  fb eb be ff ff 3b ff be  |.............;..|\n00000070  fe be bf ef fe ff ef ee  ff ff bf ea fe bf fe ff  |................|\n00000080  bf ff ff ef ff ff ff ff  ff fa ff ff eb ff ff ff  |................|\n00000090  ff ff fb fe af ff bf ff  ff ff ff ff ff ff ff ff  |................|\n</code></pre></p> <p>Reference: https://www.cog-genomics.org/plink/1.9/formats</p>"},{"location":"03_Data_formats/#imputation-dosage","title":"Imputation dosage","text":""},{"location":"03_Data_formats/#bgen-bgi","title":"bgen / bgi","text":"<p>Reference: https://www.well.ox.ac.uk/~gav/bgen_format/</p>"},{"location":"03_Data_formats/#pgenpsampvar","title":"pgen,psam,pvar","text":"<p>Reference: https://www.cog-genomics.org/plink/2.0/formats#pgen</p> <p>NOTE: <code>pgen</code> only saved the dosage for each individual (a scalar ranged from 0 to 2). It could not been converted back to the genotype probability (a vector of length 3) or allele probability (a matrix of dimension 2 x 2) saved in <code>bgen</code>.</p>"},{"location":"03_Data_formats/#summary","title":"Summary","text":""},{"location":"04_Data_QC/","title":"Plink basics","text":"<p>In this module, we will learn the basics of genotype data QC using PLINK, which is one of the most commonly used software in complex trait genomics.</p>"},{"location":"04_Data_QC/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Preparation<ul> <li>PLINK 1.9 &amp; 2 installation</li> <li>Download genotype data</li> </ul> </li> <li>PLINK tutorial<ul> <li>Calculate the missing rate and call rate</li> <li>Calculate allele frequency</li> <li>Calculate the inbreeding F coefficient </li> <li>Hardy-Weinberg equilibrium exact test</li> <li>Applying filters</li> <li>LD-Pruning</li> <li>Sample &amp; SNP filtering (extract/exclude/keep/remove)</li> <li>LD calculation</li> <li>Estimate IBD / PI_HAT</li> <li>Data management (make-bed/recode)</li> </ul> </li> <li>Exercise</li> <li>Additional resources</li> <li>Reference</li> </ul>"},{"location":"04_Data_QC/#preparation","title":"Preparation","text":""},{"location":"04_Data_QC/#plink-192-installation","title":"PLINK 1.9&amp;2 installation","text":"<p>To get prepared for genotype QC, we will need to make directories, download software and add the software to your environment path.</p> <p>First, we will simply create some directories to keep the tools we need to use.</p> <p>Create directories</p> <p><pre><code>cd ~\nmkdir tools\ncd tools\nmkdir bin\nmkdir plink\nmkdir plink2\n</code></pre> </p> <p>You can download each tool into its corresponding directories. </p> <p>The <code>bin</code> directory here is for keeping all the symbolic links to the executable files of each tool. </p> <p>In this way, it is much easier to manage and organize the paths and tools. We will only add the <code>bin</code> directory here to the environment path.</p>"},{"location":"04_Data_QC/#download-plink19-and-plink2-and-then-unzip","title":"Download PLINK1.9 and PLINK2 and then unzip","text":"<p>Next, go to the plink webpage to download the software. We will need both PLINK1.9 and PLINK2.</p> <p>Download PLINK1.9 and PLINK2 from the following webpage to the corresponding directories:</p> <ul> <li>PLINK1.9 : https://www.cog-genomics.org/plink/</li> <li>PLINK2 : https://www.cog-genomics.org/plink/2.0/</li> </ul> <p>Note</p> <p>If you are using mac or windows, then please download the mac or windows version. In this tutorial, we will use a Linux system and the Linux version of PLINK. </p> <p>Find the suitable version in the PLINK website, right click and copy the link address.</p> <p>Download PLINK</p> <pre><code>cd plink2\nwget https://s3.amazonaws.com/plink2-assets/plink2_linux_x86_64_20211011.zip\nunzip plink2_linux_x86_64_20211011.zip\n</code></pre> <p>Then do the same for PLINK1.9</p>"},{"location":"04_Data_QC/#create-symbolic-links","title":"Create symbolic links","text":"<p>After downloading and unzipping, we will create symbolic links for the plink binary files, and then move the link to <code>~/tools/bin/</code>.</p> <p>Create symbolic links</p> <pre><code>cd ~\nln -s ~/tools/plink2/plink2 ~/tools/bin/plink2\nln -s ~/tools/plink/plink ~/tools/bin/plink\n</code></pre>"},{"location":"04_Data_QC/#add-paths-to-the-environment-path","title":"Add paths to the environment path","text":"<p>Then add <code>~/tools/bin/</code> to the environment path.</p> <p>Example</p> <p><pre><code>export PATH=$PATH:~/tools/bin/\n</code></pre> This command will add the path to your current shell. </p> <p>If you restart the terminal, it will be lost. So you may need to add it to the configuration file. Then run </p> <pre><code>echo \"export PATH=$PATH:~/tools/bin/\" &gt;&gt; ~/.bashrc\n</code></pre> <p>This will add a new line at the end of <code>.bashrc</code>, which will be run every time you open a new bash shell.</p> <p>All done. Let's test if we installed PLINK successfully or not.</p> <p>Check if PLINK is installed successfully.</p> <pre><code>plink -h\nPLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\n  plink &lt;input flag(s)...&gt; [command flag(s)...] [other flag(s)...]\nplink --help [flag name(s)...]\n\nCommands include --make-bed, --recode, --flip-scan, --merge-list,\n--write-snplist, --list-duplicate-vars, --freqx, --missing, --test-mishap,\n--hardy, --mendel, --ibc, --impute-sex, --indep-pairphase, --r2, --show-tags,\n--blocks, --distance, --genome, --homozyg, --make-rel, --make-grm-gz,\n--rel-cutoff, --cluster, --pca, --neighbour, --ibs-test, --regress-distance,\n--model, --bd, --gxe, --logistic, --dosage, --lasso, --test-missing,\n--make-perm-pheno, --tdt, --qfam, --annotate, --clump, --gene-report,\n--meta-analysis, --epistasis, --fast-epistasis, and --score.\n\n\"plink --help | more\" describes all functions (warning: long).\n</code></pre> <pre><code>plink2 -h\nPLINK v2.00a3LM 64-bit Intel (1 Jul 2021)      www.cog-genomics.org/plink/2.0/\n(C) 2005-2021 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\n  plink2 &lt;input flag(s)...&gt; [command flag(s)...] [other flag(s)...]\nplink2 --help [flag name(s)...]\n\nCommands include --rm-dup list, --make-bpgen, --export, --freq, --geno-counts,\n--sample-counts, --missing, --hardy, --het, --fst, --indep-pairwise, --ld,\n--sample-diff, --make-king, --king-cutoff, --pmerge, --pgen-diff,\n--write-samples, --write-snplist, --make-grm-list, --pca, --glm, --adjust-file,\n--score, --variant-score, --genotyping-rate, --pgen-info, --validate, and\n--zst-decompress.\n\n\"plink2 --help | more\" describes all functions.\n</code></pre> <p>Well done. We have installed plink1.9 and plink2.</p>"},{"location":"04_Data_QC/#download-genotype-data","title":"Download genotype data","text":"<p>Next, we need to download the sample genotype data. The way to create the sample data is described [here].(https://cloufield.github.io/GWASTutorial/01_Dataset/) This dataset contains 504 EAS individuals from 1000 Genome Project Phase 3v5 with around 1 million variants.</p> <p>Simply run <code>download.sh</code> in 01_Dataset to download this dataset (from Dropbox).</p> <p>Sample dataset is currently hosted on Dropbox which may not be accessible for users in certain regions.</p> <p>Download sample data</p> <pre><code>cd ../01_Dataset\n./download_sampledata.sh\n</code></pre> <p>And you will get the following three PLINK files:</p> <pre><code>-rw-r-----   1 he  staff   135M Dec 23 11:45 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bed\n-rw-r-----   1 he  staff    36M Dec 23 11:46 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim\n-rw-r-----   1 he  staff   9.4K Dec 23 11:46 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam\n</code></pre> <p>Check the bim file:</p> <pre><code>head 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim\n1   1:13273:G:C 0   13273   C   G\n1   1:14599:T:A 0   14599   A   T\n1   1:14604:A:G 0   14604   G   A\n1   1:14930:A:G 0   14930   G   A\n1   1:69897:T:C 0   69897   C   T\n1   1:86331:A:G 0   86331   G   A\n1   1:91581:G:A 0   91581   A   G\n1   1:122872:T:G    0   122872  G   T\n1   1:135163:C:T    0   135163  T   C\n1   1:233473:C:G    0   233473  G   C\n</code></pre> <p>Check the fam file: <pre><code>head 1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam\n0 HG00403 0 0 0 -9\n0 HG00404 0 0 0 -9\n0 HG00406 0 0 0 -9\n0 HG00407 0 0 0 -9\n0 HG00409 0 0 0 -9\n0 HG00410 0 0 0 -9\n0 HG00419 0 0 0 -9\n0 HG00421 0 0 0 -9\n0 HG00422 0 0 0 -9\n0 HG00428 0 0 0 -9\n</code></pre></p>"},{"location":"04_Data_QC/#plink-tutorial","title":"PLINK tutorial","text":"<p>Detailed descriptions can be found on plink's website: PLINK1.9 and PLINK2.</p> <p>The functions we will learn in this tutorial:</p> <ol> <li>Calculating missing rate (call rate)</li> <li>Calculating inbreeding F coefficient</li> <li>Conducting Hardy-Weinberg equilibrium exact test</li> <li>Calculating allele Frequency</li> <li>Applying filters</li> <li>Conducting LD-Pruning</li> <li>Conducting sample &amp; SNP filtering (extract/exclude/keep/remove)</li> <li>Estimating IBD / PI_HAT</li> <li>Calculating LD</li> <li>Data management (make-bed/recode)</li> </ol> <p>All sample codes and results for this module are available in <code>./04_data_QC</code></p>"},{"location":"04_Data_QC/#qc-step-summary","title":"QC Step Summary","text":"<p>QC Step Summary</p> QC step Option in PLINK Commonly used threshold to exclude Sample missing rate <code>--geno</code>,  <code>--missing</code> missing rate &gt; 0.01 SNP missing rate <code>--mind</code>, <code>--missing</code> missing rate &gt; 0.01 Minor allele frequency <code>--freq</code>, <code>--maf</code> maf &lt; 0.01 Sample Relatedness <code>--genome</code> pi_hat &gt; 0.2 to exclude second-degree relatives Hardy-Weinberg equilibrium <code>--hwe</code>,<code>--hardy</code> hwe &lt; 5e-6 Inbreeding F coefficient <code>--het</code> outside of 3 SD from the mean <p>First, we can calculate some basic statistics of our simulated data:</p>"},{"location":"04_Data_QC/#missing-rate-call-rate","title":"Missing rate (call rate)","text":"<p>The first thing we want to know is the missing rate of our data. Usually, we need to check the missing rate of samples and SNPs to decide a threshold to exclude low-quality samples and SNPs. (https://www.cog-genomics.org/plink/1.9/basic_stats#missing)</p> <ul> <li>Sample missing rate: the proportion of missing values for an individual across all SNPs.</li> <li>SNP missing rate: the proportion of missing values for a SNP across all samples.</li> </ul> <p>Missing rate and Call rate</p> <p>Suppose we have N samples and M SNPs for each sample.</p> <p>For sample \\(j\\) :</p> \\[Sample\\ Missing\\ Rate_{j} = {{N_{missing\\ SNPs\\ for\\ j}}\\over{M}} = 1 - Call\\ Rate_{sample, j}\\] <p>For SNP \\(i\\) :</p> \\[SNP\\ Missing\\ Rate_{i} = {{N_{missing\\ samples\\ at\\ i}}\\over{N}} = 1 - Call\\ Rate_{SNP, i}\\] <p>The input is PLINK bed/bim/fam file. Usually, they have the same prefix, and we just need to pass the prefix to <code>--bfile</code> option.</p>"},{"location":"04_Data_QC/#plink-syntax","title":"PLINK syntax","text":"<p>PLINK syntax</p> <p></p> <p>To calculate the missing rate, we need the flag <code>--missing</code>, which tells PLINK to calculate the missing rate in the dataset specified by <code>--bfile</code>. </p> <p>Calculate missing rate</p> <p><pre><code>cd ../04_Data_QC\ngenotypeFile=\"../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\" #!!! Please add your own path here.  \"1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\" is the prefix of PLINK bed file. \n\nplink \\\n--bfile ${genotypeFile} \\\n--missing \\\n--out plink_results\n</code></pre> Remeber to set the value for <code>${genotypeFile}</code>.</p> <p>This code will generate two files <code>plink_results.imiss</code> and <code>plink_results.lmiss</code>, which contain the missing rate information for samples and SNPs respectively.</p> <p>Take a look at the <code>.imiss</code> file. The last column shows the missing rate for samples. Since we used part of the 1000 Genome Project data this time, there are no missing samples or SNPs. So the missing rate is zero.</p> <pre><code>head plink_results.imiss\n FID       IID MISS_PHENO   N_MISS   N_GENO   F_MISS\n   0   HG00403          Y        0  1122299        0\n0   HG00404          Y        0  1122299        0\n0   HG00406          Y        0  1122299        0\n0   HG00407          Y        0  1122299        0\n0   HG00409          Y        0  1122299        0\n0   HG00410          Y        0  1122299        0\n0   HG00419          Y        0  1122299        0\n0   HG00421          Y        0  1122299        0\n0   HG00422          Y        0  1122299        0\n</code></pre> <p>For the meaning of headers, please refer to PLINK documents.</p>"},{"location":"04_Data_QC/#allele-frequency","title":"Allele Frequency","text":"<p>One of the most important statistics of SNPs is their frequency in a certain population. A number of downstream analysis is based on investigating differences in allele frequencies.</p> <p>Usually, variants can be categorized into 3 groups based on their Minor Allel Frequency (MAF):</p> <ol> <li>Common variants : MAF&gt;=0.05</li> <li>Low-frequency variants : 0.01&lt;=MAF&lt;0.05</li> <li>Rare variants : MAF&lt;0.01</li> </ol> <p>How to calculate Minor Allele Frequency (MAF)</p> <p>Suppose the reference allele(REF) is A and the alternative allele(ALT) is B for a certain SNP. The posible genotypes are AA, AB and BB.  In a population of N samples (2N alleles), \\(N = N_{AA} + 2 \\times N_{AB} + N_{BB}\\) :</p> <ul> <li>the number of A alleles:  \\(N_A = 2 \\times N_{AA} + N_{AB}\\)</li> <li>the number of B alleles:  \\(N_B = 2 \\times N_{BB} + N_{AB}\\)</li> </ul> <p>So we can calculate the allele frequency:</p> <ul> <li>Reference Allele Frequency : \\(AF_{REF}= {{N_A}\\over{N_A + N_B}}\\)</li> <li>Alternative Allele Frequency : \\(AF_{ALT}= {{N_B}\\over{N_A + N_B}}\\)</li> </ul> <p>The MAF for this SNP in this specific population is defined as:</p> <p>\\(MAF = min( AF_{REF}, AF_{ALT} )\\)</p> <p>For different downstream analyses, we might use different sets of variants. For example, for PCA, we might use only common variants. For gene-based tests, we might use only rare variants.</p> <p>Using PLINK1.9 we can easily calculate the MAF of variants in the input data.</p> <p>Calculate the MAF of variants using PLINK1.9</p> <pre><code>plink \\\n--bfile ${genotypeFile} \\\n--freq \\\n--out plink_results\n</code></pre> <pre><code># results from plink1.9\nhead plink_results.frq\n CHR              SNP   A1   A2          MAF  NCHROBS\n   1      1:13273:G:C    C    G       0.0625     1008\n1      1:14599:T:A    A    T      0.08929     1008\n1      1:14604:A:G    G    A      0.08929     1008\n1      1:14930:A:G    G    A       0.4137     1008\n1      1:69897:T:C    T    C        0.124     1008\n1      1:86331:A:G    G    A       0.0873     1008\n1      1:91581:G:A    A    G        0.498     1008\n1     1:122872:T:G    G    T       0.2589     1008\n1     1:135163:C:T    T    C      0.09226     1008\n</code></pre> <p>Next, we use plink2 to run the same options to check the difference between the results.</p> <p>Calculate the alternative allele frequencies of variants using PLINK2</p> <pre><code>plink2 \\\n--bfile ${genotypeFile} \\\n--freq \\\n--out plink_results\n</code></pre> <pre><code># results from plink2\nhead plink_results.afreq\n#CHROM  ID  REF ALT ALT_FREQS   OBS_CT\n1   1:13273:G:C G   C   0.0625  1008\n1   1:14599:T:A T   A   0.0892857   1008\n1   1:14604:A:G A   G   0.0892857   1008\n1   1:14930:A:G A   G   0.41369 1008\n1   1:69897:T:C T   C   0.875992    1008\n1   1:86331:A:G A   G   0.0873016   1008\n1   1:91581:G:A G   A   0.498016    1008\n1   1:122872:T:G    T   G   0.258929    1008\n1   1:135163:C:T    C   T   0.0922619   1008\n</code></pre> <p>We need to pay attention to the concepts here.</p> <p>In PLINK1.9, the concept here is minor (A1) and major(A2) allele, while in PLINK2 it is the reference (REF) allele and the alternative (ALT) allele.</p> <ul> <li>Major / Minor: Major allele and minor allele are defined as the allele with the highest and lower(or the second highest for multiallelic variants) allele in a given population, respectively. So major and minor alleles for a certain SNP might be different in two independent populations. The range for MAF(minor allele frequencies) is [0,0.5].</li> <li>Ref / Alt: The reference (REF) and alternative (ALT) alleles are simply determined by the allele on a reference genome. If we use the same reference genome, the reference(REF) and alternative(ALT) alleles will be the same across populations. The reference allele could be major or minor in different populations. The range for alternative allele frequency is [0,1], since it could be the major allele or the minor allele in a given population.</li> </ul>"},{"location":"04_Data_QC/#inbreeding-f-coefficient","title":"Inbreeding F coefficient","text":"<p>Next, we can check the heterozygosity F of samples (https://www.cog-genomics.org/plink/1.9/basic_stats#ibc) : </p> <p><code>-het</code> option will compute observed and expected autosomal homozygous genotype counts for each sample. Usually, we need to exclude individuals with high or low heterozygosity coefficients, which suggests that the sample might be contaminated. </p> <p>Inbreeding F coefficient calculation by PLINK</p> \\[F = {{O(HOM) - E(HOM)}\\over{ M - E(HOM)}}\\] <ul> <li>\\(E(HOM)\\) :Expected Homozygous Genotype Count </li> <li>\\(O(HOM)\\) :Observed Homozygous Genotype Count </li> <li>M : Number of SNPs</li> </ul> <p>Calculate inbreeding F coefficient</p> <pre><code>plink \\\n--bfile ${genotypeFile} \\\n--het \\\n--out plink_results\n</code></pre> <p>Check the output:</p> <pre><code>head plink_results.het\n FID       IID       O(HOM)       E(HOM)        N(NM)            F\n   0   HG00403       747270    7.488e+05      1122299    -0.004114\n   0   HG00404       748955    7.488e+05      1122299    0.0003974\n   0   HG00406       750093    7.488e+05      1122299     0.003444\n   0   HG00407       746566    7.488e+05      1122299    -0.005999\n   0   HG00409       751694    7.488e+05      1122299     0.007731\n   0   HG00410       745078    7.488e+05      1122299    -0.009983\n   0   HG00419       747996    7.488e+05      1122299     -0.00217\n   0   HG00421       757199    7.488e+05      1122299      0.02247\n   0   HG00422       752725    7.488e+05      1122299      0.01049\n</code></pre> <p>A commonly used method is to exclude samples with heterozygosity F deviating more than 3 standard deviation(SD) from the mean.</p> <p>Usually we will use only LD-pruned SNPs  for the calculation of F.</p>"},{"location":"04_Data_QC/#hardy-weinberg-equilibrium-exact-test","title":"Hardy-Weinberg equilibrium exact test","text":"<p>For SNP QC, besides checking the missing rate, we also need to check if the SNP is in Hardy-Weinberg equilibrium:</p> <p><code>--hardy</code> will perform Hardy-Weinberg equilibrium exact test for each variant. Variants with low P value usually suggest genotyping errors, or indicate evolutionary selection for these variants.</p> <p>The following command can calculate the Hardy-Weinberg equilibrium exact test statistics for all SNPs. (https://www.cog-genomics.org/plink/1.9/basic_stats#hardy)</p> <p>Info</p> <p>Suppose we have N unrelated samples (2N alleles). Under HWE, the exact probability of observing \\(n_{AB}\\) sample with genotype AB in N samples is:</p> \\[P(N_{AB} = n_{AB} | N, n_A) = {{2^{n_{AB}}}N!\\over{n_{AA}!n_{AB}!n_{BB}!}} \\times {{n_A!n_B!}\\over{n_A!n_B!}} \\] <p>To compute the Hardy-Weinberg equilibrium exact test statistics, we will sum up the probabilities of all configurations with probability equal to or less than the observed configuration :</p> \\[P_{HWE} = \\sum_{n^{*}_AB} I[P(N_{AB} = n_{AB} | N, n_A) \\geqq P(N_{AB} = n^{*}_{AB} | N, n_A)] \\times P(N_{AB} = n^{*}_{AB} | N, n_A)\\] <p>\\(I(x)\\) is the indicator function. If x is true, \\(I(x) = 1\\); otherwise, \\(I(x) = 0\\).</p> <p>Reference : Wigginton, J. E., Cutler, D. J., &amp; Abecasis, G. R. (2005). A note on exact tests of Hardy-Weinberg equilibrium. The American Journal of Human Genetics, 76(5), 887-893. Link</p> <p>Calculate the Hardy-Weinberg equilibrium exact test statistics</p> <p><pre><code>plink \\\n--bfile ${genotypeFile} \\\n--hardy \\\n--out plink_results\n</code></pre> <pre><code>head plink_results.hwe\n CHR              SNP     TEST   A1   A2                 GENO   O(HET)   E(HET)            P 1      1:13273:G:C  ALL(NP)    C    G             1/61/442    0.121   0.1172       0.7113\n   1      1:14599:T:A  ALL(NP)    A    T             1/88/415   0.1746   0.1626       0.1625\n   1      1:14604:A:G  ALL(NP)    G    A             1/88/415   0.1746   0.1626       0.1625\n   1      1:14930:A:G  ALL(NP)    G    A             4/409/91   0.8115   0.4851    1.679e-61\n   1      1:69897:T:C  ALL(NP)    T    C            7/111/386   0.2202   0.2173            1\n1      1:86331:A:G  ALL(NP)    G    A             0/88/416   0.1746   0.1594      0.02387\n   1      1:91581:G:A  ALL(NP)    A    G          137/228/139   0.4524      0.5      0.03271\n   1     1:122872:T:G  ALL(NP)    G    T            1/259/244   0.5139   0.3838     8.04e-19\n   1     1:135163:C:T  ALL(NP)    T    C             1/91/412   0.1806   0.1675       0.1066\n</code></pre></p>"},{"location":"04_Data_QC/#applying-filters","title":"Applying filters","text":"<p>Previously we just calculate the basic statistics using PLINK. But when performing certain analyses, we just want to exclude the bad-quality samples or SNPs instead of calculating the statistics for all samples and SNPs.</p> <p>In this case we can apply the following filters for example:</p> <ul> <li><code>--maf 0.01</code> : exlcude snps with maf&lt;0.01</li> <li><code>--geno 0.01</code> :filters out all variants with missing rates exceeding 0.0</li> <li><code>--mind 0.02</code> :filters out all samples with missing rates exceeding 0.02</li> <li><code>--hwe 5e-6</code> : filters out all variants which have Hardy-Weinberg equilibrium exact test p-value below the provided threshold. NOTE: With case/control data, cases and missing phenotypes are normally ignored. (see https://www.cog-genomics.org/plink/1.9/filter#hwe)</li> </ul>"},{"location":"04_Data_QC/#ld-pruning","title":"LD Pruning","text":"<p>There is often strong Linkage disequilibrium(LD) among SNPs, for some analysis we don't need all SNPs and we need to remove the redundant SNPs to avoid bias in genetic estimations. For example, for relatedness estimation, we will use only LD-Pruned SNP set. </p> <p>We can use <code>--indep-pairwise 50 5 0.2</code> to filter out those in strong LD and keep only the independent SNPs. Please check https://www.cog-genomics.org/plink/1.9/ld#indep for the meaning of each parameter. Combined with the filters we just introduced, we can run:</p> <p>Example</p> <p><pre><code>plink \\\n--bfile ${genotypeFile} \\\n--maf 0.01 \\\n--geno 0.01 \\\n--mind 0.02 \\\n--hwe 5e-6 \\\n--indep-pairwise 50 5 0.2 \\\n--out plink_results\n</code></pre> This command generates two outputs:  <code>plink_results.prune.in</code> and <code>plink_results.prune.out</code> <code>plink_results.prune.in</code> is the independent set of SNPs we will use in the following analysis. Let's take a look at this file. Basically, it just contains one SNP id per line.</p> <pre><code>head plink_results.prune.in\n1:13273:G:C\n1:14599:T:A\n1:69897:T:C\n1:86331:A:G\n1:91581:G:A\n1:135163:C:T\n1:233473:C:G\n1:532929:T:C\n1:559480:C:T\n1:565433:C:T\n</code></pre>"},{"location":"04_Data_QC/#sample-snp-filtering-extractexcludekeepremove","title":"Sample &amp; SNP filtering (extract/exclude/keep/remove)","text":"<p>Sometimes we will use only a subset of samples or SNPs included the original dataset.  In this case, we can use <code>--extract</code> or <code>--exclude</code> to select or exclude SNPs from analysis, <code>--keep</code> or <code>--remove</code> to select or exclude samples.</p> <p>For  <code>--keep</code> or <code>--remove</code> , the input is the filename of a sample FID and IID file. For <code>--extract</code> or <code>--exclude</code> , the input is the filename of a SNP list file.</p> <pre><code>head plink_results.prune.in\n1:13273:G:C\n1:14599:T:A\n1:69897:T:C\n1:86331:A:G\n1:91581:G:A\n1:135163:C:T\n1:233473:C:G\n1:532929:T:C\n1:559480:C:T\n1:565433:C:T\n</code></pre>"},{"location":"04_Data_QC/#ibd-pi_hat","title":"IBD / PI_HAT","text":"<p><code>--genome</code> will estimate IBS/IBD. Usually, for this analysis, we need to prune our data first since the strong LD will cause bias in the results. (This step is computationally intensive)</p> <p>Combined with the <code>--extract</code>, we can run:</p> <p>How PLINK extimates IBD</p> <p>The prior probability of IBS sharing can be modeled as: </p> \\[P(I=i) = \\sum^{z=i}_{z=0}P(I=i|Z=z)P(Z=z)\\] <ul> <li>I: IBS state (I = 0, 1, or 2)</li> <li>Z: IBD state (Z = 0, 1, or 2)</li> <li>\\(P(I|Z)\\) is a function of allele frequency. PLINK will average over all SNPs to obtain the expected value for \\(P(I|Z)\\).</li> </ul> <p>So the proportion of alleles shared IBD (\\(\\hat{\\pi}\\)) can be estimated by:</p> \\[\\hat{\\pi} = {{P(Z=1)}\\over{2}} + P(Z=2)\\] <p>Estimate IBD</p> <pre><code>plink \\\n--bfile ${genotypeFile} \\\n--extract plink_results.prune.in \\\n--genome \\\n--out plink_results\n</code></pre> <p>PI_HAT is the IBD estimation. Please check https://www.cog-genomics.org/plink/1.9/ibd for more details. <pre><code>head plink_results.genome\n FID1     IID1 FID2     IID2 RT    EZ      Z0      Z1      Z2  PI_HAT PHE       DST     PPC   RATIO\n   0  HG00403   0  HG00404 OT     0  0.9800  0.0086  0.0114  0.0157  -1  0.749375  0.5531  2.0089\n   0  HG00403   0  HG00406 OT     0  0.9751  0.0231  0.0018  0.0133  -1  0.748336  0.6309  2.0225\n   0  HG00403   0  HG00407 OT     0  0.9801  0.0153  0.0046  0.0122  -1  0.748293  0.5045  2.0007\n   0  HG00403   0  HG00409 OT     0  0.9807  0.0193  0.0000  0.0097  -1  0.747215  0.6014  2.0173\n   0  HG00403   0  HG00410 OT     0  0.9744  0.0256  0.0000  0.0128  -1  0.748058  0.8621  2.0745\n   0  HG00403   0  HG00419 OT     0  0.9650  0.0350  0.0000  0.0175  -1  0.747146  0.8080  2.0595\n   0  HG00403   0  HG00421 OT     0  0.9842  0.0158  0.0000  0.0079  -1  0.746522  0.4557  1.9926\n   0  HG00403   0  HG00422 OT     0  0.9832  0.0111  0.0056  0.0112  -1  0.748156  0.4208  1.9867\n   0  HG00403   0  HG00428 OT     0  0.9929  0.0000  0.0071  0.0071  -1  0.746851  0.3115  1.9674\n</code></pre></p>"},{"location":"04_Data_QC/#ld-calculation","title":"LD calculation","text":"<p>We can also use our data to estimate the LD between a pair of SNPs.</p> <p>Details on LD can be found here</p> <p><code>--chr</code> option in PLINK allows us to include SNPs on a specific chromosome. To calculate LD r2 for SNPs on chr22 , we can run:</p> <p>Example</p> <pre><code>plink \\\n--bfile ${genotypeFile} \\\n--chr 22 \\\n--r2 \\\n--out plink_results\n</code></pre> <pre><code>head plink_results.ld\n CHR_A         BP_A             SNP_A  CHR_B         BP_B             SNP_B           R2 22     16053659   22:16053659:A:C     22     16067500   22:16067500:T:C      0.64577 22     16053862   22:16053862:C:T     22     16054454   22:16054454:C:T     0.987505 22     16053862   22:16053862:C:T     22     16055122   22:16055122:G:T     0.601708 22     16053863   22:16053863:G:A     22     16055070   22:16055070:G:A     0.931043 22     16054454   22:16054454:C:T     22     16055122   22:16055122:G:T     0.594216 22     16058767   22:16058767:A:G     22     16067462   22:16067462:C:T     0.266916 22     16067462   22:16067462:C:T     22     16069771   22:16069771:G:A     0.382323 22     16069771   22:16069771:G:A     22     16123813   22:16123813:G:A     0.291935 22     16143946   22:16143946:A:G     22     16155259   22:16155259:A:G     0.559165 </code></pre>"},{"location":"04_Data_QC/#data-management-make-bedrecode","title":"Data management (make-bed/recode)","text":"<p>By far the input data we use is in binary form, but sometimes we may want the text version.</p> <p>Info</p> <p></p> <p>To convert the formats, we can run:</p> <p>Convert PLINK formats</p> <pre><code>#extract the 1000 samples with the pruned SNPs, and make a bed file.\nplink \\\n--bfile ${genotypeFile} \\\n--extract plink_results.prune.in \\\n--make-bed \\\n--out plink_1000_pruned\n\n#convert the bed/bim/fam to ped/map\nplink \\\n--bfile plink_1000_pruned \\\n--recode \\\n--out plink_1000_pruned\n</code></pre>"},{"location":"04_Data_QC/#exercise","title":"Exercise","text":"<ul> <li> Follow this tutorial and type in the commands:</li> <li> Calculate basic statistics for the simulated data.</li> <li> <p> Learn the meaning of each QC step.</p> </li> <li> <p> Visualize the results of QC (using Python or R)</p> </li> <li> Draw the distribution of MAF.(histogram)</li> <li> Draw the distribution of het.(histogram)</li> <li> Try to briefly explain what you observe</li> </ul>"},{"location":"04_Data_QC/#additional-resources","title":"Additional resources","text":"<ul> <li>Marees, A. T., de Kluiver, H., Stringer, S., Vorspan, F., Curis, E., Marie\u2010Claire, C., &amp; Derks, E. M. (2018). A tutorial on conducting genome\u2010wide association studies: Quality control and statistical analysis. International journal of methods in psychiatric research, 27(2), e1608.</li> </ul>"},{"location":"04_Data_QC/#reference","title":"Reference","text":"<ul> <li>Purcell, S., Neale, B., Todd-Brown, K., Thomas, L., Ferreira, M. A., Bender, D., ... &amp; Sham, P. C. (2007). PLINK: a tool set for whole-genome association and population-based linkage analyses. The American journal of human genetics, 81(3), 559-575.</li> <li>Chang, C. C., Chow, C. C., Tellier, L. C., Vattikuti, S., Purcell, S. M., &amp; Lee, J. J. (2015). Second-generation PLINK: rising to the challenge of larger and richer datasets. Gigascience, 4(1), s13742-015.</li> </ul>"},{"location":"05_PCA/","title":"Principle component analysis (PCA)","text":"<p>PCA aims to find the orthogonal directions of maximum variance and project the data onto a new subspace with equal or fewer dimensions than the original one. Simply speaking, GRM (genetic relationship matrix; covariance matrix) is first estimated and then PCA is applied to this matrix to generate eigenvectors and eigenvalues. Finally, the \\(k\\) eigenvectors with the largest eigenvalues are used to transform the genotypes to a new feature subspace.</p> <p>Genetic relationship matrix (GRM)</p> <p></p> <p>Citation: Yang, J., Lee, S. H., Goddard, M. E., &amp; Visscher, P. M. (2011). GCTA: a tool for genome-wide complex trait analysis. The American Journal of Human Genetics, 88(1), 76-82.</p> <p>A simple PCA</p> <p>Source data: <pre><code>cov = np.array([[6, -3], [-3, 3.5]])\npts = np.random.multivariate_normal([0, 0], cov, size=800)\n</code></pre></p> <p>The red arrow shows the first principal component axis (PC1) and the blue arrow shows the second principal component axis (PC2). The two axes are orthogonal.</p> <p></p> <p>Interpretation of PCs</p> <p>The first principal component of a set of p variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through p iterations until all the variance is explained.</p> <p>PCA is by far the most commonly used dimension reduction approach used in population genetics which could identify the difference in ancestry among the sample individuals. The population outliers could be excluded from the main cluster. For GWAS we also need to include top PCs to adjust for the population stratification.</p> <p>Please read the following paper on how we apply PCA to genetic data: Price, A., Patterson, N., Plenge, R. et al. Principal components analysis corrects for stratification in genome-wide association studies. Nat Genet 38, 904\u2013909 (2006). https://doi.org/10.1038/ng1847 https://www.nature.com/articles/ng1847</p> <p>So before association analysis, we will learn how to run PCA analysis first.</p> <ul> <li>Preparation</li> <li>PCA steps</li> <li>Sample codes</li> <li>Plotting the PCs</li> <li>PCA-UMAP</li> <li>References</li> </ul>"},{"location":"05_PCA/#preparation","title":"Preparation","text":""},{"location":"05_PCA/#exclude-snps-in-high-ld-or-hla-regions","title":"Exclude SNPs in high-LD or HLA regions","text":"<p>For PCA, we first exclude SNPs in high-LD or HLA regions from the genotype data. </p> <p>The reason why we want to exclude such high-LD or HLA regions</p> <ul> <li>Price, A. L., Weale, M. E., Patterson, N., Myers, S. R., Need, A. C., Shianna, K. V., Ge, D., Rotter, J. I., Torres, E., Taylor, K. D., Goldstein, D. B., &amp; Reich, D. (2008). Long-range LD can confound genome scans in admixed populations. American journal of human genetics, 83(1), 132\u2013139. https://doi.org/10.1016/j.ajhg.2008.06.005 </li> </ul>"},{"location":"05_PCA/#download-bed-like-files-for-high-ld-or-hla-regions","title":"Download BED-like files for high-LD or HLA regions","text":"<p>You can simply copy the list of high-LD or HLA regions in Genome build version(.bed format) to a text file <code>high-ld.txt</code>. </p> <p>High LD regions were obtained from</p> <p>https://genome.sph.umich.edu/wiki/Regions_of_high_linkage_disequilibrium_(LD)</p> <p>High LD regions of hg19</p> high-ld-hg19.txt<pre><code>1   48000000    52000000    highld\n2   86000000    100500000   highld\n2   134500000   138000000   highld\n2   183000000   190000000   highld\n3   47500000    50000000    highld\n3   83500000    87000000    highld\n3   89000000    97500000    highld\n5   44500000    50500000    highld\n5   98000000    100500000   highld\n5   129000000   132000000   highld\n5   135500000   138500000   highld\n6   25000000    35000000    highld\n6   57000000    64000000    highld\n6   140000000   142500000   highld\n7   55000000    66000000    highld\n8   7000000 13000000    highld\n8   43000000    50000000    highld\n8   112000000   115000000   highld\n10  37000000    43000000    highld\n11  46000000    57000000    highld\n11  87500000    90500000    highld\n12  33000000    40000000    highld\n12  109500000   112000000   highld\n20  32000000    34500000    highld\n</code></pre>"},{"location":"05_PCA/#create-a-list-of-snps-in-high-ld-or-hla-regions","title":"Create a list of SNPs in high-LD or HLA regions","text":"<p>Next, use <code>high-ld.txt</code> to extract all SNPs which are located in the regions described in the file using the code as follows:</p> <pre><code>plink --file ${plinkFile} --make-set high-ld.txt --write-set --out hild\n</code></pre> <p>Create a list of SNPs in the regions specified in <code>high-ld.txt</code> </p> <pre><code>plinkFile=\"../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\" #! Please set this to your own path\n\nplink \\\n    --bfile ${plinkFile} \\\n    --make-set high-ld-hg19.txt \\\n    --write-set \\\n    --out hild\n</code></pre> <p>And all SNPs in the regions will be extracted to hild.set.</p> <pre><code>$head hild.set\nhighld\n1:48000156:C:G\n1:48002096:C:G\n1:48003081:T:C\n1:48004776:C:T\n1:48006500:A:G\n1:48006546:C:T\n1:48008102:T:G\n1:48009994:C:T\n1:48009997:C:A\n</code></pre> <p>For downstream analysis, we can exclude these SNPs using <code>--exclude hild.set</code>.</p>"},{"location":"05_PCA/#pca-steps","title":"PCA steps","text":"<p>Steps to perform a typical genomic PCA analysis</p> <ul> <li> <ol> <li>LD-Pruning (https://www.cog-genomics.org/plink/2.0/ld#indep)</li> </ol> </li> <li> <ol> <li>Removing relatives from calculating PCs (usually 2-degree) (https://www.cog-genomics.org/plink/2.0/distance#king_cutoff)</li> </ol> </li> <li> <ol> <li>Running PCA using un-related samples and independent SNPs (https://www.cog-genomics.org/plink/2.0/strat#pca)</li> </ol> </li> <li> <ol> <li>Projecting to all samples (https://www.cog-genomics.org/plink/2.0/score#pca_project)</li> </ol> </li> </ul> <p>MAF filter for LD-pruning and PCA</p> <p>For LD-pruning and PCA, we usually only use variants with MAF &gt; 0.01 or MAF&gt;0.05. Since the sample dataset only contains variants with MAF &gt; 0.05. We will skip the MAF filtering here. But please do keep this in mind when you work with your own datasets. (You can simply add <code>--maf 0.01</code> or <code>--maf 0.05</code> when performing LD-pruning or PCA.)</p>"},{"location":"05_PCA/#sample-codes","title":"Sample codes","text":"<p>Sample codes for performing PCA</p> <pre><code>plinkFile=\"\" #please set this to your own path\noutPrefix=\"plink_results\"\nthreadnum=2\nhildset = hild.set \n\n# LD-pruning, excluding high-LD and HLA regions\nplink2 \\\n        --bfile ${plinkFile} \\\n        --threads ${threadnum} \\\n        --exclude ${hildset} \\ \n        --indep-pairwise 500 50 0.2 \\\n        --out ${outPrefix}\n\n# Remove related samples using king-cuttoff\nplink2 \\\n        --bfile ${plinkFile} \\\n        --extract ${outPrefix}.prune.in \\\n        --king-cutoff 0.0884 \\\n        --threads ${threadnum} \\\n        --out ${outPrefix}\n\n# PCA after pruning and removing related samples\nplink2 \\\n        --bfile ${plinkFile} \\\n        --keep ${outPrefix}.king.cutoff.in.id \\\n        --extract ${outPrefix}.prune.in \\\n        --freq counts \\\n        --threads ${threadnum} \\\n        --pca approx allele-wts 10 \\\n        --out ${outPrefix}\n\n# Projection (related and unrelated samples)\nplink2 \\\n        --bfile ${plinkFile} \\\n        --threads ${threadnum} \\\n        --read-freq ${outPrefix}.acount \\\n        --score ${outPrefix}.eigenvec.allele 2 5 header-read no-mean-imputation variance-standardize \\\n        --score-col-nums 6-15 \\\n        --out ${outPrefix}_projected\n</code></pre> <p>After step 3, the <code>allele-wts 10</code> modifier requests an additional one-line-per-allele <code>.eigenvec.allele</code> file with the first <code>10 PCs</code> expressed as allele weights instead of sample weights.</p> <p>We will get the <code>plink_results.eigenvec.allele</code> file, which will be used to project onto all samples along with an allele count <code>plink_results.acount</code> file.</p> <p>In the projection, <code>score ${outPrefix}.eigenvec.allele 2 5</code> sets the <code>ID</code> (2nd column) and <code>A1</code> (5th column), <code>score-col-nums 6-15</code> sets the first 10 PCs to be projected.</p> <p>Please check https://www.cog-genomics.org/plink/2.0/score#pca_project for more details on the projection.</p> <p>Allele weight and count files</p> plink_results.eigenvec.allele<pre><code>#CHROM  ID  REF ALT A1  PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10\n1   1:13273:G:C G   C   G   1.12369 -0.320826   -0.0206569  -0.218665   0.869801    0.378433    -0.0723841  -0.227555   0.0361673   -0.368192\n1   1:13273:G:C G   C   C   -1.12369    0.320826    0.0206569   0.218665    -0.869801   -0.378433   0.0723841   0.227555    -0.0361673  0.368192\n1   1:14599:T:A T   A   T   0.99902 -1.15824    -1.80519    -0.36774    0.179881    0.25242 0.068899    0.206564    -0.342483   0.103762\n1   1:14599:T:A T   A   A   -0.99902    1.15824 1.80519 0.36774 -0.179881   -0.25242    -0.068899   -0.206564   0.342483    -0.103762\n1   1:14930:A:G A   G   A   -0.0704343  -0.35091    -0.41535    -0.304856   0.081039    -0.49408    -0.0667606  -0.0698847  0.245836    0.330869\n1   1:14930:A:G A   G   G   0.0704343   0.35091 0.41535 0.304856    -0.081039   0.49408 0.0667606   0.0698847   -0.245836   -0.330869\n1   1:69897:T:C T   C   T   -0.514024   0.563153    -0.997768   -0.298234   -0.840608   -0.247155   0.545471    -0.675274   -0.787836   -0.509647\n1   1:69897:T:C T   C   C   0.514024    -0.563153   0.997768    0.298234    0.840608    0.247155    -0.545471   0.675274    0.787836    0.509647\n1   1:86331:A:G A   G   A   -0.169641   -0.0125126  -0.531174   -0.0219291  0.614439    0.140143    0.133833    -0.570109   0.392805    -0.065334\n</code></pre> plink_results.acount<pre><code>#CHROM  ID  REF ALT ALT_CTS OBS_CT\n1   1:13273:G:C G   C   63  1004\n1   1:14599:T:A T   A   90  1004\n1   1:14930:A:G A   G   417 1004\n1   1:69897:T:C T   C   879 1004\n1   1:86331:A:G A   G   87  1004\n1   1:91581:G:A G   A   499 1004\n1   1:122872:T:G    T   G   259 1004\n1   1:135163:C:T    C   T   91  1004\n1   1:233473:C:G    C   G   156 1004\n</code></pre> <p>Eventually, we will get the PCA results for all samples.</p> <p>PCA results for all samples</p> plink_results_projected.sscore<pre><code>#FID    IID ALLELE_CT   NAMED_ALLELE_DOSAGE_SUM PC1_AVG PC2_AVG PC3_AVG PC4_AVG PC5_AVG PC6_AVG PC7_AVG PC8_AVG PC9_AVG PC10_AVG\n0   HG00403 219504  219504  0.000643981 -0.0297502  -0.0151499  -0.0122381  0.0229149   0.0235408   -0.033705   -0.0075127  -0.0125402  0.00271677\n0   HG00404 219504  219504  -0.000492225    -0.031018   -0.00764244 -0.0204998  0.0284068   -0.00872449 0.0123353   -0.00492058 -0.00557003 0.0248966\n0   HG00406 219504  219504  0.00620984  -0.034375   -0.00898555 -0.00335076 -0.0217559  -0.0182433  0.00333925  -0.00760613 -0.0340018  0.00641082\n0   HG00407 219504  219504  0.00678586  -0.0239308  -0.00704419 -0.00466139 0.00985433  0.000889767 0.00679557  -0.0200495  -0.0131869  0.0350328\n0   HG00409 219504  219504  -0.00236345 -0.0231604  0.0320665   0.0145563   0.0236768   0.00704788  0.012859    0.0319605   -0.0130627  0.0110219\n0   HG00410 219504  219504  0.000670927 -0.0210665  0.0467767   0.00293079  0.0184061   0.045967    0.00384994  0.0212317   -0.0296434  0.0237174\n0   HG00419 219504  219504  0.00526139  -0.0369818  -0.00974662 0.00855412  -0.0053907  -0.00102057 0.0063254   0.0140126   -0.00600854 0.00732882\n0   HG00421 219504  219504  0.00038356  -0.0319534  -0.00648054 0.00311739  -0.022044   0.0064945   -0.0105273  -0.0276718  -0.00973368 0.0208449\n0   HG00422 219504  219504  0.00437335  -0.0323416  -0.0111979  0.0106245   -0.0267334  0.00142919  -0.00487295 -0.0124099  -0.00467014 -0.0188086\n</code></pre>"},{"location":"05_PCA/#plotting-the-pcs","title":"Plotting the PCs","text":"<p>You can now create scatterplots of the PCs using R or Python.</p> <p>For plotting using Python: plot_PCA.ipynb</p> <p>Scatter plot of PC1 and PC2 using 1KG EAS individuals</p> <p></p> <p>Note : We only used 20% of all available variants. This figure only very roughly shows the population structure in East Asia.</p> <p>Requirements: - python&gt;3 - numpy,pandas,seaborn,matplotlib</p>"},{"location":"05_PCA/#pca-umap","title":"PCA-UMAP","text":"<p>(optional)  We can also apply another non-linear dimension reduction algorithm called UMAP to the PCs to further identfy the local structures. (PCA-UMAP)</p> <p>For more details, please check: - https://umap-learn.readthedocs.io/en/latest/index.html</p> <p>An example of PCA and PCA-UMAP for population genetics: - Sakaue, S., Hirata, J., Kanai, M., Suzuki, K., Akiyama, M., Lai Too, C., ... &amp; Okada, Y. (2020). Dimensionality reduction reveals fine-scale structure in the Japanese population with consequences for polygenic risk prediction. Nature communications, 11(1), 1-11.</p>"},{"location":"05_PCA/#references","title":"References","text":"<ul> <li>(PCA) Price, A., Patterson, N., Plenge, R. et al. Principal components analysis corrects for stratification in genome-wide association studies. Nat Genet 38, 904\u2013909 (2006). https://doi.org/10.1038/ng1847 https://www.nature.com/articles/ng1847</li> <li>(why removing high-LD regions) Price, A. L., Weale, M. E., Patterson, N., Myers, S. R., Need, A. C., Shianna, K. V., Ge, D., Rotter, J. I., Torres, E., Taylor, K. D., Goldstein, D. B., &amp; Reich, D. (2008). Long-range LD can confound genome scans in admixed populations. American journal of human genetics, 83(1), 132\u2013139. https://doi.org/10.1016/j.ajhg.2008.06.005 </li> <li>(UMAP) McInnes, L., Healy, J., &amp; Melville, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.</li> <li>(UMAP in population genetics) Diaz-Papkovich, A., Anderson-Trocm\u00e9, L. &amp; Gravel, S. A review of UMAP in population genetics. J Hum Genet 66, 85\u201391 (2021). https://doi.org/10.1038/s10038-020-00851-4 https://www.nature.com/articles/s10038-020-00851-4</li> <li>(king-cutoff) Manichaikul, A., Mychaleckyj, J. C., Rich, S. S., Daly, K., Sale, M., &amp; Chen, W. M. (2010). Robust relationship inference in genome-wide association studies. Bioinformatics, 26(22), 2867-2873.</li> </ul>"},{"location":"06_Association_tests/","title":"Association test","text":""},{"location":"06_Association_tests/#overview","title":"Overview","text":""},{"location":"06_Association_tests/#genetic-models","title":"Genetic models","text":"<p>To test the association between a phenotype and genotypes, we need to group the genotypes based on genetic models.</p> <p>There are three basic genetic models:</p> <ul> <li>Additive model (ADD)</li> <li>Dominant model (DOM)</li> <li>Recessive model (REC)</li> </ul> <p>Three genetic models</p> <p>For example, suppose we have a biallelic SNP whose reference allele is A and alternative allele is G.</p> <p>There are three possible genotypes for this SNP: AA, AG, and GG.</p> <p>The table shows how we group different genotypes under each genetic model.</p> Genetic models AA AG GG Additive model 0 1 2 Dominant model 0 1 1 Recessive model 0 0 1 <p>Contingency table and non-parametric tests</p> <p>A simple way to test association is to use the 2x2 or 2x3 contingency table. For dominant and recessive models,  Chi-square tests are performed using the 2x2 table. For the additive model, Cochran-Armitage trend tests are performed for the 2x3 table. However, the non-parametric tests do not adjust for the bias caused by other covariates like sex, age and so forth.</p> <p></p>"},{"location":"06_Association_tests/#association-testing-basics","title":"Association testing basics","text":"<p>For quantitative traits, we can employ a simple linear regression model to test associations:</p> \\[  y = G\\beta_G + X\\beta_X + e \\] <ul> <li>\\(G\\) is the genotype matrix.</li> <li>\\(\\beta_G\\) is the effect size for variants.</li> <li>\\(X\\) and \\(\\beta_X\\) are covariates and their effects.</li> <li>\\(e\\) is the error term.</li> </ul> <p>Interpretation of linear regression</p> <p></p> <p>For binary traits, we can utilize the logistic regression model to test associations:</p> \\[  logit(p) = G\\beta_G + X\\beta_X + e \\] <p>Linear regression and logistic regression</p> <p></p>"},{"location":"06_Association_tests/#file-preparation","title":"File Preparation","text":"<p>To perform genome-wide association tests, usually we need the following files:</p> <ul> <li>Genotype file (or dosage file) : usually in PLINK format, VCF format, or BGEN format.</li> <li>Phenotype file : plain text file.</li> <li>Covariate file (optional): plain text file. Usually covariates include age, sex, and top Principal Components. </li> </ul> <p>Phenotype and covariate files</p> <p>Phenotype file for a simulated binary trait; B1 is the phenotype name; 1 means the control, 2 means the case.</p> 1kgeas_binary.txt<pre><code>FID IID B1\n0 HG00403 2 \n0 HG00404 1 \n0 HG00406 2 \n0 HG00407 2 \n0 HG00409 2 \n0 HG00410 2\n0 HG00419 1 \n0 HG00421 2 \n0 HG00422 1\n\nCovariate file (only top PCs calculated in the previous PCA section)\n\n```txt title=\"plink_results_projected.sscore\"\n#FID    IID ALLELE_CT   NAMED_ALLELE_DOSAGE_SUM PC1_AVG PC2_AVG PC3_AVG PC4_AVG PC5_AVG PC6_AVG     PC7_AVG PC8_AVG PC9_AVG PC10_AVG\n0   HG00403 224016  224016  0.000246109 0.0292717   -0.0127437  -0.0135105  0.0301317   0.    0196699   0.0232392   -0.0205941  -0.00416543 0.0121819\n0   HG00404 224016  224016  -0.000716664    0.032043    -0.00573731 -0.0189504  0.0277385   -0.    0154478  -0.0136551  -0.00147269 0.00510851  0.0268694\n0   HG00406 224016  224016  0.00681476  0.0346759   -0.00706221 -0.0054266  -0.025458   -0.    0166505  -0.00510707 -0.00105581 -0.0224028  0.0202523\n0   HG00407 224016  224016  0.00695106  0.0244759   -0.00890072 -0.000694057    0.00946838  -0.    00773378 -0.0139923  -0.0204871  -0.0003338  0.0387022\n0   HG00409 224016  224016  -0.0023481  0.0213108   0.03235 0.0158793   0.026655    0.    00324455-0.0107152    0.0317714   -0.00764455 0.0155973\n0   HG00410 224016  224016  0.000926147 0.0198139   0.0475798   0.00314974  0.0275373   0.    041886    -0.0133896  0.0184717   -0.0143644  0.0291036\n0   HG00419 224016  224016  0.00580767  0.0369208   -0.00907507 0.00903163  -0.00649345 -0.    000472359    -0.00327011 0.0160456   -0.005133   0.0141021\n0   HG00421 224016  224016  0.000987901 0.0336895   -0.00697814 0.00557199  -0.0210537  0.    00700968  0.00319921  -0.0215999  0.00127686  0.0350116\n0   HG00422 224016  224016  0.00440288  0.0335901   -0.0125043  0.0135621   -0.0228428  0.    00492741  0.00445856  -0.00911147 -0.00312742 -0.00784459\n</code></pre>"},{"location":"06_Association_tests/#association-tests-using-plink","title":"Association tests using PLINK","text":"<p>Please check https://www.cog-genomics.org/plink/2.0/assoc for more details.</p> <p>We will perform logistic regression with firth correction for a simulated binary trait under the additive model using the 1KG East Asian individuals.</p> <p>Firth correction</p> <p>Adding a penalty term to the log-likelihood function when fitting the logistic model, which results in less bias. - Firth, David. \"Bias reduction of maximum likelihood estimates.\" Biometrika 80.1 (1993): 27-38.</p> <p>Quantitative traits</p> <p>For quantitative traits, linear regressions will be performed and in this case, we do not need to add <code>firth</code> (since Firth correction is not appliable). </p> <p>Sample codes for association test using plink for binary traits</p> <pre><code>genotypeFile=\"../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\"  #!please set this to your own path\nphenotypeFile=\"../01_Dataset/1kgeas_binary.txt\" #!please set this to your own path\ncovariateFile=\"../05_PCA/plink_results_projected.sscore\"\n\ncovariateCols=6-10\ncolName=\"B1\"\nthreadnum=2\n\nplink2 \\\n    --bfile ${genotypeFile} \\\n    --pheno ${phenotypeFile} \\\n    --pheno-name ${colName} \\\n    --maf 0.01 \\\n    --covar ${covariateFile} \\\n    --covar-col-nums ${covariateCols} \\\n    --glm hide-covar firth  firth-residualize single-prec-cc \\\n    --threads ${threadnum} \\\n    --out 1kgeas\n</code></pre> <p>Note</p> <p>Using the latest version of PLINK2, you need to add <code>firth-residualize single-prec-cc</code> to generate the results. (The algorithm and precision have been changed since 2023 for firth regression)</p> <p>You will see a similar log like:</p> <p>Log</p> 1kgeas.log<pre><code>PLINK v2.00a3LM 64-bit Intel (12 Dec 2020)     www.cog-genomics.org/plink/2.0/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\nLogging to 1kgeas.log.\nOptions in effect:\n  --bfile ../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\n  --covar ../05_PCA/plink_results_projected.sscore\n  --covar-col-nums 6-10\n  --glm hide-covar firth\n  --maf 0.01\n  --out 1kgeas\n  --pheno ../01_Dataset/1kgeas_binary.txt\n  --pheno-name B1\n  --threads 2\n\nStart time: Tue Dec 27 23:02:52 2022\n15957 MiB RAM detected; reserving 7978 MiB for main workspace.\nUsing up to 2 compute threads.\n504 samples (0 females, 0 males, 504 ambiguous; 504 founders) loaded from\n../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam.\n1122299 variants loaded from\n../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim.\n1 binary phenotype loaded (201 cases, 302 controls).\n5 covariates loaded from ../05_PCA/plink_results_projected.sscore.\nCalculating allele frequencies... done.\n0 variants removed due to allele frequency threshold(s)\n(--maf/--max-maf/--mac/--max-mac).\n1122299 variants remaining after main filters.\n--glm Firth regression on phenotype 'B1': done.\nResults written to 1kgeas.B1.glm.firth .\nEnd time: Tue Dec 27 23:04:40 2022\n</code></pre> <p>Let's check the first lines of the output:</p> <p>Association test results</p> 1kgeas.B1.glm.firth<pre><code>#CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1:13273:G:C G   C   C   ADD 503 0.746149    0.282904    -1.03509    0.300628    .\n1   14599   1:14599:T:A T   A   A   ADD 503 1.67693 0.240899    2.14598 0.0318742.\n1   14604   1:14604:A:G A   G   G   ADD 503 1.67693 0.240899    2.14598 0.0318742.\n1   14930   1:14930:A:G A   G   G   ADD 503 1.64359 0.242872    2.04585 0.0407708.\n1   69897   1:69897:T:C T   C   T   ADD 503 1.69142 0.200238    2.62471 0.00867216.\n1   86331   1:86331:A:G A   G   G   ADD 503 1.41887 0.238055    1.46968 0.141649    .\n1   91581   1:91581:G:A G   A   A   ADD 503 0.931304    0.123644    -0.5755980.564887   .\n1   122872  1:122872:T:G    T   G   G   ADD 503 1.04828 0.182036    0.259034    0.795609    .\n1   135163  1:135163:C:T    C   T   T   ADD 503 0.676666    0.242611    -1.60989    0.    107422    .\n</code></pre> <p>Usually, other options are added to enhance the sumstats</p> <ul> <li>--keep xxx/kiso2021/for_plink2/unrelated.sample.id    # Because the standard linear     regression does not account for the relatedness, the kinship-pruned samples in last steps are suggested.</li> <li>--mach-r2-filter 0.7 2.0  # It allows to use only the variants passed an (MaCH)Rsq filter.  NOTE: when pgen file is used, the upper boundary should be 2.</li> <li>--glm cols=+a1freq,+machr2 firth-fallback omit-ref    # The <code>cols=</code> requests the  following columns in the sumstats: here are allele1 frequency and (MaCH)Rsq, <code>firth-fallback</code>     will test the common variants without firth correction, which could improve the speed,     <code>omit-ref</code> will force the ALT==A1==effect allele, otherwise the minor allele would be tested     (see the above result, which ALT may not equal A1).</li> <li>--covar-variance-standardize  # To normalize the covariates which may at a huge scale, like     AGE**AGE.</li> <li>--covar-name AGE SEX PC1-PC20 # Instead of setting the index of columns, directly specify the     column name.</li> </ul>"},{"location":"06_Association_tests/#genomic-control","title":"Genomic control","text":"<p>Genomic control (GC) is a basic method for controlling for confounding factors including population stratification.</p> <p>We will calculate the genomic control factor (lambda GC) to evaluate the inflation. The genomic control factor is calculated by dividing the median of observed Chi square statistics by the median of Chi square distribution with the degree of freedom being 1 (which is approximately 0.455).</p> \\[  \\lambda_{GC} = {median(\\chi^{2}_{observed}) \\over median(\\chi^{2}_1)}  \\] <p>Then, we can used the genomic control factor to correct observed Chi suqare statistics.</p> \\[  \\chi^{2}_{corrected} = {\\chi^{2}_{observed} \\over \\lambda_{GC}}  \\] <p>Genomic inflation is based on the idea that most of the variants are not associated, thus no deviation between the observed and expected Chi square distribution, except the spikes at the end. However, if the trait is highly polygenic, this assumption may be violated.</p> <p>Reference: Devlin, B., &amp; Roeder, K. (1999). Genomic control for association studies. Biometrics, 55(4), 997-1004.</p>"},{"location":"06_Association_tests/#significant-loci","title":"Significant loci","text":"<p>Please check Visualization using gwaslab</p> <p>Loci that reached suggestive significance threhold (P value &lt; 5e-6) : <pre><code>SNPID   CHR POS EA  NEA SE  Z   P   OR  N   STATUS\n1:217437563:C:T 1   217437563   C   T   0.151157    -5.22793    1.714210e-07    0.453736    503 9999999\n2:55574452:G:C  2   55574452    C   G   0.160948    -5.98392    2.178320e-09    0.381707    503 9999999\n3:176524872:C:T 3   176524872   T   C   0.248418    4.92774 8.318440e-07    3.401240    503 9999999\n3:193128900:G:A 3   193128900   A   G   0.153788    4.70811 2.500290e-06    2.062770    503 9999999\n6:29919659:T:C  6   29919659    T   C   0.155457    -5.89341    3.782970e-09    0.400048    503 9999999\n9:36660672:A:G  9   36660672    G   A   0.160275    5.63422 1.758540e-08    2.467060    503 9999999\n11:56249438:A:G 11  56249438    G   A   0.188891    -4.77836    1.767350e-06    0.405518    503 9999999\n</code></pre></p>"},{"location":"06_Association_tests/#visualization","title":"Visualization","text":"<p>To visualize the sumstats, we will create the Manhattan plot, QQ plot and regional plot.</p> <p>Please check for codes : Visualization using gwaslab</p> <p></p>"},{"location":"06_Association_tests/#manhattan-plot","title":"Manhattan plot","text":"<p>Manhattan plot is the most classic visualization of GWAS summary statistics. It is a form of scatter plot. Each dot represents the test result for a variant. variants are sorted by its genome coordinates and are aligned along the X axis. Y axis shows the -log10(P value) for tests of variants in GWAS. </p> <p>Note</p> <p>This kind of plot was named after Manhattan in New York City since it resembles the Manhattan skyline.   </p> <p>A real Manhattan plot</p> <p> I took this photo in 2020 just before the COVID-19 pandemic. It was a cloudy and misty day. Those birds formed a significance threshold line. And the skyscrapers above that line resembled the significant signals in your GWAS.  I believe you could easily get how the GWAS Manhattan plot was named. </p> <p>Data we need from sumstats to create Manhattan plots:</p> <ul> <li>Chromosome </li> <li>Basepair position</li> <li>P value or -log10(P)</li> </ul> <p>Steps to create Manhattan plot</p> <ol> <li>sort the variants by genome coordinates.</li> <li>map the genome coordinates of variants to the x axis.</li> <li>convert P value to -log10(P).</li> <li>create the scatter plot.</li> </ol>"},{"location":"06_Association_tests/#quantile-quantile-plot","title":"Quantile-quantile plot","text":"<p>Quantile-quantile plot (as known as Q-Q plot), is commonly used to compare an observed distribution with its expected distribution. For a specific point (x,y) on Q-Q plot, its y coordinate corresponds to one of the quantiles of the observed distribution, while its x coordinate corresponds to the same quantile of the expected distribution.</p> <p>Quantile-quantile plot is used to check if there is any significant inflation in P value distribution, which usually indicates population stratification or cryptic relatedness. </p> <p>Data we need from sumstats to create the Manhattan plot:</p> <ul> <li>P value or -log10(P)</li> </ul> <p>Steps to create Q-Q plot</p> <p>Suppose we have <code>n</code> variants in our sumstats,</p> <ol> <li>convert the <code>n</code> P value to -log10(P).</li> <li>sort the -log10(P) values in asending order.</li> <li>get <code>n</code> numbers from <code>(0,1)</code> with equal intervals.</li> <li>convert the <code>n</code> numbers to -log10(P) and sort in ascending order.</li> <li>create scatter plot using the sorted -log10(P) of sumstats as Y and sorted -log10(P) we generated as X.</li> </ol> <p>Note</p> <p>The expected distribution of P value is a Uniform distribution from 0 to 1.</p> \\[P_{expected} \\sim U(0,1)\\]"},{"location":"06_Association_tests/#regional-plot","title":"Regional plot","text":"<p>Manhattan plot is very useful to check the overview of our sumstats. But if we want to check a specific genomic locus, we need a plot with finer resolution. This kind of plot is called regional plot. It is basically the Manhattan plot of only a small region on the genome, with points colored by its LD r2 with the lead variant in this region.</p> <p>Such a plot is especially helpful to understand the signal and loci, e.g., LD structure, independent signals and genes.</p> <p>The regional plot for the loci of 2:55574452:G:C. </p> <p>Please check Visualization using gwaslab</p> <p></p>"},{"location":"06_Association_tests/#gwas-ssf","title":"GWAS-SSF","text":"<p>To standardized the format of GWAS summary statistics for sharing, GWAS-SSF format was proposed in 2022. This format is now used as the stadard format for GWAS Catalog.</p> <p>GWAS-SSF consists of :</p> <ol> <li>a tab-separated data file with well-defined fields (shown in the following figure)</li> <li>a ccompanying meta data file describing the study (such as sample ancestry, gentyping method, md5sum and so forth)</li> </ol> <p>!!! \"Schematic representation of GWAS-SSF data file\"     </p> <p>Quote</p> <p>Hayhurst, J., Buniello, A., Harris, L., Mosaku, A., Chang, C., Gignoux, C. R., ... &amp; Barroso, I. (2022). A community driven GWAS summary statistics standard. bioRxiv, 2022-07.</p> <p>For details, please check:</p> <ul> <li>https://github.com/EBISPOT/gwas-summary-statistics-standard</li> <li>https://www.ebi.ac.uk/gwas/docs/summary-statistics-format</li> </ul>"},{"location":"07_Annotation/","title":"Variant Annotation","text":""},{"location":"07_Annotation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>ANNOVAR</li> <li>VEP</li> </ul>"},{"location":"07_Annotation/#annovar","title":"ANNOVAR","text":"<p>ANNOVAR is a simple and efficient command line tool for variant annotation. </p> <p>In this tutorial, we will use ANNOVAR to annotate the variants in our summary statistics (hg19).</p>"},{"location":"07_Annotation/#install","title":"Install","text":"<p>Download ANNOVAR from here (registration required; freely available to personal, academic and non-profit use only.)</p> <p>You will receive an email with the download link after registration. Download it and decompress:</p> <pre><code>tar -xvzf annovar.latest.tar.gz\n</code></pre> <p>For refGene annotation for hg19, we do not need to download additional files.</p>"},{"location":"07_Annotation/#format-input-file","title":"Format input file","text":"<p>The default input file for ANNOVAR is a 1-based coordinate file.</p> <p>We will only use the first 100000 variants as an example.</p> <p>annovar_input</p> <pre><code>awk 'NR&gt;1 &amp;&amp; NR&lt;100000 {print $1,$2,$2,$4,$5}' ../06_Association_tests/1kgeas.B1.glm.logistic.    hybrid &gt; annovar_input.txt\n</code></pre> <pre><code>head annovar_input.txt \n1 13273 13273 G C\n1 14599 14599 T A\n1 14604 14604 A G\n1 14930 14930 A G\n1 69897 69897 T C\n1 86331 86331 A G\n1 91581 91581 G A\n1 122872 122872 T G\n1 135163 135163 C T\n1 233473 233473 C G\n</code></pre> <p>With <code>-vcfinput</code> option, ANNOVAR can accept input files in VCF format.</p>"},{"location":"07_Annotation/#annotation","title":"Annotation","text":"<p>Annotate the variants with gene information.</p> <p>A minimal example of annotation using refGene</p> <pre><code>input=annovar_input.txt\nhumandb=/home/he/tools/annovar/annovar/humandb\ntable_annovar.pl ${input} ${humandb} -buildver hg19 -out myannotation -remove -protocol refGene     -operation g -nastring . -polish\n</code></pre> <pre><code>Chr Start   End Ref Alt Func.refGene    Gene.refGene    GeneDetail.refGene  ExonicFunc.refGene  AAChange.    refGene\n1   13273   13273   G   C   ncRNA_exonic    DDX11L1;LOC102725121    .   .   .\n1   14599   14599   T   A   ncRNA_exonic    WASH7P  .   .   .\n1   14604   14604   A   G   ncRNA_exonic    WASH7P  .   .   .\n1   14930   14930   A   G   ncRNA_intronic  WASH7P  .   .   .\n1   69897   69897   T   C   exonic  OR4F5   .   synonymous SNV  OR4F5:NM_001005484:exon1:c.T807C:p.S269S\n1   86331   86331   A   G   intergenic  OR4F5;LOC729737 dist=16323;dist=48442   .   .\n1   91581   91581   G   A   intergenic  OR4F5;LOC729737 dist=21573;dist=43192   .   .\n1   122872  122872  T   G   intergenic  OR4F5;LOC729737 dist=52864;dist=11901   .   .\n1   135163  135163  C   T   ncRNA_exonic    LOC729737   .   .   .\n</code></pre>"},{"location":"07_Annotation/#additional-databases","title":"Additional databases","text":"<p>ANNOVAR supports a wide range of commonly used databases including <code>dbsnp</code> , <code>dbnsfp</code>, <code>clinvar</code>, <code>gnomad</code>, <code>1000g</code>, <code>cadd</code> and so forth. For details, please check ANNOVAR's official documents</p> <p>You can check the Table Name listed in the link above  and download the database you need using the following command.</p> <p>Example: Downloading avsnp150 for hg19 from ANNOVAR</p> <pre><code>annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp150 humandb/\n</code></pre> <p>An example of annotation using multiple databases</p> <pre><code># input file is in vcf format\ntable_annovar.pl \\\n  ${in_vcf} \\\n  ${humandb} \\\n  -buildver hg19 \\\n  -protocol refGene,avsnp150,clinvar_20200316,gnomad211_exome \\\n  -operation g,f,f,f \\\n  -remove \\\n  -out ${out_prefix} \\ \n  -vcfinput\n</code></pre>"},{"location":"07_Annotation/#vep-under-construction","title":"VEP (under construction)","text":""},{"location":"07_Annotation/#install_1","title":"Install","text":"<pre><code>git clone https://github.com/Ensembl/ensembl-vep.git\ncd ensembl-vep\nperl INSTALL.pl\n</code></pre> <pre><code>Hello! This installer is configured to install v108 of the Ensembl API for use by the VEP.\nIt will not affect any existing installations of the Ensembl API that you may have.\n\nIt will also download and install cache files from Ensembl's FTP server.\n\nChecking for installed versions of the Ensembl API...done\n\nSetting up directories\nDestination directory ./Bio already exists.\nDo you want to overwrite it (if updating VEP this is probably OK) (y/n)? y\n - fetching BioPerl\n - unpacking ./Bio/tmp/release-1-6-924.zip\n - moving files\n\nDownloading required Ensembl API files\n - fetching ensembl\n - unpacking ./Bio/tmp/ensembl.zip\n - moving files\n - getting version information\n - fetching ensembl-variation\n - unpacking ./Bio/tmp/ensembl-variation.zip\n - moving files\n - getting version information\n - fetching ensembl-funcgen\n - unpacking ./Bio/tmp/ensembl-funcgen.zip\n - moving files\n - getting version information\n - fetching ensembl-io\n - unpacking ./Bio/tmp/ensembl-io.zip\n - moving files\n - getting version information\n\nTesting VEP installation\n - OK!\n\nThe VEP can either connect to remote or local databases, or use local cache files.\nUsing local cache files is the fastest and most efficient way to run the VEP\nCache files will be stored in /home/he/.vep\nDo you want to install any cache files (y/n)? y\n\nThe following species/files are available; which do you want (specify multiple separated by spaces or 0 for all): \n1 : acanthochromis_polyacanthus_vep_108_ASM210954v1.tar.gz (69 MB)\n2 : accipiter_nisus_vep_108_Accipiter_nisus_ver1.0.tar.gz (55 MB)\n...\n466 : homo_sapiens_merged_vep_108_GRCh37.tar.gz (16 GB)\n467 : homo_sapiens_merged_vep_108_GRCh38.tar.gz (26 GB)\n468 : homo_sapiens_refseq_vep_108_GRCh37.tar.gz (13 GB)\n469 : homo_sapiens_refseq_vep_108_GRCh38.tar.gz (22 GB)\n470 : homo_sapiens_vep_108_GRCh37.tar.gz (14 GB)\n471 : homo_sapiens_vep_108_GRCh38.tar.gz (22 GB)\n\n  Total: 221 GB for all 471 files\n\n? 470\n - downloading https://ftp.ensembl.org/pub/release-108/variation/indexed_vep_cache/homo_sapiens_vep_108_GRCh37.tar.gz\n</code></pre>"},{"location":"08_LDSC/","title":"LD score regression","text":""},{"location":"08_LDSC/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Install LDSC</li> <li>Data Preparation</li> <li>LD score regression</li> <li>Cross-trait LD score regression</li> <li>Partitioned LD regression</li> <li>Celltype specificity LD regression</li> </ul>"},{"location":"08_LDSC/#introduction","title":"Introduction","text":"<p>LDSC is one of the most commonly used command line tool to estimate inflation, hertability, genetic correlation and cell/tissue type specificity from GWAS summary statistics. </p>"},{"location":"08_LDSC/#ld-linkage-disequilibrium","title":"LD: Linkage disequilibrium","text":"<p>Linkage disequilibrium (LD) :  non-random association of alleles at different loci in a given population. (Wiki)</p>"},{"location":"08_LDSC/#ld-score","title":"LD score","text":"<p>LD score \\(l_j\\) for a SNP \\(j\\) is defined as the sum of \\(r^2\\) for the SNP and other SNPs in a region. </p> \\[ l_j= \\Sigma_k{r^2_{j,k}} \\]"},{"location":"08_LDSC/#ld-score-regression_1","title":"LD score regression","text":"<p>Key idea: A variant will have higher test statistics if it is in LD with causal variant, and the elevation is proportional to the correlation ( \\(r^2\\) ) with the causal variant.  </p> \\[ E[\\chi^2|l_j] = {{Nh^2l_j}\\over{M}} + Na + 1 \\] <ul> <li>\\(N\\): sample size.</li> <li>\\(M\\) : number of SNPs.</li> <li>\\(h^2\\) : observed-scale heritability</li> <li>\\(a\\) : the effect of confounding factors, including crytic relatedness and populatiuon stratification.</li> </ul> <p>For more details of LD score regression, please refer to : - Bulik-Sullivan, Brendan K., et al. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies.\" Nature genetics 47.3 (2015): 291-295.</p>"},{"location":"08_LDSC/#install-ldsc","title":"Install LDSC","text":"<p>LDSC can be downloaded from github (GPL-3.0 license): https://github.com/bulik/ldsc</p> <p>For ldsc, we need anaconda to create virtual environment (for python2).  If you haven't installed Anaconda, please check how to install anaconda.</p> <pre><code># change to your directory for tools\ncd ~/tools\n\n# clone the ldsc github repository\ngit clone https://github.com/bulik/ldsc.git\n\n# create a virtual environment for ldsc (python2)\ncd ldsc\nconda env create --file environment.yml  # activate ldsc environment\nconda activate ldsc\n</code></pre>"},{"location":"08_LDSC/#data-preparation","title":"Data Preparation","text":"<p>In this tutoial, we will use sample summary statistics for HDLC and LDLC from Jenger.  - Kanai, Masahiro, et al. \"Genetic analysis of quantitative traits in the Japanese population links cell types to complex human diseases.\" Nature genetics 50.3 (2018): 390-400.</p> <p>The Miami plot for the two traits:</p> <p></p>"},{"location":"08_LDSC/#download-sample-summary-statistics","title":"Download sample summary statistics","text":"<pre><code># HDL-c and LDL-c in Biobank Japan\nwget -O BBJ_LDLC.txt.gz http://jenger.riken.jp/61analysisresult_qtl_download/\nwget -O BBJ_HDLC.txt.gz http://jenger.riken.jp/47analysisresult_qtl_download/\n</code></pre>"},{"location":"08_LDSC/#download-reference-files","title":"Download reference files","text":"<p><pre><code># change to your ldsc directory\ncd ~/tools/ldsc\nmkdir resource\ncd ./resource\n\n# snplist\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/w_hm3.snplist.bz2\n\n# EAS ld score files\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/eas_ldscores.tar.bz2\n\n# EAS weight\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase3_EAS_weights_hm3_no_MHC.tgz\n\n# EAS frequency\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase3_EAS_plinkfiles.tgz\n\n# EAS baseline model\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/1000G_Phase3_EAS_baseline_v1.2_ldscores.tgz\n\n# Cell type ld score files\nwget https://storage.googleapis.com/broad-alkesgroup-public/LDSCORE/LDSC_SEG_ldscores/Cahoy_EAS_1000Gv3_ldscores.tar.gz\n</code></pre> You can then decompress the files and organize them.</p>"},{"location":"08_LDSC/#munge-sumstats","title":"Munge sumstats","text":"<p>Before the analysis, we need to format and clean the raw sumstats.</p> <p>Note</p> <p>Rsid is used here. If the sumstats only contained id like CHR:POS:REF:ALT, annotate it first.</p> <pre><code>snplist=~/tools/ldsc/resource/w_hm3.snplist\nmunge_sumstats.py \\\n--sumstats BBJ_HDLC.txt.gz \\\n--merge-alleles $snplist \\\n--a1 ALT \\\n--a2 REF \\\n--chunksize 500000 \\\n--out BBJ_HDLC\nmunge_sumstats.py \\\n--sumstats BBJ_LDLC.txt.gz \\\n--a1 ALT \\\n--a2 REF \\\n--chunksize 500000 \\\n--merge-alleles $snplist \\\n--out BBJ_LDLC\n</code></pre> <p>After munging, you will get two munged and formatted files:</p> <p><pre><code>BBJ_HDLC.sumstats.gz\nBBJ_LDLC.sumstats.gz\n</code></pre> And these are the files we will use to run LD score regression.</p>"},{"location":"08_LDSC/#ld-score-regression_2","title":"LD score regression","text":"<p>Univariate LD score regression is utilized to estimate heritbility and confuding factors (cryptic relateness and population stratification) of a certain trait. </p> <ul> <li>Reference : Bulik-Sullivan, Brendan K., et al. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies.\" Nature genetics 47.3 (2015): 291-295.</li> </ul> <p>Using the munged sumstats, we can run:</p> <pre><code>ldsc.py \\\n--h2 BBJ_HDLC.sumstats.gz \\\n--ref-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--w-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--out BBJ_HDLC\n\nldsc.py \\\n--h2 BBJ_LDLC.sumstats.gz \\\n--ref-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--w-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--out BBJ_LDLC\n</code></pre> <p>Lest's check the results for HDLC:</p> <pre><code>cat BBJ_HDLC.log\n*********************************************************************\n* LD Score Regression (LDSC)\n* Version 1.0.1\n* (C) 2014-2019 Brendan Bulik-Sullivan and Hilary Finucane\n* Broad Institute of MIT and Harvard / MIT Department of Mathematics\n* GNU General Public License v3\n*********************************************************************\nCall: \n./ldsc.py \\\n--h2 BBJ_HDLC.sumstats.gz \\\n--ref-ld-chr /home/he/tools/ldsc/resource/eas_ldscores/ \\\n--out BBJ_HDLC \\\n--w-ld-chr /home/he/tools/ldsc/resource/eas_ldscores/ \n\nBeginning analysis at Sat Dec 24 20:40:34 2022\nReading summary statistics from BBJ_HDLC.sumstats.gz ...\nRead summary statistics for 1020377 SNPs.\nReading reference panel LD Score from /home/he/tools/ldsc/resource/eas_ldscores/[1-22] ... (ldscore_fromlist)\nRead reference panel LD Scores for 1208050 SNPs.\nRemoving partitioned LD Scores with zero variance.\nReading regression weight LD Score from /home/he/tools/ldsc/resource/eas_ldscores/[1-22] ... (ldscore_fromlist)\nRead regression weight LD Scores for 1208050 SNPs.\nAfter merging with reference panel LD, 1012040 SNPs remain.\nAfter merging with regression SNP LD, 1012040 SNPs remain.\nUsing two-step estimator with cutoff at 30.\nTotal Observed scale h2: 0.1583 (0.0281)\nLambda GC: 1.1523\nMean Chi^2: 1.2843\nIntercept: 1.0563 (0.0114)\nRatio: 0.1981 (0.0402)\nAnalysis finished at Sat Dec 24 20:40:41 2022\nTotal time elapsed: 6.57s\n</code></pre> <p>We can see that from the log:</p> <ul> <li>Observed scale h2 = 0.1583</li> <li>lambda GC = 1.1523 </li> <li>intercept = 1.0563</li> <li>Ratio = 0.1981</li> </ul> <p>According to LDSC documents, Ratio measures the proportion of the inflation in the mean chi^2 that the LD Score regression intercept ascribes to causes other than polygenic heritability. The value of ratio should be close to zero, though in practice values of 10-20% are not uncommon.</p> \\[ Ratio = {{intercept-1}\\over{mean(\\chi^2)-1}} \\]"},{"location":"08_LDSC/#cross-trait-ld-score-regression","title":"Cross-trait LD score regression","text":"<p>Cross-trait LD score regression is employed to estimate the genetic correlation between a pair of traits.</p> <p>Key idea: replace <code>\\chi^2</code> in univariate LD score regression and the relationship (SNPs with high LD ) still holds.</p> \\[ E[z_{1j}z_{2j}] = {{\\sqrt{N_1N_2}\\rho_g}\\over{M}}l_j + {{\\rho N_s}\\over{\\sqrt{N_1N_2}}} \\] <ul> <li>\\(z_ij\\) : z score of trait i for SNP j</li> <li>\\(N_i\\) : sample size of trait i</li> <li>\\(\\rho\\) : phenotypic correlation</li> <li>\\(\\rho_g\\) : genetic covariance </li> <li>\\(l_j\\) : LD score</li> <li>\\(M\\) : number of SNPs</li> </ul> <p>Then we can get the genetic correlation by :</p> \\[ r_g = {{\\rho_g}\\over{\\sqrt{h_1^2h_2^2}}} \\] <ul> <li>Reference: Bulik-Sullivan, Brendan, et al. \"An atlas of genetic correlations across human diseases and traits.\" Nature genetics 47.11 (2015): 1236-1241.</li> </ul> <p><pre><code>ldsc.py \\\n--rg BBJ_HDLC.sumstats.gz,BBJ_LDLC.sumstats.gz \\\n--ref-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--w-ld-chr ~/tools/ldsc/resource/eas_ldscores/ \\\n--out BBJ_HDLC_LDLC\n</code></pre> Let's check the results:</p> <pre><code>*********************************************************************\n* LD Score Regression (LDSC)\n* Version 1.0.1\n* (C) 2014-2019 Brendan Bulik-Sullivan and Hilary Finucane\n* Broad Institute of MIT and Harvard / MIT Department of Mathematics\n* GNU General Public License v3\n*********************************************************************\nCall: \n./ldsc.py \\\n--ref-ld-chr /home/he/tools/ldsc/resource/eas_ldscores/ \\\n--out BBJ_HDLC_LDLC \\\n--rg BBJ_HDLC.sumstats.gz,BBJ_LDLC.sumstats.gz \\\n--w-ld-chr /home/he/tools/ldsc/resource/eas_ldscores/ \n\nBeginning analysis at Thu Dec 29 21:02:37 2022\nReading summary statistics from BBJ_HDLC.sumstats.gz ...\nRead summary statistics for 1020377 SNPs.\nReading reference panel LD Score from /home/he/tools/ldsc/resource/eas_ldscores/[1-22] ... (ldscore_fromlist)\nRead reference panel LD Scores for 1208050 SNPs.\nRemoving partitioned LD Scores with zero variance.\nReading regression weight LD Score from /home/he/tools/ldsc/resource/eas_ldscores/[1-22] ... (ldscore_fromlist)\nRead regression weight LD Scores for 1208050 SNPs.\nAfter merging with reference panel LD, 1012040 SNPs remain.\nAfter merging with regression SNP LD, 1012040 SNPs remain.\nComputing rg for phenotype 2/2\nReading summary statistics from BBJ_LDLC.sumstats.gz ...\nRead summary statistics for 1217311 SNPs.\nAfter merging with summary statistics, 1012040 SNPs remain.\n1012040 SNPs with valid alleles.\n\nHeritability of phenotype 1\n---------------------------\nTotal Observed scale h2: 0.1054 (0.0383)\nLambda GC: 1.1523\nMean Chi^2: 1.2843\nIntercept: 1.1234 (0.0607)\nRatio: 0.4342 (0.2134)\n\nHeritability of phenotype 2/2\n-----------------------------\nTotal Observed scale h2: 0.0543 (0.0211)\nLambda GC: 1.0833\nMean Chi^2: 1.1465\nIntercept: 1.0583 (0.0335)\nRatio: 0.398 (0.2286)\n\nGenetic Covariance\n------------------\nTotal Observed scale gencov: 0.0121 (0.0106)\nMean z1*z2: -0.001\nIntercept: -0.0198 (0.0121)\n\nGenetic Correlation\n-------------------\nGenetic Correlation: 0.1601 (0.1821)\nZ-score: 0.8794\nP: 0.3792\n\n\nSummary of Genetic Correlation Results\np1                    p2      rg      se       z       p  h2_obs  h2_obs_se  h2_int  h2_int_se  gcov_int  gcov_int_se\nBBJ_HDLC.sumstats.gz  BBJ_LDLC.sumstats.gz  0.1601  0.1821  0.8794  0.3792  0.0543     0.0211  1.0583     0.0335   -0.0198       0.0121\n\nAnalysis finished at Thu Dec 29 21:02:47 2022\nTotal time elapsed: 10.39s\n</code></pre>"},{"location":"08_LDSC/#partitioned-ld-regression","title":"Partitioned LD regression","text":"<p>Partitioned LD regression is utilized to evaluate the contribution of each functional group to the total SNP heriatbility.</p> \\[ E[\\chi^2] = N \\sum\\limits_C \\tau_C l(j,C) + Na + 1 \\] <ul> <li>\\(C\\) : functional categories</li> <li>\\(l(j,C)\\) : LD score for SNP j with respect to C. </li> <li> <p>\\(\\tau_C\\) : per-SNP contribution of category C to heritability</p> </li> <li> <p>Reference: Finucane, Hilary K., et al. \"Partitioning heritability by functional annotation using genome-wide association summary statistics.\" Nature genetics 47.11 (2015): 1228-1235.</p> </li> </ul> <pre><code>ldsc.py \\\n--h2 BBJ_HDLC.sumstats.gz \\\n--overlap-annot \\\n--ref-ld-chr ~/tools/ldsc/resource/1000G_Phase3_EAS_baseline_v1_2_ldscores/baseline. \\\n--frqfile-chr ~/tools/ldsc/resource/1000G_Phase3_EAS_plinkfiles/1000G.EAS.QC. \\\n--w-ld-chr ~/tools/ldsc/resource/1000G_Phase3_EAS_weights_hm3_no_MHC/weights.EAS.hm3_noMHC. \\\n--out BBJ_HDLC_baseline\n</code></pre>"},{"location":"08_LDSC/#celltype-specificity-ld-regression","title":"Celltype specificity LD regression","text":"<p>LDSC-SEG :  LD score regression applied to specifically expressed genes</p> <p>An extension of Partitioned LD regression. Categories are defined by tissue or cell-type specific genes.</p> <ul> <li>Reference: Finucane, Hilary K., et al. \"Heritability enrichment of specifically expressed genes identifies disease-relevant tissues and cell types.\" Nature genetics 50.4 (2018): 621-629.</li> </ul> <pre><code>ldsc.py \\\n--h2-cts BBJ_HDLC.sumstats.gz \\\n--ref-ld-chr-cts ~/tools/ldsc/resource/Cahoy_EAS_1000Gv3_ldscores/Cahoy.EAS.ldcts \\\n--ref-ld-chr ~/tools/ldsc/resource/1000G_Phase3_EAS_baseline_v1_2_ldscores/baseline. \\\n--w-ld-chr ~/tools/ldsc/resource/1000G_Phase3_EAS_weights_hm3_no_MHC/weights.EAS.hm3_noMHC. \\\n--out BBJ_HDLC_baseline_cts\n</code></pre>"},{"location":"08_LDSC/#reference","title":"Reference","text":"<ul> <li>Bulik-Sullivan, Brendan K., et al. \"LD Score regression distinguishes confounding from polygenicity in genome-wide association studies.\" Nature genetics 47.3 (2015): 291-295.</li> <li>Bulik-Sullivan, Brendan, et al. \"An atlas of genetic correlations across human diseases and traits.\" Nature genetics 47.11 (2015): 1236-1241.</li> <li>Finucane, Hilary K., et al. \"Partitioning heritability by functional annotation using genome-wide association summary statistics.\" Nature genetics 47.11 (2015): 1228-1235.</li> <li>Finucane, Hilary K., et al. \"Heritability enrichment of specifically expressed genes identifies disease-relevant tissues and cell types.\" Nature genetics 50.4 (2018): 621-629.</li> <li>Kanai, Masahiro, et al. \"Genetic analysis of quantitative traits in the Japanese population links cell types to complex human diseases.\" Nature genetics 50.3 (2018): 390-400.</li> </ul>"},{"location":"09_Gene_based_analysis/","title":"Gene and gene-set analysis","text":""},{"location":"09_Gene_based_analysis/#table-of-contents","title":"Table of Contents","text":"<ul> <li>MAGMA Introduction</li> <li>Install MAGMA</li> <li>Download reference files</li> <li>Format input files</li> <li>Annotate SNPs</li> <li>Gene-based analysis</li> <li>Gene-set level analysis</li> <li>Reference</li> </ul>"},{"location":"09_Gene_based_analysis/#magma-introduction","title":"MAGMA Introduction","text":"<p>MAGMA is one the most commonly used tools for gene-based and gene-set analysis. </p> <p>Gene-level analysis in MAGMA uses two models:</p> <p>1.Multiple linear principal components regression</p> <p>MAGMA employs a multiple linear principal components regression, and F test to obtain P values for genes. The multiple linear principal components regression: </p> \\[ Y = \\alpha_{0,g} + X_g \\alpha_g + W \\beta_g + \\epsilon_g \\] <ul> <li>\\(X_g\\) : principal component matrix </li> <li>\\(\\alpha_g\\) : genetic effects</li> <li>\\(W\\) : covariate matrix</li> <li>\\(\\beta_g\\) : effects of covariates </li> </ul> <p>\\(X_g\\) is obtained by first projecting the variant matrix of a gene onto its PC, and removing PCs with samll eigenvalues.</p> <p>Note</p> <p>The linear principal components regression model requires raw genotype data.</p> <p>2.SNP-wise models</p> <p>SNP-wise Mean: perform tests on mean SNP association</p> <p>Note</p> <p>SNP-wise models use summary statistics and reference LD panel</p> <p>Gene-set analysis</p> <p>Quote</p> <p>Competitive gene-set analysis tests whether the genes in a gene-set are more strongly associated with the phenotype of interest than other genes.</p> <p>P values for each gene were converted to Z scores to perform gene-set level analysis.</p> \\[ Z = \\beta_{0,S} + S_S \\beta_S + \\epsilon \\] <ul> <li>\\(S_S\\) : indicator (if the gene is in a specified gene set)</li> <li>\\(\\beta_S\\) : difference in effects between genes in the specified set and genes ouside the set.</li> </ul>"},{"location":"09_Gene_based_analysis/#install-magma","title":"Install MAGMA","text":"<p>Dowload MAGMA for your operating system from the following url: https://ctg.cncr.nl/software/magma</p> <p>For example: <pre><code>cd ~/tools\nmkdir MAGMA\ncd MAGMA\nwget https://ctg.cncr.nl/software/MAGMA/prog/magma_v1.10.zip\nunzip magma_v1.10.zip\n</code></pre> Add magma to your environment path.</p> <p>Test if it is successfully installed. <pre><code>$ magma --version\nMAGMA version: v1.10 (linux)\n</code></pre></p>"},{"location":"09_Gene_based_analysis/#download-reference-files","title":"Download reference files","text":"<p>We nedd the following reference files: - gene location files - LD reference panel - Gene-set files</p> <p>The gene location files and LD reference panel can be downloaded from magma website. -&gt; https://ctg.cncr.nl/software/magma The third one can be downloaded form MsigDB. -&gt; https://www.gsea-msigdb.org/gsea/msigdb/</p>"},{"location":"09_Gene_based_analysis/#format-input-files","title":"Format input files","text":"<pre><code>zcat ../08_LDSC/BBJ_HDLC.txt.gz | awk 'NR&gt;1 &amp;&amp; $2==3 {print $1,$2,$3}' &gt; HDLC_chr3.magma.input.snp.chr.pos.txt\nzcat ../08_LDSC/BBJ_HDLC.txt.gz | awk 'NR&gt;1 &amp;&amp; $2==3 {print $1,10^(-$11)}' &gt;  HDLC_chr3.magma.input.p.txt\n</code></pre>"},{"location":"09_Gene_based_analysis/#annotate-snps","title":"Annotate SNPs","text":"<pre><code>snploc=./HDLC_chr3.magma.input.snp.chr.pos.txt\nncbi37=~/tools/magma/NCBI37/NCBI37.3.gene.loc\nmagma --annotate \\\n      --snp-loc ${snploc} \\\n      --gene-loc ${ncbi37} \\\n      --out HDLC_chr3\n</code></pre> <p>Tip</p> <p>Usually to capture the variants in the regulatory regions, we will add windows upstream and downstream of the genes with <code>--annotate window</code>. For example, <code>--annotate window=35,10</code> set a 35 kilobase pair(kb) upstream and 10kb downstream window.</p>"},{"location":"09_Gene_based_analysis/#gene-based-analysis","title":"Gene-based analysis","text":"<pre><code>ref=~/tools/magma/g1000_eas/g1000_eas\nmagma \\\n    --bfile $ref \\\n    --pval ./HDLC_chr3.magma.input.p.txt N=70657 \\\n    --gene-annot HDLC_chr3.genes.annot \\\n    --out HDLC_chr3\n</code></pre>"},{"location":"09_Gene_based_analysis/#gene-set-level-analysis","title":"Gene-set level analysis","text":"<pre><code>geneset=/home/he/tools/magma/MSigDB/msigdb_v2022.1.Hs_files_to_download_locally/msigdb_v2022.1.Hs_GMTs/msigdb.v2022.1.Hs.entrez.gmt\nmagma \\\n    --gene-results HDLC_chr3.genes.raw \\\n    --set-annot ${geneset} \\\n    --out HDLC_chr3\n</code></pre>"},{"location":"09_Gene_based_analysis/#reference","title":"Reference","text":"<ul> <li>de Leeuw, Christiaan A., et al. \"MAGMA: generalized gene-set analysis of GWAS data.\" PLoS computational biology 11.4 (2015): e1004219.</li> </ul>"},{"location":"10_PRS/","title":"Polygenic risk scores","text":""},{"location":"10_PRS/#definition","title":"Definition","text":"<p>Polygenic risk score(PRS), as known as polygenic score (PGS) or genetic risk score (GRS), is a score that summarizes the effect sizes of genetic variants on a certain disease or trait (weighted sum of disease/trait-associated alleles).</p> <p>To calculate the PRS for sample j, </p> \\[PRS_j = \\sum_{i=0}^{i=M} x_{i,j} \\beta_{i}\\] <ul> <li>\\(\\beta_i\\) : effect size for variant \\(i\\)</li> <li>\\(x_{i,j}\\) : the effect allele count for sample \\(j\\) at variant \\(i\\)</li> <li>\\(M\\) : the number of variants</li> </ul>"},{"location":"10_PRS/#prs-analysis-workflow","title":"PRS Analysis Workflow","text":"<ol> <li>Developing PRS model using base data</li> <li>Performing validation to obtain best-fit parameters</li> <li>Evaluation in an independent population</li> </ol>"},{"location":"10_PRS/#methods","title":"Methods","text":"Category Description Representative Methods P value thresholding P + T C+T, PRSice Beta shrinkage genome-wide PRS model LDpred, PRS-CS <p>In this tutorial, we will first briefly introduce how to develop PRS model using the sample data and then demonstrate how we can download PRS models from PGS Catalog and apply to our sample genotype data. </p>"},{"location":"10_PRS/#ctpt-using-plink","title":"C+T/P+T using PLINK","text":"<p>P+T stands for Pruning + Thresholding, also known as Clumping and Thresholding(C+T), which is a very simple and straightforward approach to constructing PRS models.</p> <p>Clumping</p> <p>Clumping: LD-pruning based on P value. It is a approach to select variants when there are multiple significant associations in high LD in the same region.</p> <p>The three important parameters for clumping in PLINK are:</p> <ul> <li>clump-p1 0.0001       # Significance threshold for index SNPs</li> <li>clump-r2 0.50         # LD threshold for clumping</li> <li>clump-kb 250          # Physical distance threshold for clumping</li> </ul> <p>Clumping using PLINK</p> <pre><code>#!/bin/bash\n\nplinkFile=../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\nsumStats=../06_Association_tests/1kgeas.B1.glm.firth\n\nplink \\\n    --bfile ${plinkFile} \\\n    --clump-p1 0.0001 \\\n    --clump-r2 0.1 \\\n    --clump-kb 250 \\\n    --clump ${sumStats} \\\n    --clump-snp-field ID \\\n    --clump-field P \\\n    --out 1kg_eas\n</code></pre> <p>log <pre><code>--clump: 40 clumps formed from 307 top variants.\n</code></pre> check only the header and the first \"clump\" of SNPs.</p> <pre><code>head -n 2 1kg_eas.clumped\n CHR    F              SNP         BP        P    TOTAL   NSIG    S05    S01   S001  S0001    SP2\n   2    1   2:55574452:G:C   55574452   2.18e-09      111     16     15     20     17     43 2:55376609:T:G(1),2:55376892:T:G(1),2:55379420:A:G(1),2:55384226:A:G(    1),2:55397522:A:T(1),2:55405037:T:A(1),2:55405468:G:A(1),2:55410835:C:T(1),2:55424229:G:A(1),2:55426447:A:G(1),2:55452014:C:T(1),2:55460751:C:T(1),2:55461053:A:G(    1),2:55465633:T:G(1),2:55470115:C:T(1),2:55478226:A:G(1),2:55482911:C:T(1),2:55489629:T:C(1),2:55491330:G:A(1),2:55495251:C:T(1),2:55500234:A:G(1),2:55501125:C:T(    1),2:55513738:C:T(1),2:55521039:A:G(1),2:55554956:C:A(1),2:55555726:T:C(1),2:55557192:C:T(1),2:55560347:C:A(1),2:55563020:A:G(1),2:55578761:C:T(1),2:55582635:T:C(    1),2:55584526:T:C(1),2:55585577:A:T(1),2:55587807:G:T(1),2:55598053:T:C(1),2:55602809:G:A(1),2:55605264:A:G(1),2:55607830:C:T(1),2:55609851:C:T(1),2:55610999:C:T(    1),2:55611741:A:G(1),2:55611766:T:C(1),2:55612986:G:C(1),2:55617255:C:T(1),2:55618813:C:T(1),2:55619407:C:A(1),2:55619923:C:T(1),2:55620927:G:A(1),2:55621660:T:C(    1),2:55622431:A:C(1),2:55625464:C:T(1),2:55632329:T:C(1),2:55632724:T:C(1),2:55635477:C:T(1),2:55636351:T:C(1),2:55636755:T:C(1),2:55636795:A:C(1),2:55638697:T:C(    1),2:55642350:T:C(1),2:55643047:G:A(1),2:55643666:A:C(1),2:55650512:G:A(1),2:55650572:G:C(1),2:55650886:G:A(1),2:55650940:C:A(1),2:55653053:G:C(1),2:55653269:C:A(    1),2:55654354:A:G(1),2:55654847:A:G(1),2:55659155:A:G(1),2:55662423:T:C(1),2:55669642:G:C(1),2:55685127:A:G(1),2:55685669:G:C(1),2:55687290:G:C(1),2:55695696:A:G(    1),2:55696124:A:G(1),2:55709416:G:A(1),2:55720739:G:C(1),2:55724035:T:C(1)\n</code></pre>"},{"location":"10_PRS/#beta-shrinkage-using-prs-cs","title":"Beta shrinkage using PRS-CS","text":"\\[ \\beta_j | \\Phi_j \\sim N(0,\\phi\\Phi_j) ,  \\Phi_j \\sim g \\] <p>Reference: Ge, T., Chen, C. Y., Ni, Y., Feng, Y. C. A., &amp; Smoller, J. W. (2019). Polygenic prediction via Bayesian regression and continuous shrinkage priors. Nature communications, 10(1), 1-10.</p>"},{"location":"10_PRS/#parameter-tuning","title":"Parameter tuning","text":"Method Description Cross-validation 10-fold cross validation. This method usually requires large-scale genotype dataset. Independent population Perform validation in an independent population of the same ancestry. Pseudo-validation A few methods can estimate a single optimal shrinkage parameter using only the base GWAS summary statistics."},{"location":"10_PRS/#pgs-catalog","title":"PGS Catalog","text":"<p>Just like GWAS Catalog, you can now download published PRS  models from PGS catalog. </p> <p>URL: http://www.pgscatalog.org/</p> <p></p> <p>Reference: Lambert, S. A., Gil, L., Jupp, S., Ritchie, S. C., Xu, Y., Buniello, A., ... &amp; Inouye, M. (2021). The Polygenic Score Catalog as an open database for reproducibility and systematic evaluation. Nature Genetics, 53(4), 420-425.</p>"},{"location":"10_PRS/#calculate-prs-using-plink","title":"Calculate PRS using PLINK","text":"<pre><code>plink --score &lt;score_filename&gt; [variant ID col.] [allele col.] [score col.] ['header']\n</code></pre> <ul> <li><code>&lt;score_filename&gt;</code>: the score file</li> <li><code>[variant ID col.]</code>: the column number for variant IDs</li> <li><code>[allele col.]</code>: the column number for effect alleles</li> <li><code>[score col.]</code>: the column number for betas</li> <li><code>['header']</code>: skip the first header line</li> </ul> <p>Please check here for detailed documents on <code>plink --score</code>.</p> <p>Example</p> <pre><code># genotype data\nplinkFile=../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\n# summary statistics for scoring\nsumStats=./t2d_plink_reduced.txt\n# SNPs after clumpping\nawk 'NR!=1{print $3}' 1kgeas.clumped &gt;  1kgeas.valid.snp\n\nplink \\\n    --bfile ${plinkFile} \\\n    --score ${sumStats} 1 2 3 header \\\n    --extract 1kgeas.valid.snp \\\n    --out 1kgeas\n</code></pre> <p>For thresholding using P values,  we can create a range file and a p-value file.</p> <p>The options we use: <pre><code>--q-score-range &lt;range file&gt; &lt;data file&gt; [variant ID col.] [data col.] ['header']\n</code></pre></p> <p>Example</p> <pre><code># SNP - P value file for thresholding\nawk '{print $1,$4}' ${sumStats} &gt; SNP.pvalue\n\n# create a range file with 3 columns: range label, p-value lower bound, p-value upper bound\nhead range_list\npT0.001 0 0.001\npT0.05 0 0.05\npT0.1 0 0.1\npT0.2 0 0.2\npT0.3 0 0.3\npT0.4 0 0.4\npT0.5 0 0.5\n</code></pre> <p>and then calculate the scores using the p-value ranges:</p> <pre><code>plink2 \\\n--bfile ${plinkFile} \\\n--score ${sumStats} 1 2 3 header cols+=denom,scoresums \\\n--q-score-range range_list SNP.pvalue \\\n--extract 1kgeas.valid.snp \\\n--out 1kgeas\n</code></pre> <p>You will get the following files: <pre><code>1kgeas.pT0.001.sscore\n1kgeas.pT0.05.sscore\n1kgeas.pT0.1.sscore\n1kgeas.pT0.2.sscore\n1kgeas.pT0.3.sscore\n1kgeas.pT0.4.sscore\n1kgeas.pT0.5.sscore\n</code></pre></p> <p>Take a look at the files:</p> <pre><code>head 1kgeas.pT0.1.sscore\n    #FID    IID     ALLELE_CT       NAMED_ALLELE_DOSAGE_SUM SCORE1_AVG      SCORE1_SUM\n0       HG00403 43228   14559   -6.10599e-05    -2.6395\n0       HG00404 43228   14700   2.53449e-05     1.09561\n0       HG00406 43228   14702   1.08678e-05     0.469793\n0       HG00407 43228   14668   -0.000132824    -5.74171\n0       HG00409 43228   14713   1.0477e-05      0.452899\n0       HG00410 43228   14749   -0.000140386    -6.0686\n0       HG00419 43228   14687   -5.25762e-05    -2.27277\n0       HG00421 43228   14755   -8.9396e-05     -3.86441\n0       HG00422 43228   14733   3.94514e-06     0.170541\n</code></pre>"},{"location":"10_PRS/#meta-scoring-methods-for-prs","title":"Meta-scoring methods for PRS","text":"<p>It has been shown recently that the PRS models generated from multiple traits using a meta-scoring method potentially outperforms PRS models generated from a single trait. Inouye et al. first used this approach for generating a PRS model for CAD from multiple PRS models. </p> <p>Potential advantages of meta-score for PRS generation</p> <ul> <li>increased marker coverage</li> <li>reduced genotyping or imputation uncertainty</li> <li>more accurate effect size estimates</li> </ul> <p>Reference: Inouye, M., Abraham, G., Nelson, C. P., Wood, A. M., Sweeting, M. J., Dudbridge, F., ... &amp; UK Biobank CardioMetabolic Consortium CHD Working Group. (2018). Genomic risk prediction of coronary artery disease in 480,000 adults: implications for primary prevention. Journal of the American College of Cardiology, 72(16), 1883-1893.</p> <p>elastic net</p> <p>Elastic net is a common approach for variable selection when there are highly correlated variables (for example, PRS of correlated diseases are often highly correlated.). When fitting linear or logistic models, L1 and L2 penalties are added (regularization). </p> \\[ \\hat{\\beta} \\equiv argmin({\\parallel y- X \\beta \\parallel}^2 + \\lambda_2{\\parallel \\beta \\parallel}^2 + \\lambda_1{\\parallel \\beta \\parallel} ) \\] <p>After validation, PRS can be generated from distinct PRS for other genetically correlated diseases :  </p> \\[PRS_{meta} = {w_1}PRS_{Trait1} + {w_2}PRS_{Trait2} + {w_3}PRS_{Trait3} + ... \\] <p>An example: Abraham, G., Malik, R., Yonova-Doing, E., Salim, A., Wang, T., Danesh, J., ... &amp; Dichgans, M. (2019). Genomic risk score offers predictive performance comparable to clinical risk factors for ischaemic stroke. Nature communications, 10(1), 1-10.</p>"},{"location":"10_PRS/#reference","title":"Reference","text":"<ul> <li>PLINK : Purcell, Shaun, et al. \"PLINK: a tool set for whole-genome association and population-based linkage analyses.\" The American journal of human genetics 81.3 (2007): 559-575.</li> <li>PGS Catalog : Lambert, Samuel A., et al. \"The Polygenic Score Catalog as an open database for reproducibility and systematic evaluation.\" Nature Genetics 53.4 (2021): 420-425.</li> <li>PRS-CS : Ge, Tian, et al. \"Polygenic prediction via Bayesian regression and continuous shrinkage priors.\" Nature communications 10.1 (2019): 1-10.</li> <li>PRS Tutorial: Choi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O\u2019Reilly. \"Tutorial: a guide to performing polygenic risk score analyses.\" Nature protocols 15.9 (2020): 2759-2772.</li> <li>metaGRS 1: Inouye, M., Abraham, G., Nelson, C. P., Wood, A. M., Sweeting, M. J., Dudbridge, F., ... &amp; UK Biobank CardioMetabolic Consortium CHD Working Group. (2018). Genomic risk prediction of coronary artery disease in 480,000 adults: implications for primary prevention. Journal of the American College of Cardiology, 72(16), 1883-1893.</li> <li>metaGRS 2: Abraham, G., Malik, R., Yonova-Doing, E., Salim, A., Wang, T., Danesh, J., ... &amp; Dichgans, M. (2019). Genomic risk score offers predictive performance comparable to clinical risk factors for ischaemic stroke. Nature communications, 10(1), 1-10.</li> </ul>"},{"location":"11_meta_analysis/","title":"Meta-analysis","text":""},{"location":"11_meta_analysis/#aims","title":"Aims","text":"<p>Meta-analysis is one of the most commonly used statistical methods to combine the evidence from multiple studies into a single result. </p> <p>Potential problems for small-scale genome-wide association studies</p> <ul> <li>Low coverage of markers and genetic variability</li> <li>Less accurate effect size estimation</li> <li>Low statistical power</li> </ul> <p>To address these problems, meta-analysis is a powerful approach to integrate multiple GWAS summary statistics, especially when more and more summary statistics are publicly available. . This method allows us to obtain increases in statistical power as sample size increases. </p> <p>What we could achieve by conducting meta-analysis</p> <ul> <li> Increase the statistical power for GWASs. </li> <li> Improve the effect size estimations, which could facilitate downstream analyses. (For example, PRS or MR).</li> <li> Provide opportunities to study the less prevalent or understudied diseases. </li> <li> Cross-validate findings across different studies. </li> </ul>"},{"location":"11_meta_analysis/#a-typical-workflow-of-meta-analysis","title":"A typical workflow of meta-analysis","text":""},{"location":"11_meta_analysis/#harmonization-and-qc-for-gwa-meta-analysis","title":"Harmonization and QC for GWA meta-analysis","text":"<p>Before performing any type of meta-analysis, we need to make sure our datasets contain sufficient information and the datasets are QCed and harmonized. It is important to perform this step to avoid any unexpected errors and heterogeneity.</p> <p>Key points for Dataset selection</p> <ul> <li>[] Minimal requirements for data (CHR,POS,BETA,SE,P,N,EA,NEA,EAF... )</li> <li>[] Phenotype definition </li> <li>[] Study design </li> <li>[] Sample overlap (independent population)</li> <li>[] Proper citations (we can obtain sufficient information on study design, phenotype definition and QC)</li> <li>[] Data integrity (md5sum check)</li> <li>[] Ancestry (population with the same ancestry)</li> <li>[] Downloading from the source (preferably not second-hand datasets)</li> </ul> <p>Key points for Quality control</p> <ul> <li>[] Remove variants with minor allele frequency being too low</li> <li>[] Remove Multi-allelic Variants</li> <li>[] Remove Duplicated variants</li> <li>[] Remove Copy number variation</li> <li>[] Normalize Indels</li> <li>[] Standardize notations</li> <li>[] Removed variants with extreme effect sizes</li> <li>[] Filter out variants with low imputation accuracy</li> </ul> <p>Key points for Harmonization</p> <ul> <li>[] On the genomic coordinate </li> <li>[] On the same strand (mostly forward)</li> <li>[] Be cautious for palindromic SNPs</li> </ul>"},{"location":"11_meta_analysis/#fixed-effects-meta-analysis","title":"Fixed effects meta-analysis","text":"<p>Simply speaking, the fixed effects we mentioned here mean that the between-study variance is zero. Under the fixed effect model, we assume a common effect size across studies for a certain SNP.</p> <p>Fixed effect model</p> \\[ \\bar{\\beta_{ij}} = {{\\sum_{i=1}^{k} {w_{ij} \\beta_{ij}}}\\over{\\sum_{i=1}^{k} {w_{ij}}}} \\] <ul> <li>\\(w_{ij} = 1 / Var(\\beta_{ij})\\)</li> </ul>"},{"location":"11_meta_analysis/#heterogeneity-test","title":"Heterogeneity test","text":"<p>Cochran's Q test and \\(I^2\\) </p> \\[ Q = \\sum_{i=1}^{k} {w_i (\\beta_i - \\bar{\\beta})^2} \\] \\[ I_j^2 =  {{Q_j - df_j}\\over{Q_j}}\\times 100% =  {{Q - (k - 1)}\\over{Q}}\\times 100% \\]"},{"location":"11_meta_analysis/#metal","title":"METAL","text":"<p>METAL is one of the most commonly used tools for GWA meta-analysis. Its official documentation can be found here. METAL supports two models: (1) Sample size based approach and (2) Inverse variance based approach.</p> <p>A minimal example of meta-analysis using the IVW method</p> metal_script.txt<pre><code># classical approach, uses effect size estimates and standard errors\nSCHEME STDERR  \n\n# === DESCRIBE AND PROCESS THE FIRST INPUT FILE ===\nMARKER SNP\nALLELE REF_ALLELE OTHER_ALLELE\nEFFECT BETA\nPVALUE PVALUE \nSTDERR SE \nPROCESS inputfile1.txt\n\n# === THE SECOND INPUT FILE HAS THE SAME FORMAT AND CAN BE PROCESSED IMMEDIATELY ===\nPROCESS inputfile2.txt\n\nANALYZE\n</code></pre> <p>Then, just run the following command to execute the metal script.</p> <pre><code>metal meta_input.txt\n</code></pre>"},{"location":"11_meta_analysis/#random-effects-meta-analysis","title":"Random effects meta-analysis","text":"<p>On the other hand, random effects mean that we need to model the between-study variance, which is not zero in this case. Under the random effect model, we assume the true effect size for a certain SNP varies across studies.</p> <p>If heterogeneity of effects exists across studies, we need to model the between-study variance to correct for the deflation of variance in fixed-effect estimates.  </p>"},{"location":"11_meta_analysis/#gwama","title":"GWAMA","text":"<p>Random effect model</p> <p>The random effect variance component can be estimated by:</p> \\[ r_j^2 = max\\left(0, {{Q_j - (N_j -1)}\\over{\\sum_iw_{ij} - ({{\\sum_iw_{ij}^2} \\over {\\sum_iw_    {ij}}})}}\\right)\\] <p>Then the effect size for SNP j can be obtained by:</p> \\[ \\bar{\\beta_j}^* = {{\\sum_{i=1}^{k} {w_{ij}^* \\beta_i}}\\over{\\sum_{i=1}^{k} {w_{ij}^*}}} \\] <p>The weights are estimated by:</p> \\[w_{ij}^* = {{1}\\over{r_j^2 + Var(\\beta_{ij})}} \\] <p>The random effect model was implemented in GWAMA, which is another very popular GWA meta-analysis tool. Its official documentation can be found here.</p> <p>A minimal example of random effect meta-analysis using GWAMA</p> <p>The input file for GWAMA contains the path to each sumstats. Column names need to be standardized.</p> GWAMA_script.in<pre><code>Pop1.txt\nPop2.txt\nPop3.txt\n</code></pre> <pre><code>GWAMA \\\n-i GWAMA_script.in \\\n--random \\\n-o myresults\n</code></pre>"},{"location":"11_meta_analysis/#cross-ancestry-meta-analysis","title":"Cross-ancestry meta-analysis","text":""},{"location":"11_meta_analysis/#mantra","title":"MANTRA","text":"<p>MANTRA (Meta-ANalysis of Transethnic Association studies) is one of the early efforts to address the heterogeneity for cross-ancestry meta-analysis.</p> <p>MANTRA implements a Bayesian partition model where GWASs were clustered into ancestry clusters based on a prior model of similarity between them. MANTRA then uses Markov chain Monte Carlo (MCMC) algorithms to approximate the posterior distribution of parameters (which might be quite computationally intensive). MANTRA has been shown to increase power and mapping resolution over random-effects meta-analysis over a range of models of heterogeneity situations.</p>"},{"location":"11_meta_analysis/#mr-mega","title":"MR-MEGA","text":"<p>MR-MEGA employs meta-regression to model the heterogeneity in effect sizes across ancestries. Its official documentation can be found here (The same first author as GWAMA).</p> <p>Meta-regression implemented in MR-MEGA</p> <p>It will first construct a matrix \\(D\\) of pairwise Euclidean distances between GWAS across autosomal     variants. The elements of D , $d_{k'k} $ for a pair of studies can be expressed as the following.     For each variant \\(j\\), \\(p_{kj}\\) is the allele frequency of j in study k, then:</p> \\[d_{k'k} = {{\\sum_jI_j(p_{kj}-p_{k'j})^2}\\over{\\sum_jI_j}}\\] <ul> <li>\\(I\\) : an indicator of the inclusion of the \\(j\\)th variant </li> </ul> <p>Then multi-dimensional scaling (MDS) will be performed to derive T axes of genetic variation (\\(x_k\\)     for study k)</p> <p>For each variant j, the effect size of the reference allele can be modeled in a linear regression     model as :</p> \\[E[\\beta_{kj}] = \\beta_j + \\sum_{t=1}^T\\beta_{tj}x_{kj}\\] <ul> <li>\\(\\beta_j\\) : intercept</li> <li>\\(\\beta_{tj}\\) : the effect size of the \\(t\\) th axis of genetic variation for the \\(j\\) th variant</li> </ul> <p>A minimal example of meta-analysis using MR-MEGA</p> <p>The input file for MR-MEGA contains the path to each sumstats. Column names need to be standardized like GWAMA.</p> MRMEGA_script.in<pre><code>Pop1.txt.gz\nPop2.txt.gz\nPop3.txt.gz\nPop4.txt.gz\nPop5.txt.gz\nPop6.txt.gz\nPop7.txt.gz\nPop8.txt.gz\n</code></pre> <pre><code>MR-MEGA \\\n-i MRMEGA_script.in \\\n--pc 4 \\\n-o myresults\n</code></pre>"},{"location":"11_meta_analysis/#global-biobank-meta-analysis-initiative-gbmi","title":"Global Biobank Meta-analysis Initiative (GBMI)","text":"<p>As a recent success achieved by meta-analysis, GBMI showed an example of the improvement of our understanding of diseases by taking advantage of large-scale meta-analyses.</p> <p>For more details, you check check here.</p>"},{"location":"11_meta_analysis/#reference","title":"Reference","text":"<ul> <li>review : Zeggini, E., &amp; Ioannidis, J. P. (2009). Meta-analysis in genome-wide association studies.</li> <li>review : Evangelou, E., &amp; Ioannidis, J. P. (2013). Meta-analysis methods for genome-wide association studies and beyond. Nature Reviews Genetics, 14(6), 379-389.</li> <li>metal : Willer, C. J., Li, Y., &amp; Abecasis, G. R. (2010). METAL: fast and efficient meta-analysis of genomewide association scans. Bioinformatics, 26(17), 2190-2191.</li> <li>gwama : M\u00e4gi, R., &amp; Morris, A. P. (2010). GWAMA: software for genome-wide association meta-analysis. BMC bioinformatics, 11, 1-6.</li> <li>mantra: Morris, A. P. (2011). Transethnic meta\u2010analysis of genomewide association studies. Genetic epidemiology, 35(8), 809-822.</li> <li>mr-mega :M\u00e4gi, R., Horikoshi, M., Sofer, T., Mahajan, A., Kitajima, H., Franceschini, N., ... &amp; Morris, A. P. (2017). Trans-ethnic meta-regression of genome-wide association studies accounting for ancestry increases power for discovery and improves fine-mapping resolution. Human molecular genetics, 26(18), 3639-3650.</li> <li>GBMI : Zhou, W., Kanai, M., Wu, K. H. H., Rasheed, H., Tsuo, K., Hirbo, J. B., ... &amp; Study, C. O. H. (2022). Global Biobank Meta-analysis Initiative: Powering genetic discovery across human disease. Cell Genomics, 2(10), 100192.</li> </ul>"},{"location":"12_fine_mapping/","title":"Fine-mapping","text":""},{"location":"12_fine_mapping/#introduction","title":"Introduction","text":"<p>Fine-mapping : Fine-mapping aims to identfiy the causal variant(s) within a locus for a disease, given the evidence of the significant association of the locus (or genomic region) in GWAS of a disease. </p> <p>Fine-mapping using individual data is usually performed by fitting the multiple linear regression model:</p> \\[y = Xb + e\\] <ul> <li>\\(b = (b_1, \u2026, b_J)^T\\) is a vector of genetic effects of variants.</li> </ul> <p>The aim of fine-mapping is to estimate the PIP (posterior inclusion probability), which indicates the evidence for SNP j having a non-zero effect (namely, causal) :</p> \\[ PIP_j:=Pr(b_j\\neq0|X,y) \\] <p>Commonly used tools for fine-mapping</p> <ul> <li>FINEMAP</li> <li>SUSIE / SUSIE-RSS</li> <li>CAVIAR</li> <li>PAINTOR</li> </ul> <p>You can check here for more information.</p> <p>In this tutorial, we will introduce SuSiE as an example. SuSiE stands for Sum of Single Effects\u201d model.</p> <p>The key idea behind SuSiE is : </p> \\[b = \\sum_{l=1}^L b_l \\] <p>where each vector \\(b_l = (b_{l1}, \u2026, b_{lJ})^T\\) is a so-called single effect vector (a vector with only one non-zero element).  L is the upper bound of number of causal variants. And this model could be fitted using Iterative Bayesian Stepwise Selection (IBSS).</p> <p>For fine-mapping with summary statistics using Susie (SuSiE-RSS), IBSS was modified (IBSS-ss) to take sufficient statistics (which can be computed from other combinations of summary statistics) as input. SuSie will then approximate the sufficient statistics to run fine-mapping. </p> <p>Quote</p> <p>For details of SuSiE and SuSiE-RSS, please check : Zou, Y., Carbonetto, P., Wang, G., &amp; Stephens, M. (2022). Fine-mapping from summary data with the \u201cSum of Single Effects\u201d model. PLoS Genetics, 18(7), e1010299. Link</p>"},{"location":"12_fine_mapping/#file-preparation","title":"File Preparation","text":"<ul> <li>Sumstats for the loci</li> <li>SNP list for calculating LD matrix</li> </ul> <p>Using python to check novel loci and extract the files.</p> <p><pre><code>import gwaslab as gl\nimport pandas as pd\nimport numpy as np\n\nsumstats = gl.Sumstats(\"../06_Association_tests/1kgeas.B1.glm.firth\",fmt=\"plink2\")\n...\n\nsumstats.basic_check()\n...\n\nsumstats.get_lead()\n\nFri Jan 13 23:31:43 2023 Start to extract lead variants...\nFri Jan 13 23:31:43 2023  -Processing 1122285 variants...\nFri Jan 13 23:31:43 2023  -Significance threshold : 5e-08\nFri Jan 13 23:31:43 2023  -Sliding window size: 500  kb\nFri Jan 13 23:31:44 2023  -Found 59 significant variants in total...\nFri Jan 13 23:31:44 2023  -Identified 3 lead variants!\nFri Jan 13 23:31:44 2023 Finished extracting lead variants successfully!\n\nSNPID CHR POS EA  NEA SE  Z P OR  N STATUS\n110723  2:55574452:G:C  2 55574452  C G 0.160948  -5.98392  2.178320e-09  0.381707  503 9960099\n424615  6:29919659:T:C  6 29919659  T C 0.155457  -5.89341  3.782970e-09  0.400048  503 9960099\n635128  9:36660672:A:G  9 36660672  G A 0.160275  5.63422 1.758540e-08  2.467060  503 9960099\n</code></pre> We will perform fine-mapping for the first significant loci whose lead variant is <code>2:55574452:G:C</code>.</p> <pre><code># filter in the variants in the this locus.\n\nlocus = sumstats.filter_value('CHR==2 &amp; POS&gt;55074452 &amp; POS&lt;56074452')\nlocus.fill_data(to_fill=[\"BETA\"])\nlocus.harmonize(basic_check=False, ref_seq=\"/Users/he/mydata/Reference/Genome/human_g1k_v37.fasta\")\nlocus.data.to_csv(\"sig_locus.tsv\",sep=\"\\t\",index=None)\nlocus.data[\"SNPID\"].to_csv(\"sig_locus.snplist\",sep=\"\\t\",index=None,header=None)\n</code></pre> <p>check in terminal: <pre><code>head sig_locus.tsv\nSNPID   CHR     POS     EA      NEA     BETA    SE      Z       P       OR      N       STATUS\n2:54535206:C:T  2       54535206        T       C       0.30028978      0.142461        2.10786 0.0350429       1.35025 503     9960099\n2:54536167:C:G  2       54536167        G       C       0.14885099      0.246871        0.602952        0.546541        1.1605  503     9960099\n2:54539096:A:G  2       54539096        G       A       -0.0038474211   0.288489        -0.0133355      0.98936 0.99616 503     9960099\n2:54540264:G:A  2       54540264        A       G       -0.1536723      0.165879        -0.926409       0.354234        0.857553        503     9960099\n2:54540614:G:T  2       54540614        T       G       -0.1536723      0.165879        -0.926409       0.354234        0.857553        503     9960099\n2:54540621:A:G  2       54540621        G       A       -0.1536723      0.165879        -0.926409       0.354234        0.857553        503     9960099\n2:54540970:T:C  2       54540970        C       T       -0.049506452    0.149053        -0.332144       0.739781        0.951699        503     9960099\n2:54544229:T:C  2       54544229        C       T       -0.14338203     0.151172        -0.948468       0.342891        0.866423        503     9960099\n2:54545593:T:C  2       54545593        C       T       -0.1536723      0.165879        -0.926409       0.354234        0.857553        503     9960099\n\nhead  sig_locus.snplist\n2:54535206:C:T\n2:54536167:C:G\n2:54539096:A:G\n2:54540264:G:A\n2:54540614:G:T\n2:54540621:A:G\n2:54540970:T:C\n2:54544229:T:C\n2:54545593:T:C\n2:54546032:C:G\n</code></pre></p>"},{"location":"12_fine_mapping/#ld-matrix-calculation","title":"LD Matrix Calculation","text":"<p>Example</p> <p><pre><code>#!/bin/bash\n\nplinkFile=\"../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\"\n\n# LD r matrix\nplink \\\n  --bfile ${plinkFile} \\\n  --keep-allele-order \\\n  --r square \\\n  --extract sig_locus.snplist \\\n  --out sig_locus_mt\n\n# LD r2 matrix\nplink \\\n  --bfile ${plinkFile} \\\n  --keep-allele-order \\\n  --r2 square \\\n  --extract sig_locus.snplist \\\n  --out sig_locus_mt_r2\n</code></pre> Take a look at the LD matrix (first 5 rows and columns)</p> <p><pre><code>head -5 sig_locus_mt.ld | cut -f 1-5\n1       -0.145634       0.252616        -0.0876317      -0.0876317\n-0.145634       1       -0.0916734      -0.159635       -0.159635\n0.252616        -0.0916734      1       0.452333        0.452333\n-0.0876317      -0.159635       0.452333        1       1\n-0.0876317      -0.159635       0.452333        1       1\n\nhead -5 sig_locus_mt_r2.ld | cut -f 1-5\n1       0.0212091       0.0638148       0.00767931      0.00767931\n0.0212091       1       0.00840401      0.0254833       0.0254833\n0.0638148       0.00840401      1       0.204605        0.204605\n0.00767931      0.0254833       0.204605        1       1\n0.00767931      0.0254833       0.204605        1       1\n</code></pre> Heatmap of the LD matrix:</p> <p></p>"},{"location":"12_fine_mapping/#fine-mapping-with-summary-statistics-using-susier","title":"Fine-mapping with summary statistics using SusieR","text":"<p>Note</p> <pre><code>install.packages(\"susieR\")\n\n# Fine-mapping with summary statistics\nfitted_rss2 = susie_rss(bhat = sumstats$betahat, shat = sumstats$sebetahat, R = R, n = n, L = 10)\n</code></pre> <p><code>R</code> : a <code>p</code> x <code>p</code> LD r matrix. <code>N</code> : Sample size. <code>bhat</code> : Alternative summary data giving the estimated effects (a vector of length <code>p</code>). This, together with shat, may be provided instead of z. <code>shat</code> : Alternative summary data giving the standard errors of the estimated effects (a vector of length <code>p</code>). This, together with bhat, may be provided instead of z. <code>L</code> : Maximum number of non-zero effects in the susie regression model. (defaul : <code>L = 10</code>)</p> <p>Quote</p> <p>For deatils, please check SusieR tutorial - Fine-mapping with susieR using summary statistics</p> <p>Use susieR in jupyter notebook (with Python):</p> <p>Please check : https://github.com/Cloufield/GWASTutorial/blob/main/12_fine_mapping/finemapping_susie.ipynb</p> <p></p>"},{"location":"12_fine_mapping/#reference","title":"Reference","text":"<ul> <li>Wang, G., Sarkar, A., Carbonetto, P. &amp; Stephens, M. (2020). A simple new approach to variable selection in regression, with application to genetic fine mapping. Journal of the Royal Statistical Society, Series B 82, 1273\u20131300. https://doi.org/10.1111/rssb.12388</li> <li>Zou, Y., Carbonetto, P., Wang, G. &amp; Stephens, M. (2021). Fine-mapping from summary data with the \u201cSum of Single Effects\u201d model. bioRxiv https://doi.org/10.1101/2021.11.03.467167</li> <li>Schaid, Daniel J., Wenan Chen, and Nicholas B. Larson. \"From genome-wide associations to candidate causal variants by statistical fine-mapping.\" Nature Reviews Genetics 19.8 (2018): 491-504.</li> <li>Purcell, Shaun, et al. \"PLINK: a tool set for whole-genome association and population-based linkage analyses.\" The American journal of human genetics 81.3 (2007): 559-575.</li> </ul>"},{"location":"13_heritability/","title":"Heritability","text":"<p>Heritability is a term used in genetics to describe how much phenotypic variation can be explained by genetic variation.</p> <p>For any phenotype, its variation \\(Var(P)\\) can be modeled as the combination of genetic effects \\(Var(G)\\) and environmental effects \\(Var(E)\\).</p> \\[ Var(P) = Var(G) + Var(E) \\]"},{"location":"13_heritability/#broad-sense-heritability","title":"Broad-sense Heritability","text":"<p>The broad-sense heritability \\(H^2_{broad-sense}\\) is mathmatically defined as :</p> \\[ H^2_{broad-sense} = {Var(G)\\over{Var(P)}} \\]"},{"location":"13_heritability/#narrow-sense-heritability","title":"Narrow-sense Heritability","text":"<p>Genetic effects \\(Var(G)\\) is composed of multiple effects including additive effects \\(Var(A)\\), dominant effects, recessive effects, epistatic effects and so forth.</p> <p>Narrrow-sense heritability is defined as: </p> \\[ h^2_{narrow-sense} = {Var(A)\\over{Var(P)}} \\]"},{"location":"13_heritability/#snp-heritability","title":"SNP Heritability","text":"<p>SNP heritability \\(h^2_{SNP}\\) : the proportion of phenotypic variance explained by tested SNPs in a GWAS.</p> <p>Common methods to estimate SNP heritability includes:</p> <ul> <li>GCTA-GREML  (based on Genome-based  Restricted  Maximum  Likelihood)</li> <li>LDSC (based on LD score regression)</li> </ul>"},{"location":"13_heritability/#liability-and-threshold-model","title":"Liability and Threshold model","text":""},{"location":"13_heritability/#observed-scale-heritability-and-liability-scaled-heritability","title":"Observed-scale heritability and liability-scaled heritability","text":"<p>Issue for binary traits : </p> <p>The scale issue for binary traits</p> <ul> <li>For quantitative traits the scale of measurement is the same as the scale on which heritability is expressed. </li> <li>For disease traits, the phenotypes (case-control status) are measured on the 0\u20131 scale, but heritability is most interpretable on a scale of liability.</li> <li>Reference: Lee, S. H., Wray, N. R., Goddard, M. E., &amp; Visscher, P. M. (2011). Estimating missing heritability for disease from genome-wide association studies. The American Journal of Human Genetics, 88(3), 294-305.</li> </ul> <p>Conversion formula (Equation 23 from Lee. 2011):</p> \\[ h^2_{liability-scale} = h^2_{observed-scale} * {{K(1-K)}\\over{Z^2}} *  {{K(1-K)}\\over{P(1-P)}} \\] <ul> <li>\\(K\\) : Population disease prevalence.</li> <li>\\(P\\) : Sample disease prevalence.</li> <li>\\(Z\\) : The height of the standard normal probability density function at threshold T. <code>scipy.stats.norm.pdf(T, loc=0, scale=1)</code>.</li> <li>\\(T\\) : The threshold. <code>scipy.stats.norm.ppf(1 - K, loc=0, scale=1)</code> or <code>scipy.stats.norm.isf(K)</code>.</li> </ul>"},{"location":"13_heritability/#further-reading","title":"Further Reading","text":"<ul> <li>(Blog by Neale Lab) http://www.nealelab.is/blog/2017/9/13/heritability-101-what-is-heritability</li> <li>Manolio, T. A., Collins, F. S., Cox, N. J., Goldstein, D. B., Hindorff, L. A., Hunter, D. J., ... &amp; Visscher, P. M. (2009). Finding the missing heritability of complex diseases. Nature, 461(7265), 747-753.</li> <li>Visscher, P. M., Hill, W. G., &amp; Wray, N. R. (2008). Heritability in the genomics era\u2014concepts and misconceptions. Nature reviews genetics, 9(4), 255-266.</li> <li>Yang, J., Zeng, J., Goddard, M. E., Wray, N. R., &amp; Visscher, P. M. (2017). Concepts, estimation and interpretation of SNP-based heritability. Nature genetics, 49(9), 1304-1310.</li> <li>Witte, J. S., Visscher, P. M., &amp; Wray, N. R. (2014). The contribution of genetic variants to disease depends on the ruler. Nature Reviews Genetics, 15(11), 765-776.</li> </ul>"},{"location":"14_gcta_greml/","title":"SNP-Heritability estimation by GCTA-GREML","text":""},{"location":"14_gcta_greml/#introduction","title":"Introduction","text":"<p>The basic model behind GCTA-GREML is the linear mixed model (LMM):</p> \\[y = X\\beta + Wu + e\\] \\[ Var(y) = V = WW^{'}\\delta^2_u + I \\delta^2_e\\] <ul> <li>\\(X\\) :  covariate matrix</li> <li>\\(W\\) :  standardized genotype matrix</li> </ul> <p>GCTA defines \\(A = WW^{'}/N\\) and \\(\\delta^2_g\\) as the variance explained by SNPs.</p> <p>So the oringinal model can be written as:</p> \\[y = X\\beta + g + e\\] <ul> <li>\\(g\\) : a vector of total genetic effects</li> </ul> \\[ Var(y) = V = A\\delta^2_g + I \\delta^2_e\\] <ul> <li>\\(A\\) can be regarded as genetic relationship matrix (GRM)  </li> <li>\\(\\delta^2_e\\) can be estimated by the restricted maximum likelihood (REML) method using all SNPs.</li> </ul> <p>Quote</p> <p>For details, please check Yang, J., Lee, S. H., Goddard, M. E., &amp; Visscher, P. M. (2011). GCTA: a tool for genome-wide complex trait analysis. The American Journal of Human Genetics, 88(1), 76-82. link.</p>"},{"location":"14_gcta_greml/#donwload","title":"Donwload","text":"<p>Download the version of GCTA for your system from : https://yanglab.westlake.edu.cn/software/gcta/#Download</p> <p>Example</p> <pre><code>wget https://yanglab.westlake.edu.cn/software/gcta/bin/gcta-1.94.1-linux-kernel-3-x86_64.zip\nunzip gcta-1.94.1-linux-kernel-3-x86_64.zip\ncd gcta-1.94.1-linux-kernel-3-x86_64.zip\n\n./gcta-1.94.1\n*******************************************************************\n* Genome-wide Complex Trait Analysis (GCTA)\n* version v1.94.1 Linux\n* Built at Nov 15 2022 21:14:25, by GCC 8.5\n* (C) 2010-present, Yang Lab, Westlake University\n* Please report bugs to Jian Yang &lt;jian.yang@westlake.edu.cn&gt;\n*******************************************************************\nAnalysis started at 12:22:19 JST on Sun Jan 15 2023.\nHostname: Home-Desktop\n\nError: no analysis has been launched by the option(s)\nPlease see online documentation at https://yanglab.westlake.edu.cn/software/gcta/\n</code></pre> <p>Tip</p> <p>Add GCTA to your environment</p>"},{"location":"14_gcta_greml/#make-grm","title":"Make GRM","text":"<pre><code>#!/bin/bash\nplinkFile=\"../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\"\ngcta \\\n--bfile ${plinkFile} \\\n--autosome \\\n--maf 0.01 \\\n--make-grm \\\n--out 1kg_eas\n</code></pre> <pre><code>*******************************************************************\n* Genome-wide Complex Trait Analysis (GCTA)\n* version v1.94.1 Linux\n* Built at Nov 15 2022 21:14:25, by GCC 8.5\n* (C) 2010-present, Yang Lab, Westlake University\n* Please report bugs to Jian Yang &lt;jian.yang@westlake.edu.cn&gt;\n*******************************************************************\nAnalysis started at 12:24:19 JST on Sun Jan 15 2023.\nHostname: Home-Desktop\n\nOptions:\n\n--bfile ../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\n--autosome\n--maf 0.01\n--make-grm\n--out 1kg_eas\n\nNote: GRM is computed using the SNPs on the autosomes.\nReading PLINK FAM file from [../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.fam]...\n504 individuals to be included from FAM file.\n504 individuals to be included. 0 males, 0 females, 504 unknown.\nReading PLINK BIM file from [../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020.bim]...\n1122299 SNPs to be included from BIM file(s).\nThreshold to filter variants: MAF &gt; 0.010000.\nComputing the genetic relationship matrix (GRM) v2 ...\nSubset 1/1, no. subject 1-504\n  504 samples, 1122299 markers, 127260 GRM elements\nIDs for the GRM file have been saved in the file [1kg_eas.grm.id]\nComputing GRM...\n  100% finished in 8.2 sec\n1122299 SNPs have been processed.\n  Used 1122299 valid SNPs.\nThe GRM computation is completed.\nSaving GRM...\nGRM has been saved in the file [1kg_eas.grm.bin]\nNumber of SNPs in each pair of individuals has been saved in the file [1kg_eas.grm.N.bin]\n\nAnalysis finished at 12:24:29 JST on Sun Jan 15 2023\nOverall computational time: 9.52 sec.\n</code></pre>"},{"location":"14_gcta_greml/#estimation","title":"Estimation","text":"<pre><code>#!/bin/bash\n\n#the grm we calculated in step1\nGRM=1kg_eas\n# phenotype file\nphenotypeFile=../01_Dataset/1kgeas_binary.txt\n# disease prevalence used for conversion to liability-scale heritability\nprevalence=0.4\n\ngcta \\\n  --grm ${GRM} \\\n  --pheno ${phenotypeFIile} \\\n  --prevalence ${prevalence} \\\n  --reml \\\n  --out 1kg_eas\n</code></pre>"},{"location":"14_gcta_greml/#results","title":"Results","text":"<pre><code>head 1kg_eas.hsq\n\nSource  Variance        SE\nV(G)    0.100469        0.094051\nV(e)    0.138337        0.093889\nVp      0.238806        0.015077\nV(G)/Vp 0.420715        0.392317\nThe estimate of variance explained on the observed scale is transformed to that on the underlying scale:\n(Proportion of cases in the sample = 0.399602; User-specified disease prevalence = 0.400000)\nV(G)/Vp_L       0.676703        0.631026\nlogL    105.187\nlogL0   103.680\n</code></pre>"},{"location":"14_gcta_greml/#reference","title":"Reference","text":"<ul> <li>Yang, J., Lee, S. H., Goddard, M. E., &amp; Visscher, P. M. (2011). GCTA: a tool for genome-wide complex trait analysis. The American Journal of Human Genetics, 88(1), 76-82.</li> <li>https://yanglab.westlake.edu.cn/software/gcta/#Overview</li> </ul>"},{"location":"15_winners_curse/","title":"Winner's curse","text":""},{"location":"15_winners_curse/#winners-curse-definition","title":"Winner's curse definition","text":"<p>Winner's curse refers to the phenomenon that genetic effects are systematically overestimated by thresholding or selection process in genetic association studies. </p> <p>Winner's curse in auctions</p> <p>This term was initially used to describe a phenomenon that occurs in auctions. The winning bid is very likely to overestimate the intrinsic value of an item even if all the bids are unbiased (the auctioned item is of equal value to all bidders). The thresholding process in GWAS resembles auctions, where the lead variants are the winning bids. </p> <p>Reference: </p> <ul> <li>Bazerman, M. H., &amp; Samuelson, W. F. (1983). I won the auction but don't want the prize. Journal of conflict resolution, 27(4), 618-634.</li> <li>G\u00f6ring, H. H., Terwilliger, J. D., &amp; Blangero, J. (2001). Large upward bias in estimation of locus-specific effects from genomewide scans. The American Journal of Human Genetics, 69(6), 1357-1369.</li> </ul>"},{"location":"15_winners_curse/#wc-correction","title":"WC correction","text":"<p>The asymptotic distribution of \\(\\beta_{Observed}\\) is:</p> \\[\\beta_{Observed} \\sim N(\\beta_{True},\\sigma^2)\\] <p>An example of distribution of \\(\\beta_{Observed}\\)</p> <p></p> <ul> <li>\\(c\\) :  Z score cutpoint corresponding to the significance threshold.</li> </ul> <p>It is equivalent to:</p> \\[{{\\beta_{Observed} - \\beta_{True}}\\over{\\sigma}} \\sim N(0,1)\\] <p>An example of distribution of \\({{\\beta_{Observed} - \\beta_{True}}\\over{\\sigma}}\\)</p> <p></p> <p>We can obtain the asymptotic sampling distribution (which is a truncated normal distribution) for \\(\\beta_{Observed}\\) by:</p> \\[f(x,\\beta_{True}) ={{1}\\over{\\sigma}} {{\\phi({{{x - \\beta_{True}}\\over{\\sigma}}})} \\over {\\Phi({{{\\beta_{True}}\\over{\\sigma}}-c}) + \\Phi({{{-\\beta_{True}}\\over{\\sigma}}-c})}}\\] <p>when</p> \\[|{{x}\\over{\\sigma}}|\\geq c\\] <ul> <li>\\(\\phi(x)\\) : standard normal density function.</li> <li>\\(\\Phi(x)\\) : standard normal cumulative density function.</li> </ul> <p>From the asymptotic sampling distribution, the expectation of effect sizes for the selected variants can then be approximated by: </p> \\[ E(\\beta_{Observed}; \\beta_{True}) = \\beta_{True} + \\sigma {{\\phi({{{\\beta_{True}}\\over{\\sigma}}-c}) - \\phi({{{-\\beta_{True}}\\over{\\sigma}}-c})} \\over {\\Phi({{{\\beta_{True}}\\over{\\sigma}}-c}) + \\Phi({{{-\\beta_{True}}\\over{\\sigma}}-c})}}\\] <ul> <li>\\(\\beta_{Observed}\\) is biased. </li> <li>The bias is dependent on \\(\\beta_{True}\\), SE \\(\\sigma\\), and the selection threshold.</li> </ul> <p>Derivation of this equation can be found in the Appendix A of Ghosh, A., Zou, F., &amp; Wright, F. A. (2008). Estimating odds ratios in genome scans: an approximate conditional likelihood approach. The American Journal of Human Genetics, 82(5), 1064-1074.</p> <p>Reference: </p> <ul> <li>Zhong, H., &amp; Prentice, R. L. (2008). Bias-reduced estimators and confidence intervals for odds ratios in genome-wide association studies. Biostatistics, 9(4), 621-634.</li> <li>Ghosh, A., Zou, F., &amp; Wright, F. A. (2008). Estimating odds ratios in genome scans: an approximate conditional likelihood approach. The American Journal of Human Genetics, 82(5), 1064-1074.</li> </ul> <p>Also see reference: https://amandaforde.github.io/winnerscurse/articles/winners_curse_methods.html</p>"},{"location":"16_mendelian_randomization/","title":"Mendelian randomization","text":""},{"location":"16_mendelian_randomization/#mendelian-randomization-introduction","title":"Mendelian randomization introduction","text":"<p>Comparison between RCT and MR</p> <p></p>"},{"location":"16_mendelian_randomization/#instrumental-variables-iv","title":"Instrumental Variables (IV)","text":"<p>Instrumental variable (IV) can be defined as a variable  that is correlated with the exposure X and uncorrelated with the error \\(\\epsilon\\) in the following regression: </p> \\[ Y = X\\beta + \\epsilon \\] <ul> <li>\\(Y\\) is the outcome</li> </ul>"},{"location":"16_mendelian_randomization/#assumptions","title":"Assumptions","text":"<p>Key Assumptions</p> Assumptions Description Relevance Instrumental variables are strongly associated with the exposure. Exclusion restriction Instrumental variables do not affect the outcome except through the exposure. Independence There are no confounders of the instrumental variables and the outcome. Monotonicity Variants affect the exposure in the same direction for all individuals No assortative mating Assortative mating might cause bias in MR"},{"location":"16_mendelian_randomization/#two-stage-least-squares-2sls","title":"Two-stage least-squares (2SLS)","text":"\\[ X = \\mu_1 + \\beta_{IV} IV + \\epsilon_1  \\] \\[ Y = \\mu_2 + \\beta_{2SLS} \\hat{X} + \\epsilon_2 \\]"},{"location":"16_mendelian_randomization/#two-sample-mr","title":"Two-sample MR","text":"<p>Two-sample MR refers to the approach that the genetic effects of the instruments on the exposure can be estimated in an independent sample other than that used to estimate effects between instruments on the outcome. As more and more GWAS summary statistics become publicly available, the scope of MR also expands with Two-sample MR methods.</p> \\[ \\hat{\\beta}_{X,Y} = {{\\hat{\\beta}_{IV,Y}}\\over{\\hat{\\beta}_{IV,X}}} \\] <p>Caveats</p> <p>For two-sample MR, there is an additional key assumption:</p> <p>The two samples used for MR are from the same underlying populations. (The effect size of instruments on exposure should be the same in both samples.) </p> <p>Therefore, for two-sample MR, we usually use datasets from similar non-overlapping populations in terms of not only ancestry but also contextual factors. </p>"},{"location":"16_mendelian_randomization/#iv-selection","title":"IV selection","text":"<p>One of the first things to do when you plan to perform any type of MR is to check the associations of instrumental variables with the exposure to avoid bias caused by weak IVs.</p> <p>The most commonly used method here is the F-statistic, which tests the association of instrumental variables with the exposure.</p>"},{"location":"16_mendelian_randomization/#practice","title":"Practice","text":"<p>In this tutorial, we will walk you through how to perform a minimal TwoSampleMR analysis. We will use the R package TwoSampleMR, which provides easy-to-use functions for formatting, clumping and harmonizing GWAS summary statistics. </p> <p>This package integrates a variety of commonly used MR methods for analysis, including: <pre><code>&gt; mr_method_list()\n                             obj\n1                  mr_wald_ratio\n2               mr_two_sample_ml\n3            mr_egger_regression\n4  mr_egger_regression_bootstrap\n5               mr_simple_median\n6             mr_weighted_median\n7   mr_penalised_weighted_median\n8                         mr_ivw\n9                  mr_ivw_radial\n10                    mr_ivw_mre\n11                     mr_ivw_fe\n12                mr_simple_mode\n13              mr_weighted_mode\n14         mr_weighted_mode_nome\n15           mr_simple_mode_nome\n16                       mr_raps\n17                       mr_sign\n18                        mr_uwr\n\n                                                        name PubmedID\n1                                                 Wald ratio\n2                                         Maximum likelihood\n3                                                   MR Egger 26050253\n4                                       MR Egger (bootstrap) 26050253\n5                                              Simple median\n6                                            Weighted median\n7                                  Penalised weighted median\n8                                  Inverse variance weighted\n9                                                 IVW radial\n10 Inverse variance weighted (multiplicative random effects)\n11                 Inverse variance weighted (fixed effects)\n12                                               Simple mode\n13                                             Weighted mode\n14                                      Weighted mode (NOME)\n15                                        Simple mode (NOME)\n16                      Robust adjusted profile score (RAPS)\n17                                     Sign concordance test\n18                                     Unweighted regression\n</code></pre></p>"},{"location":"16_mendelian_randomization/#file-preparation","title":"File Preparation","text":"<p>To perform two-sample MR analysis, we need summary statistics for exposure and outcome generated from independent populations with the same ancestry.</p> <p>In this tutorial, we will use sumstats from Biobank Japan pheweb and KoGES pheweb.</p> <ul> <li>Type 2 diabetes sumstats from BBJ : <code>wget -O bbj_t2d.zip https://pheweb.jp/download/T2D</code></li> <li>BMI sumstats from KoGES :  <code>wget -O koges_bmi.txt.gz https://koges.leelabsg.org/download/KoGES_BMI</code></li> </ul>"},{"location":"16_mendelian_randomization/#r-package-twosamplemr","title":"R package TwoSampleMR","text":"<p>First, to use TwosampleMR, we need R&gt;= 4.1. To install the package, run:</p> <pre><code>library(remotes)\ninstall_github(\"MRCIEU/TwoSampleMR\")\n</code></pre>"},{"location":"16_mendelian_randomization/#loading-package","title":"Loading package","text":"<pre><code>library(TwoSampleMR)\n</code></pre>"},{"location":"16_mendelian_randomization/#reading-exposure-sumstats","title":"Reading exposure sumstats","text":"<pre><code>#format exposures dataset\n\nexp_raw &lt;- fread(\"koges_bmi.txt.gz\")\n</code></pre>"},{"location":"16_mendelian_randomization/#extracting-instrumental-variables","title":"Extracting instrumental variables","text":"<pre><code># select only significant variants\nexp_raw &lt;- subset(exp_raw,exp_raw$pval&lt;5e-8)\n\nexp_dat &lt;- format_data( exp_raw,\ntype = \"exposure\",\nsnp_col = \"rsids\",\nbeta_col = \"beta\",\nse_col = \"sebeta\",\neffect_allele_col = \"alt\",\nother_allele_col = \"ref\",\neaf_col = \"af\",\npval_col = \"pval\",\n)\n</code></pre>"},{"location":"16_mendelian_randomization/#clumping-exposure-variables","title":"Clumping exposure variables","text":"<pre><code>clumped_exp &lt;- clump_data(exp_dat,clump_r2=0.01,pop=\"EAS\") </code></pre>"},{"location":"16_mendelian_randomization/#outcome","title":"outcome","text":"<pre><code>out_raw &lt;- fread(\"hum0197.v3.BBJ.T2D.v1/GWASsummary_T2D_Japanese_SakaueKanai2020.auto.txt.gz\",\nselect=c(\"SNPID\",\"Allele1\",\"Allele2\",\"BETA\",\"SE\",\"p.value\"))\nout_dat &lt;- format_data( out_raw,\ntype = \"outcome\",\nsnp_col = \"SNPID\",\nbeta_col = \"BETA\",\nse_col = \"SE\",\neffect_allele_col = \"Allele2\",\nother_allele_col = \"Allele1\",\npval_col = \"p.value\",\n)\n</code></pre>"},{"location":"16_mendelian_randomization/#harmonizing-data","title":"Harmonizing data","text":"<pre><code>harmonized_data &lt;- harmonise_data(clumped_exp,out_dat,action=1)\n</code></pre>"},{"location":"16_mendelian_randomization/#perform-mr-analysis","title":"Perform MR analysis","text":"<pre><code>res &lt;- mr(harmonized_data)\n\nid.exposure id.outcome  outcome exposure    method  nsnp    b   se  pval\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n9J8pv4  IyUv6b  outcome exposure    MR Egger    28  1.3337580   0.69485260  6.596064e-02\n9J8pv4  IyUv6b  outcome exposure    Weighted median 28  0.6298980   0.09401352  2.083081e-11\n9J8pv4  IyUv6b  outcome exposure    Inverse variance weighted   28  0.5598956   0.23225806  1.592361e-02\n9J8pv4  IyUv6b  outcome exposure    Simple mode 28  0.6097842   0.15180476  4.232158e-04\n9J8pv4  IyUv6b  outcome exposure    Weighted mode   28  0.5946778   0.12820220  8.044488e-05\n</code></pre>"},{"location":"16_mendelian_randomization/#sensitivity-analysis","title":"Sensitivity analysis","text":""},{"location":"16_mendelian_randomization/#heterogeneity","title":"Heterogeneity","text":"<pre><code>mr_heterogeneity(harmonized_data)\n\nid.exposure id.outcome  outcome exposure    method  Q   Q_df    Q_pval\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n9J8pv4  IyUv6b  outcome exposure    MR Egger    670.7022    26  1.000684e-124\n9J8pv4  IyUv6b  outcome exposure    Inverse variance weighted   706.6579    27  1.534239e-131\n</code></pre>"},{"location":"16_mendelian_randomization/#horizontal-pleiotropy","title":"Horizontal Pleiotropy","text":"<p>Intercept in MR-Egger</p> <pre><code>mr_pleiotropy_test(harmonized_data)\n\nid.exposure id.outcome  outcome exposure    egger_intercept se  pval\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n9J8pv4  IyUv6b  outcome exposure    -0.03603697 0.0305241   0.2484472\n</code></pre>"},{"location":"16_mendelian_randomization/#single-snp-mr-and-leave-one-out-mr","title":"Single SNP MR and leave-one-out MR","text":"<p>Single SNP MR</p> <pre><code>res_single &lt;- mr_singlesnp(harmonized_data)\nres_single\n\nexposure    outcome id.exposure id.outcome  samplesize  SNP b   se  p\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   exposure    outcome 9J8pv4  IyUv6b  NA  rs10198356  0.6323140   0.2082837   2.398742e-03\n2   exposure    outcome 9J8pv4  IyUv6b  NA  rs10209994  0.9477808   0.3225814   3.302164e-03\n3   exposure    outcome 9J8pv4  IyUv6b  NA  rs10824329  0.6281765   0.3246214   5.297739e-02\n4   exposure    outcome 9J8pv4  IyUv6b  NA  rs10938397  1.2376316   0.2775854   8.251150e-06\n5   exposure    outcome 9J8pv4  IyUv6b  NA  rs11066132  0.6024303   0.2232401   6.963693e-03\n6   exposure    outcome 9J8pv4  IyUv6b  NA  rs12522139  0.2905201   0.2890240   3.148119e-01\n7   exposure    outcome 9J8pv4  IyUv6b  NA  rs12591730  0.8930490   0.3076687   3.700413e-03\n8   exposure    outcome 9J8pv4  IyUv6b  NA  rs13013021  1.4867889   0.2207777   1.646925e-11\n9   exposure    outcome 9J8pv4  IyUv6b  NA  rs1955337   0.5442640   0.2994146   6.910079e-02\n10  exposure    outcome 9J8pv4  IyUv6b  NA  rs2076308   1.1176226   0.2657969   2.613132e-05\n11  exposure    outcome 9J8pv4  IyUv6b  NA  rs2278557   0.6238587   0.2968184   3.556906e-02\n12  exposure    outcome 9J8pv4  IyUv6b  NA  rs2304608   1.5054682   0.2968905   3.961740e-07\n13  exposure    outcome 9J8pv4  IyUv6b  NA  rs2531995   1.3972908   0.3130157   8.045689e-06\n14  exposure    outcome 9J8pv4  IyUv6b  NA  rs261967    1.5303384   0.2921192   1.616714e-07\n15  exposure    outcome 9J8pv4  IyUv6b  NA  rs35332469  -0.2307314  0.3479219   5.072217e-01\n16  exposure    outcome 9J8pv4  IyUv6b  NA  rs35560038  -1.5730870  0.2018968   6.619637e-15\n17  exposure    outcome 9J8pv4  IyUv6b  NA  rs3755804   0.5314915   0.2325073   2.225933e-02\n18  exposure    outcome 9J8pv4  IyUv6b  NA  rs4470425   0.6948046   0.3079944   2.407689e-02\n19  exposure    outcome 9J8pv4  IyUv6b  NA  rs476828    1.1739083   0.1568550   7.207355e-14\n20  exposure    outcome 9J8pv4  IyUv6b  NA  rs4883723   0.5479721   0.2855004   5.494141e-02\n21  exposure    outcome 9J8pv4  IyUv6b  NA  rs509325    0.5491040   0.1598196   5.908641e-04\n22  exposure    outcome 9J8pv4  IyUv6b  NA  rs55872725  1.3501891   0.1259791   8.419325e-27\n23  exposure    outcome 9J8pv4  IyUv6b  NA  rs6089309   0.5657525   0.3347009   9.096620e-02\n24  exposure    outcome 9J8pv4  IyUv6b  NA  rs6265  0.6457693   0.1901871   6.851804e-04\n25  exposure    outcome 9J8pv4  IyUv6b  NA  rs6736712   0.5606962   0.3448784   1.039966e-01\n26  exposure    outcome 9J8pv4  IyUv6b  NA  rs7560832   0.6032080   0.2904972   3.785077e-02\n27  exposure    outcome 9J8pv4  IyUv6b  NA  rs825486    -0.6152759  0.3500334   7.878772e-02\n28  exposure    outcome 9J8pv4  IyUv6b  NA  rs9348441   -4.9786332  0.2572782   1.992909e-83\n29  exposure    outcome 9J8pv4  IyUv6b  NA  All - Inverse variance weighted 0.5598956   0.2322581   1.592361e-02\n30  exposure    outcome 9J8pv4  IyUv6b  NA  All - MR Egger  1.3337580   0.6948526   6.596064e-02\n</code></pre> <p>leave-one-out MR</p> <pre><code>res_loo &lt;- mr_leaveoneout(harmonized_data)\nres_loo\n\nexposure    outcome id.exposure id.outcome  samplesize  SNP b   se  p\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   exposure    outcome 9J8pv4  IyUv6b  NA  rs10198356  0.5562834   0.2424917   2.178871e-02\n2   exposure    outcome 9J8pv4  IyUv6b  NA  rs10209994  0.5520576   0.2388122   2.079526e-02\n3   exposure    outcome 9J8pv4  IyUv6b  NA  rs10824329  0.5585335   0.2390239   1.945341e-02\n4   exposure    outcome 9J8pv4  IyUv6b  NA  rs10938397  0.5412688   0.2388709   2.345460e-02\n5   exposure    outcome 9J8pv4  IyUv6b  NA  rs11066132  0.5580606   0.2417275   2.096381e-02\n6   exposure    outcome 9J8pv4  IyUv6b  NA  rs12522139  0.5667102   0.2395064   1.797373e-02\n7   exposure    outcome 9J8pv4  IyUv6b  NA  rs12591730  0.5524802   0.2390990   2.085075e-02\n8   exposure    outcome 9J8pv4  IyUv6b  NA  rs13013021  0.5189715   0.2386808   2.968017e-02\n9   exposure    outcome 9J8pv4  IyUv6b  NA  rs1955337   0.5602635   0.2394505   1.929468e-02\n10  exposure    outcome 9J8pv4  IyUv6b  NA  rs2076308   0.5431355   0.2394403   2.330758e-02\n11  exposure    outcome 9J8pv4  IyUv6b  NA  rs2278557   0.5583634   0.2394924   1.972992e-02\n12  exposure    outcome 9J8pv4  IyUv6b  NA  rs2304608   0.5372557   0.2377325   2.382639e-02\n13  exposure    outcome 9J8pv4  IyUv6b  NA  rs2531995   0.5419016   0.2379712   2.277590e-02\n14  exposure    outcome 9J8pv4  IyUv6b  NA  rs261967    0.5358761   0.2376686   2.415093e-02\n15  exposure    outcome 9J8pv4  IyUv6b  NA  rs35332469  0.5735907   0.2378345   1.587739e-02\n16  exposure    outcome 9J8pv4  IyUv6b  NA  rs35560038  0.6734906   0.2217804   2.391474e-03\n17  exposure    outcome 9J8pv4  IyUv6b  NA  rs3755804   0.5610215   0.2413249   2.008503e-02\n18  exposure    outcome 9J8pv4  IyUv6b  NA  rs4470425   0.5568993   0.2392632   1.993549e-02\n19  exposure    outcome 9J8pv4  IyUv6b  NA  rs476828    0.5037555   0.2443224   3.922224e-02\n20  exposure    outcome 9J8pv4  IyUv6b  NA  rs4883723   0.5602050   0.2397325   1.945000e-02\n21  exposure    outcome 9J8pv4  IyUv6b  NA  rs509325    0.5608429   0.2468506   2.308693e-02\n22  exposure    outcome 9J8pv4  IyUv6b  NA  rs55872725  0.4419446   0.2454771   7.180543e-02\n23  exposure    outcome 9J8pv4  IyUv6b  NA  rs6089309   0.5597859   0.2388902   1.911519e-02\n24  exposure    outcome 9J8pv4  IyUv6b  NA  rs6265  0.5547068   0.2436910   2.282978e-02\n25  exposure    outcome 9J8pv4  IyUv6b  NA  rs6736712   0.5598815   0.2387602   1.902944e-02\n26  exposure    outcome 9J8pv4  IyUv6b  NA  rs7560832   0.5588113   0.2396229   1.969836e-02\n27  exposure    outcome 9J8pv4  IyUv6b  NA  rs825486    0.5800026   0.2367545   1.429330e-02\n28  exposure    outcome 9J8pv4  IyUv6b  NA  rs9348441   0.7378967   0.1366838   6.717515e-08\n29  exposure    outcome 9J8pv4  IyUv6b  NA  All 0.5598956   0.2322581   1.592361e-02\n</code></pre>"},{"location":"16_mendelian_randomization/#visualization","title":"Visualization","text":""},{"location":"16_mendelian_randomization/#scatter-plot","title":"Scatter plot","text":"<pre><code>res &lt;- mr(harmonized_data)\np1 &lt;- mr_scatter_plot(res, harmonized_data)\np1[[1]]\n</code></pre>"},{"location":"16_mendelian_randomization/#single-snp","title":"Single SNP","text":"<pre><code>res_single &lt;- mr_singlesnp(harmonized_data)\np2 &lt;- mr_forest_plot(res_single)\np2[[1]]\n</code></pre>"},{"location":"16_mendelian_randomization/#leave-one-out","title":"Leave-one-out","text":"<pre><code>res_loo &lt;- mr_leaveoneout(harmonized_data)\np3 &lt;- mr_leaveoneout_plot(res_loo)\np3[[1]]\n</code></pre>"},{"location":"16_mendelian_randomization/#funnel-plot","title":"Funnel plot","text":"<pre><code>res_single &lt;- mr_singlesnp(harmonized_data)\np4 &lt;- mr_funnel_plot(res_single)\np4[[1]]\n</code></pre>"},{"location":"16_mendelian_randomization/#mr-steiger-directionality-test","title":"MR Steiger directionality test","text":"<p>MR Steiger directionality test is a method to test the causal direction.</p> <pre><code>harmonized_data$\"r.outcome\" &lt;- get_r_from_lor(\n  harmonized_data$\"beta.outcome\",\n  harmonized_data$\"eaf.outcome\",\n  45383,\n  132032,\n  0.26,\n  model = \"logit\",\n  correction = FALSE\n)\n\nout &lt;- directionality_test(harmonized_data)\nout\n\nid.exposure id.outcome  exposure    outcome snp_r2.exposure snp_r2.outcome  correct_causal_direction    steiger_pval\n&lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;lgl&gt;   &lt;dbl&gt;\nrvi6Om  ETcv15  BMI T2D 0.02125453  0.005496427 TRUE    NA\n</code></pre> <p>Reference: Hemani, G., Tilling, K., &amp; Davey Smith, G. (2017). Orienting the causal relationship between imprecisely measured traits using GWAS summary data. PLoS genetics, 13(11), e1007081.</p>"},{"location":"16_mendelian_randomization/#mr-base-web-app","title":"MR-Base (web app)","text":"<p>MR-Base web app</p>"},{"location":"16_mendelian_randomization/#reference","title":"Reference","text":"<ul> <li>Sanderson, E., Glymour, M. M., Holmes, M. V., Kang, H., Morrison, J., Munaf\u00f2, M. R., ... &amp; Davey Smith, G. (2022). Mendelian randomization. Nature Reviews Methods Primers, 2(1), 1-21.</li> <li> </li> </ul>"},{"location":"16_mendelian_randomization/#hemani-g-zheng-j-elsworth-b-wade-k-h-haberland-v-baird-d-haycock-p-c-2018-the-mr-base-platform-supports-systematic-causal-inference-across-the-human-phenome-elife-7-e34408","title":"Hemani, G., Zheng, J., Elsworth, B., Wade, K. H., Haberland, V., Baird, D., ... &amp; Haycock, P. C. (2018). The MR-Base platform supports systematic causal inference across the human phenome. elife, 7, e34408.","text":""},{"location":"17_colocalization/","title":"Colocalization","text":""},{"location":"17_colocalization/#co-localization","title":"Co-localization","text":""},{"location":"17_colocalization/#coloc-assuming-a-single-causal-variant","title":"Coloc assuming a single causal variant","text":"<p><code>Coloc</code> uses the assumption of 0 or 1 causal variant in each trait,  and tests for whether they share the same causal variant.</p> <p>Note</p> <p>Actually such a assumption is different from fine-mapping. In fine-mapping, the aim is to find the putative causal variants, which is determined at birth. In colocalization, the aim is to find the \"signal overlapping\" to support the causality inference, like eQTL --&gt; A trait. It is possible that the causal variants are different in two traits.</p> <p>Datasets used:</p> <ul> <li>For binary traits, <code>coloc</code> requires \"beta\", \"varbeta\", and \"snp\". For quantitative traits,  the trait standard deviation \"sdY\" is required to estimate the scale of estimated beta.</li> <li>LD matrix will be a square numeric matrix of dimension equal to the number of SNPs, with dimnames corresponding to the SNP ids.</li> </ul> <p>Result interpretation:</p> <p>Basically, five configurations are calculated, </p> <pre><code>## PP.H0.abf PP.H1.abf PP.H2.abf PP.H3.abf PP.H4.abf \n##  1.73e-08  7.16e-07  2.61e-05  8.20e-05  1.00e+00 \n## [1] \"PP abf for shared variant: 100%\"\n</code></pre> <p>\\(H_0\\): neither trait has a genetic association in the region</p> <p>\\(H_1\\): only trait 1 has a genetic association in the region</p> <p>\\(H_2\\): only trait 2 has a genetic association in the region</p> <p>\\(H_3\\): both traits are associated, but with different causal variants</p> <p>\\(H_4\\): both traits are associated and share a single causal variant</p> <p><code>PP.H4.abf</code> is the posterior probability that two traits share a same causal variant.</p> <p>Then based on <code>H4</code> is true, a 95% credible set could be constructed (as a shared causal variant does not necessarily mean a specific variant). <pre><code>o &lt;- order(my.res$results$SNP.PP.H4,decreasing=TRUE)\ncs &lt;- cumsum(my.res$results$SNP.PP.H4[o])\nw &lt;- which(cs &gt; 0.95)[1]\nmy.res$results[o,][1:w,]$snp\n</code></pre></p> <p>References:</p> <p>Coloc: a package for colocalisation analyses</p>"},{"location":"17_colocalization/#coloc-assuming-multiple-causal-variants-or-multiple-signals","title":"Coloc assuming multiple causal variants or multiple signals","text":"<p>When the single-causal variant assumption is violeted, several ways could be used to relieve it.</p> <ol> <li> <p>Assuming multiple causal variants in SuSiE-Coloc pipeline.    In this pipeline, putative causal variants are fine-mapped, then each signal is passed to the coloc engine.</p> </li> <li> <p>Conditioning analysis using GCTA-COJO-Coloc pipeline.    In this pipeline, signals are segregated, then passed to the coloc engine.</p> </li> </ol>"},{"location":"17_colocalization/#other-pipelines","title":"Other pipelines","text":"<p>Many other strategies and pipelines are available for colocalization and prioritize the variants/genes/traits. For example: * HyPrColoc * OpenTargets * </p>"},{"location":"18_Conditioning_analysis/","title":"Conditioning analysis","text":"<p>Multiple association signals could exist in one locus, especially when observing complex LD structures in the regional plot. Conditioning on one signal allows the separation of independent signals.</p> <p>Several ways to perform the conditioning analysis:</p> <ul> <li>Adding the lead variant to the covariates step-wisely and rerun the association test.</li> <li>Conditional &amp; joint association analysis using GWAS summary statistics (GCTA-COJO).</li> </ul>"},{"location":"18_Conditioning_analysis/#adding-the-lead-variant-to-the-covariates","title":"Adding the lead variant to the covariates","text":"<p>First, extract the individual genotype (dosage) to the text file. Then add it to covariates.</p> <pre><code>plink2 \\\n--pfile chr1.dose.Rsq0.3 vzs \\\n--extract chr1.list \\\n--threads 1 \\\n--export A \\\n--out genotype/chr1\n</code></pre> <p>The exported format could be found in Export non-PLINK 2 fileset.</p> <p>Note</p> <p>Major allele dosage would be outputted. If adding <code>ref-first</code>, REF allele would be outputted. It does not matter as a covariate.</p> <p>Then just paste it to the covariates table and run the association test.</p> <p>Note</p> <p>Some association test software will also provide options for condition analysis. For example, in PLINK, you can use <code>--condition &lt;variant ID&gt;</code> for condition analysis. You can simply provide a list of variant IDs to run the condition analysis.</p>"},{"location":"18_Conditioning_analysis/#gcta-cojo","title":"GCTA-COJO","text":"<p>If raw genotypes and phenotypes are not available, GCTA-COJO performs conditioning analysis using sumstats and external LD reference.</p> <p><code>cojo-top-SNPs 10</code> will perform a step-wise model selection to select 10 independently associated SNPs (including non-significant ones).</p> <pre><code>gcta \\\n  --bfile chr1 \\\n  --chr 1 \\\n  --maf 0.001 \\\n  --cojo-file chr1_cojo.input \\\n  --cojo-top-SNPs 10 \\\n  --extract-region-bp 1 152383617 5000 \\\n  --out chr1_cojo.output\n</code></pre> <p>Note</p> <p><code>bfile</code> is used to generate LD. A size of &gt; 4000 unrelated samples is suggested. Estimation of LD in GATC is based on the hard-call genotype.</p> <p>Input file format <code>less chr1_cojo.input</code>: <pre><code>ID      ALLELE1 ALLELE0 A1FREQ  BETA    SE      P       N\nchr1:11171:CCTTG:C      C       CCTTG   0.0831407       -0.0459889      0.0710074       0.5172  180590\nchr1:13024:G:A  A       G       1.63957e-05     -3.2714 3.26302 0.3161  180590\n</code></pre> Here <code>A1</code> is the effect allele. </p> <p>Then <code>--cojo-cond</code> could be used to generate new sumstats conditioned on the above-selected variant(s).</p> <p>Reference:</p> <ul> <li>https://www.cog-genomics.org/plink/1.9/assoc</li> <li>Yang, J., Ferreira, T., Morris, A. P., Medland, S. E., Genetic Investigation of ANthropometric Traits (GIANT) Consortium, DIAbetes Genetics Replication And Meta-analysis (DIAGRAM) Consortium, ... &amp; Visscher, P. M. (2012). Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits. Nature genetics, 44(4), 369-375.</li> </ul>"},{"location":"19_ld/","title":"Linkage disequilibrium(LD)","text":""},{"location":"19_ld/#ld-definition","title":"LD Definition","text":"<p>In meiosis, homologous chromosomes are recombined. Recombination rates at different DNA regions are not equal.  The fragments can be detected after tens of generations, causing Linkage disequilibrium, which refers to the non-random association of alleles of different loci.</p> <p>Factors affecting LD</p> <ul> <li>Recombination rates</li> <li>Mutation</li> <li>Natural selection</li> <li>Genetic drift</li> <li>Population subdivision and bottlenecks</li> <li>Inbreeding </li> </ul>"},{"location":"19_ld/#ld-estimation","title":"LD Estimation","text":"<p>Suppose we have two SNPs whose alleles are \\(A/a\\) and \\(B/b\\). </p> <p>The haplotype frequencies are: </p> Haplotype Frequency AB \\(p_{AB}\\) Ab \\(p_{Ab}\\) aB \\(p_{aB}\\) ab \\(p_{ab}\\) <p>The allele frequencies are: </p> Allele Frequency A \\(p_A=p_{AB}+p_{Ab}\\) a \\(p_A=p_{aB}+p_{ab}\\) B \\(p_A=p_{AB}+p_{aB}\\) b \\(p_A=p_{Ab}+p_{ab}\\) <p>D : the level of LD between A and B can be estimated using coefficient of linkage disequilibrium (D), which is defined as:</p> \\[D_{AB} = p_{AB} - p_Ap_B\\] <p>If A and B are in linkage equilibrium, we can get </p> \\[D_{AB} = p_{AB} - p_Ap_B = 0\\] <p>which means the coefficient of linkage disequilibrium is 0 in this case.</p> <p>D can be calculated for each pair of alleles and their relationships can be expressed as:</p> \\[D_{AB} = -D_{Ab} = -D_{aB} = D_{ab} \\] <p>So we can simply denote \\(D = D_{AB}\\), and the relationship between haplotype frequencies and allele frequencies can be summarized in the following table.</p> Allele A a Total B \\(p_{AB}=p_Ap_B+D\\) \\(p_{aB}=p_ap_B-D\\) \\(p_B\\) b \\(p_{AB}=p_Ap_b-D\\) \\(p_{AB}=p_ap_b+D\\) \\(p_b\\) Total \\(p_A\\) \\(p_a\\) 1 <p>The range of possible values of D depends on the allele frequencies, which is not suitable for comparison between different pairs of alleles.</p> <p>Lewontin suggested a method for the normalization of D :</p> \\[D_{normalized} = {{D}\\over{D_{max}}}\\] <p>where </p> \\[ D_{max} = \\begin{cases}  max\\{-p_Ap_B, -(1-p_A)(1-p_B)\\} &amp; \\text{when } D \\lt 0 \\\\ min\\{ p_A(1-p_B), p_B(1-p_A)  \\} &amp; \\text{when } D \\gt 0 \\\\  \\end{cases} \\] <p>It measures how much proportion of the haplotypes had undergone recombination.</p> <p>In practice, the most commonly used alternative metric to \\(D_{normalized}\\) is \\(r^2\\), the correlation coefficient, which can be obtained by:</p> \\[ r^2 = {{D^2}\\over{p_A(1-p_A)p_B(1-p_B)}} \\] <p>Reference: Slatkin, M. (2008). Linkage disequilibrium\u2014understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics, 9(6), 477-485.</p>"},{"location":"19_ld/#ld-calculation-using-software","title":"LD Calculation using software","text":""},{"location":"19_ld/#ldstore2","title":"LDstore2","text":"<p>LDstore2: http://www.christianbenner.com/#</p> <p>Reference: Benner, C. et al. Prospects of fine-papping trait-associated genomic regions by using summary statistics from genome-wide association studies. Am. J. Hum. Genet. (2017).</p>"},{"location":"19_ld/#plink-ld","title":"PLINK LD","text":"<p>Please check Calculate LD using PLINK.</p>"},{"location":"19_ld/#ld-lookup-using-ldlink","title":"LD Lookup using LDlink","text":"<p>LDlink</p> <p>LDlink is a suite of web-based applications designed to easily and efficiently interrogate linkage disequilibrium in population groups. Each included application is specialized for querying and displaying unique aspects of linkage disequilibrium.</p> <p>https://ldlink.nci.nih.gov/?tab=home</p> <p>Reference: Machiela, M. J., &amp; Chanock, S. J. (2015). LDlink: a web-based application for exploring population-specific haplotype structure and linking correlated alleles of possible functional variants. Bioinformatics, 31(21), 3555-3557.</p> <p>LDlink is a very useful tool for quick lookups of any information related to LD. </p>"},{"location":"19_ld/#ldlink-ldpair","title":"LDlink-LDpair","text":"<p>LDpair</p> <p></p>"},{"location":"19_ld/#ldlink-ldproxy","title":"LDlink-LDproxy","text":"<p>LDproxy for rs671</p> <p></p>"},{"location":"19_ld/#query-in-batch-using-ldlink-api","title":"Query in batch using LDlink API","text":"<p>LDlink provides API for queries using command line. </p> <p>You need to register and get a token first.</p> <p>https://ldlink.nci.nih.gov/?tab=apiaccess</p> <p>Query LD proxies for variants using LDproxy API</p> <pre><code>curl -k -X GET 'https://ldlink.nci.nih.gov/LDlinkRest/ldproxy?var=rs3&amp;pop=MXL&amp;r2_d=r2&amp;window=500000&amp;    genome_build=grch37&amp;token=faketoken123'\n</code></pre>"},{"location":"19_ld/#ldlinkr","title":"LDlinkR","text":"<p>There is also a related R package for LDlink. </p> <p>Query LD proxies for variants using LDlinkR</p> <pre><code>install.packages(\"LDlinkR\")\n\nlibrary(LDlinkR)\n\nmy_proxies &lt;- LDproxy(snp = \"rs671\", \n                      pop = \"EAS\", \n                      r2d = \"r2\", \n                      token = \"YourTokenHere123\",\n                      genome_build = \"grch38\"\n                     )\n</code></pre> <p>Reference: Myers, T. A., Chanock, S. J., &amp; Machiela, M. J. (2020). LDlinkR: an R package for rapidly calculating linkage disequilibrium statistics in diverse populations. Frontiers in genetics, 11, 157.</p>"},{"location":"19_ld/#ld-pruning","title":"LD-pruning","text":"<p>Please check LD-pruning</p>"},{"location":"19_ld/#ld-clumping","title":"LD-clumping","text":"<p>Please check LD-clumping</p>"},{"location":"19_ld/#ld-score","title":"LD score","text":"<p>Definition: https://cloufield.github.io/GWASTutorial/08_LDSC/#ld-score</p>"},{"location":"19_ld/#ldsc","title":"LDSC","text":"<p>LD score can be estimated with LDSC using PLINK format genotype data as the reference panel. <pre><code>plinkPrefix=chr22\n\npython ldsc.py \\\n    --bfile ${plinkPrefix}\n    --l2 \\\n    --ld-wind-cm 1\\\n    --out ${plinkPrefix}\n</code></pre></p> <p>Check here for details.</p>"},{"location":"19_ld/#gcta","title":"GCTA","text":"<p>GCTA also provides a function to estimate LD scores using PLINK format genotype data.</p> <pre><code>plinkPrefix=chr22\n\ngcta64 \\\n    --bfile  ${plinkPrefix} \\\n    --ld-score \\\n    --ld-wind 1000 \\\n    --ld-rsq-cutoff 0.01 \\\n    --out  ${plinkPrefix}\n</code></pre> <p>Check here for details.</p>"},{"location":"19_ld/#ld-score-regression","title":"LD score regression","text":"<p>Please check LD score regression</p>"},{"location":"19_ld/#reference","title":"Reference","text":"<ul> <li>LD review :  Slatkin, M. (2008). Linkage disequilibrium\u2014understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics, 9(6), 477-485.</li> <li>LD normalization : Lewontin, R. C. (1964). The interaction of selection and linkage. I. General considerations; heterotic models. Genetics, 49(1), 49.</li> <li>plink: Purcell, S., Neale, B., Todd-Brown, K., Thomas, L., Ferreira, M. A., Bender, D., ... &amp; Sham, P. C. (2007). PLINK: a tool set for whole-genome association and population-based linkage analyses. The American journal of human genetics, 81(3), 559-575.</li> <li>gcta: Yang, J., Lee, S. H., Goddard, M. E., &amp; Visscher, P. M. (2011). GCTA: a tool for genome-wide complex trait analysis. The American Journal of Human Genetics, 88(1), 76-82.</li> <li>ldstore: Benner, C. et al. Prospects of fine-papping trait-associated genomic regions by using summary statistics from genome-wide association studies. Am. J. Hum. Genet. (2017).</li> <li>ldlink: Machiela, M. J., &amp; Chanock, S. J. (2015). LDlink: a web-based application for exploring population-specific haplotype structure and linking correlated alleles of possible functional variants. Bioinformatics, 31(21), 3555-3557.</li> <li>ldlinkR: Myers, T. A., Chanock, S. J., &amp; Machiela, M. J. (2020). LDlinkR: an R package for rapidly calculating linkage disequilibrium statistics in diverse populations. Frontiers in genetics, 11, 157.</li> </ul>"},{"location":"20_power_analysis/","title":"Power analysis for GWAS","text":""},{"location":"20_power_analysis/#type-i-type-ii-errors-and-statistical-power","title":"Type I, type II errors and Statistical power","text":"<p>This table shows the relationship between the null hypothesis \\(H_0\\) and the results of a statistical test (whether or not to reject the null hypothesis \\(H_0\\) ).</p> H0 is True H0 is False Do Not Reject True negative : \\(1 -  \\alpha\\) Type II error (false negative) : \\(\\beta\\) Reject Type I error (false positive) : \\(\\alpha\\) True positive : \\(1 -  \\beta\\) <p>\\(\\alpha\\) : significance level</p> <p>By definition, the statistical power of a test refers to the probability that the test will correctly reject the null hypothesis, namely the True positive rate in the table above. </p> <p>\\(Power = Pr ( Reject\\ | H_0\\ is\\ False) = 1 -  \\beta\\) </p> <p>Power</p> <p></p> <p>Factors affecting power</p> <ul> <li>Total sample size</li> <li>Case and control ratio </li> <li>Effect size of the variant </li> <li>Risk allele frequency</li> <li>Significance threshold</li> </ul>"},{"location":"20_power_analysis/#non-centrality-parameter","title":"Non-centrality parameter","text":"<p>NCP describes the degree of difference between the alternative hypothesis \\(H_1\\) and the null hypothesis \\(H_0\\) values.</p> <p>Consider a simple linear regression model:</p> \\[y = \\mu +\\beta x + \\epsilon\\] <p>The variance of the error term:</p> \\[\\sigma^2 = Var(y) - Var(x)\\beta^2\\] <p>Usually, the phenotypic variance that a single SNP could explain is very limited, so we can approximate \\(\\sigma^2\\) by:</p> \\[ \\sigma^2  \\thickapprox Var(y)\\] <p>Under Hardy-Weinberg equilibrium, we can get: </p> \\[Var(x) = 2f(1-f)\\] <ul> <li>\\(f\\) : the allele frequency for this variant</li> </ul> <p>So the Non-centrality parameter(NCP) \\(\\lambda\\) for \\(\\chi^2\\) distribution with degree of freedom 1: </p> \\[ \\lambda = ({{\\beta}\\over{SE_{\\beta}}})^2\\]"},{"location":"20_power_analysis/#power-for-quantitative-traits","title":"Power for quantitative traits","text":"\\[ \\lambda = ({{\\beta}\\over{SE_{\\beta}}})^2 \\thickapprox N \\times {{Var(x)\\beta^2}\\over{\\sigma^2}} \\thickapprox N \\times {{2f(1-f) \\beta^2 }\\over {Var(y)}}  \\] <p>Significance threshold: \\(C = CDF_{\\chi^2}^{-1}(1 - \\alpha,df=1)\\)</p> <ul> <li>\\(CDF_{\\chi^2}^{-1}(x)\\) : is the inverse of the cumulative distribution function for \\(\\chi^2\\) distribution.</li> </ul> \\[ Power = Pr(\\lambda &gt; C ) = CDF_{\\chi^2}(C, ncp = \\lambda,df=1) \\] <ul> <li>\\(CDF_{\\chi^2}(x, ncp= \\lambda)\\) : is the cumulative distribution function for non-central \\(\\chi^2\\) distribution with non-centrality parameter \\(\\lambda\\).</li> </ul>"},{"location":"20_power_analysis/#power-for-large-scale-case-control-genome-wide-association-studies","title":"Power for large-scale case-control genome-wide association studies","text":"<p>Denote :</p> <ul> <li>\\(P_{case}\\) : Risk allele frequency in cases</li> <li>\\(N_{case}\\) : Number of cases. The total allele count for cases is then \\(2N_{case}\\).</li> <li>\\(P_{control}\\) : Risk allele frequency in controls</li> <li>\\(N_{control}\\) : Number of control. The total allele count for control is then \\(2N_{control}\\).</li> </ul> <p>Null hypothesis : \\(P_{case} = P_{control}\\)</p> <p>To test whether one proportion \\(P_{case}\\) equals the other proportion \\(P_{control}\\), the test statistic is:</p> \\[z = {{P_{case} - P_{control}}\\over {\\sqrt{ {{P_{case}(1 - P_{case})}\\over{2N_{case}}} + {{P_{control}(1 - P_{control})}\\over{2N_{control}}} }}}\\] <p>Significance threshold: \\(C = \\Phi^{-1}(1 - \\alpha / 2 )\\)</p> \\[ Power = Pr(|Z|&gt;C) = 1 - \\Phi(-C-z) + \\Phi(C-z)\\] <p>GAS power calculator</p> <p>GAS power calculator implemented this method, and you can easily calculate the power using their website</p> <p></p>"},{"location":"20_power_analysis/#reference","title":"Reference:","text":"<ul> <li>Skol, A. D., Scott, L. J., Abecasis, G. R., &amp; Boehnke, M. (2006). Joint analysis is more efficient than replication-based analysis for two-stage genome-wide association studies. Nature genetics, 38(2), 209-213.</li> <li>Johnson, J. L., &amp; Abecasis, G. R. (2017). GAS Power Calculator: web-based power calculator for genetic association studies. BioRxiv, 164343.</li> <li>Sham, P. C., &amp; Purcell, S. M. (2014). Statistical power and significance testing in large-scale genetic studies. Nature Reviews Genetics, 15(5), 335-346.</li> </ul>"},{"location":"32_whole_genome_regression/","title":"Whole-genome regression : REGENIE","text":""},{"location":"32_whole_genome_regression/#concepts","title":"Concepts","text":""},{"location":"32_whole_genome_regression/#overview","title":"Overview","text":"<p>Overview of REGENIE</p> <p>Reference: https://rgcgithub.github.io/regenie/overview/</p>"},{"location":"32_whole_genome_regression/#whole-genome-model","title":"Whole genome model","text":""},{"location":"32_whole_genome_regression/#stacked-regressions","title":"Stacked regressions","text":""},{"location":"32_whole_genome_regression/#firth-correction","title":"Firth correction","text":""},{"location":"32_whole_genome_regression/#tutorial","title":"Tutorial","text":""},{"location":"32_whole_genome_regression/#installation","title":"Installation","text":"<p>Please check here</p>"},{"location":"32_whole_genome_regression/#step1","title":"Step1","text":"<p>Sample codes for running step 1</p> <pre><code>plinkFile=../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\nphenoFile=../01_Dataset/1kgeas_binary_regenie.txt\ncovarFile=../05_PCA/plink_results_projected.sscore\ncovarList=\"PC1_AVG,PC2_AVG,PC3_AVG,PC4_AVG,PC5_AVG,PC6_AVG,PC7_AVG,PC8_AVG,PC9_AVG,PC10_AVG\"\nextract=../05_PCA/plink_results.prune.in\n\n# revise the header of covariate file\nsed -i 's/#FID/FID/' ../05_PCA/plink_results_projected.sscore\nmkdir tmpdir\n\nregenie \\\n  --step 1 \\\n  --bed ${plinkFile} \\\n  --extract ${extract} \\\n  --phenoFile ${phenoFile} \\\n  --covarFile ${covarFile} \\\n  --covarColList ${covarList} \\\n  --bt \\\n  --bsize 1000 \\\n  --lowmem \\\n  --lowmem-prefix tmpdir/regenie_tmp_preds \\\n  --out 1kg_eas_step1_BT\n</code></pre>"},{"location":"32_whole_genome_regression/#step2","title":"Step2","text":"<p>Sample codes for running step 2</p> <pre><code>plinkFile=../01_Dataset/1KG.EAS.auto.snp.norm.nodup.split.maf005.thinp020\nphenoFile=../01_Dataset/1kgeas_binary_regenie.txt\ncovarFile=../05_PCA/plink_results_projected.sscore\ncovarList=\"PC1_AVG,PC2_AVG,PC3_AVG,PC4_AVG,PC5_AVG,PC6_AVG,PC7_AVG,PC8_AVG,PC9_AVG,PC10_AVG\"\nextract=../05_PCA/plink_results.prune.in\n\nsed -i 's/#FID/FID/' ../05_PCA/plink_results_projected.sscore\nmkdir tmpdir\n\nregenie \\\n  --step 2 \\\n  --bed ${plinkFile} \\\n  --ref-first \\\n  --phenoFile ${phenoFile} \\\n  --covarFile ${covarFile} \\\n  --covarColList ${covarList} \\\n  --bt \\\n  --bsize 400 \\\n  --firth --approx --pThresh 0.01 \\\n  --pred 1kg_eas_step1_BT_pred.list \\\n  --out 1kg_eas_step1_BT\n</code></pre>"},{"location":"32_whole_genome_regression/#visualization","title":"Visualization","text":""},{"location":"32_whole_genome_regression/#reference","title":"Reference","text":"<ul> <li>Mbatchou, J., Barnard, L., Backman, J., Marcketta, A., Kosmicki, J. A., Ziyatdinov, A., ... &amp; Marchini, J. (2021). Computationally efficient whole-genome regression for quantitative and binary traits. Nature genetics, 53(7), 1097-1103.</li> </ul>"},{"location":"60_awk/","title":"AWK","text":""},{"location":"60_awk/#awk-introduction","title":"AWK Introduction","text":"<p>'awk' is one of the most powerful text processing tools for tabular text files.</p>"},{"location":"60_awk/#awk-syntax","title":"AWK syntax","text":"<pre><code>awk OPTION 'CONDITION {PROCESS}' FILENAME\n</code></pre> <p>Some special variables in awk:</p> <ul> <li><code>$0</code> : all columns</li> <li><code>$n</code> : column n. For example, $1 means the first column. $4 means column 4.</li> <li><code>NR</code> : Row number.</li> </ul>"},{"location":"60_awk/#examples","title":"Examples","text":"<p>Using the sample sumstats, we will demonstrate some simple but useful one-liners.</p> <pre><code># sample sumstats\nhead ../02_Linux_basics/sumstats.txt #CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1:13273:G:C G   C   C   ADD 503 0.7461490.282904    -1.03509    0.300628    .\n1   14599   1:14599:T:A T   A   A   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14604   1:14604:A:G A   G   G   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14930   1:14930:A:G A   G   G   ADD 503 1.643590.242872 2.04585 0.0407708   .\n1   69897   1:69897:T:C T   C   T   ADD 503 1.691420.200238 2.62471 0.00867216  .\n1   86331   1:86331:A:G A   G   G   ADD 503 1.418870.238055 1.46968 0.141649    .\n1   91581   1:91581:G:A G   A   A   ADD 503 0.9313040.123644    -0.575598   0.564887    .\n1   122872  1:122872:T:G    T   G   G   ADD 503 1.048280.182036 0.259034    0.795609    .\n1   135163  1:135163:C:T    C   T   T   ADD 503 0.6766660.242611    -1.60989    0.107422    .\n</code></pre>"},{"location":"60_awk/#example-1","title":"Example 1","text":"<p>Select variants on chromosome 3 (keeping the headers)</p> <pre><code>awk 'NR==1 ||  $1==2 {print $0}' ../02_Linux_basics/sumstats.txt | head\n#CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n2   22398   2:22398:C:T C   T   T   ADD 503 1.287540.161017 1.56962 0.116503    .\n2   24839   2:24839:C:T C   T   T   ADD 503 1.318170.179754 1.53679 0.124344    .\n2   26844   2:26844:C:T C   T   T   ADD 503 1.3173  0.161302    1.70851 0.0875413   .\n2   28786   2:28786:T:C T   C   C   ADD 503 1.3043  0.161184    1.64822 0.0993082   .\n2   30091   2:30091:C:G C   G   G   ADD 503 1.3043  0.161184    1.64822 0.0993082   .\n2   30762   2:30762:A:G A   G   A   ADD 503 1.099560.158614 0.598369    0.549594    .\n2   34503   2:34503:G:T G   T   T   ADD 503 1.323720.179789 1.55988 0.118789    .\n2   39340   2:39340:A:G A   G   G   ADD 503 1.3043  0.161184    1.64822 0.0993082   .\n2   55237   2:55237:T:C T   C   C   ADD 503 1.314860.161988 1.68983 0.0910614   .\n</code></pre> <p>The <code>NR</code> here means row number. The condition here <code>NR==1 || $1==3</code> means if it is the first row or the first column is equal to 3, conduct the process <code>print $0</code>, which mean print all columns. </p>"},{"location":"60_awk/#example-2","title":"Example 2","text":"<p>Select all genome-wide significant variants (p&lt;5e-8)</p> <pre><code>awk 'NR==1 ||  $13 &lt;5e-8 {print $0}' ../02_Linux_basics/sumstats.txt | head\n#CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1:13273:G:C G   C   C   ADD 503 0.7461490.282904    -1.03509    0.300628    .\n1   14599   1:14599:T:A T   A   A   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14604   1:14604:A:G A   G   G   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14930   1:14930:A:G A   G   G   ADD 503 1.643590.242872 2.04585 0.0407708   .\n1   69897   1:69897:T:C T   C   T   ADD 503 1.691420.200238 2.62471 0.00867216  .\n1   86331   1:86331:A:G A   G   G   ADD 503 1.418870.238055 1.46968 0.141649    .\n1   91581   1:91581:G:A G   A   A   ADD 503 0.9313040.123644    -0.575598   0.564887    .\n1   122872  1:122872:T:G    T   G   G   ADD 503 1.048280.182036 0.259034    0.795609    .\n1   135163  1:135163:C:T    C   T   T   ADD 503 0.6766660.242611    -1.60989    0.107422    .\n</code></pre>"},{"location":"60_awk/#example-3","title":"Example 3","text":"<p>Create a bed-like format for annotation</p> <pre><code>awk 'NR&gt;1 {print $1,$2,$2,$4,$5}' ../02_Linux_basics/sumstats.txt | head\n1 13273 13273 G C\n1 14599 14599 T A\n1 14604 14604 A G\n1 14930 14930 A G\n1 69897 69897 T C\n1 86331 86331 A G\n1 91581 91581 G A\n1 122872 122872 T G\n1 135163 135163 C T\n1 233473 233473 C G\n</code></pre>"},{"location":"60_awk/#awk-workflow","title":"AWK workflow","text":"<p>The workflow of awk can be summarized in the following figure: </p> <p>awk workflow</p> <p></p>"},{"location":"60_awk/#awk-variables","title":"AWK variables","text":"<p>Frequently used awk variables</p> Variable Desciption NR The number of input records NF The number of input fields FS The input field separator. The default value is <code>\" \"</code> OFS The output field separator.  The default value is <code>\" \"</code> RS The input record separator. The default value is <code>\"\\n\"</code> ORS The output record separator.The default value is <code>\"\\n\"</code> FILENAME The name of the current input file. FNR The current record number in the current file <p>Handle csv and tsv files</p> <pre><code>head ../03_Data_formats/sample_data.csv\n#CHROM,POS,ID,REF,ALT,A1,FIRTH?,TEST,OBS_CT,OR,LOG(OR)_SE,Z_STAT,P,ERRCODE\n1,13273,1:13273:G:C,G,C,C,N,ADD,503,0.750168,0.280794,-1.02373,0.305961,.\n1,14599,1:14599:T:A,T,A,A,N,ADD,503,1.80972,0.231595,2.56124,0.0104299,.\n1,14604,1:14604:A:G,A,G,G,N,ADD,503,1.80972,0.231595,2.56124,0.0104299,.\n1,14930,1:14930:A:G,A,G,G,N,ADD,503,1.70139,0.240245,2.21209,0.0269602,.\n1,69897,1:69897:T:C,T,C,T,N,ADD,503,1.58002,0.194774,2.34855,0.0188466,.\n1,86331,1:86331:A:G,A,G,G,N,ADD,503,1.47006,0.236102,1.63193,0.102694,.\n1,91581,1:91581:G:A,G,A,A,N,ADD,503,0.924422,0.122991,-0.638963,0.522847,.\n1,122872,1:122872:T:G,T,G,G,N,ADD,503,1.07113,0.180776,0.380121,0.703856,.\n1,135163,1:135163:C:T,C,T,T,N,ADD,503,0.711822,0.23908,-1.42182,0.155079,.\n</code></pre> <pre><code>awk -v FS=',' -v OFS=\"\\t\" '{print $1,$2}' sample_data.csv\n#CHROM  POS\n1       13273\n1       14599\n1       14604\n1       14930\n1       69897\n1       86331\n1       91581\n1       122872\n1       135163\n</code></pre> <p>convert csv to tsv</p> <pre><code>awk 'BEGIN { FS=\",\"; OFS=\"\\t\" } {$1=$1; print}' sample_data.csv\n</code></pre> <p>Skip and replace headers</p> <pre><code>awk -v FS=',' -v OFS=\"\\t\" 'BEGIN{print \"CHR\\tPOS\"} NR&gt;1 {print $1,$2}' sample_data.csv\n\nCHR     POS\n1       13273\n1       14599\n1       14604\n1       14930\n1       69897\n1       86331\n1       91581\n1       122872\n1       135163\n</code></pre> <p>Extract a line</p> <pre><code>awk 'NR==4' sample_data.csv\n\n1,14604,1:14604:A:G,A,G,G,N,ADD,503,1.80972,0.231595,2.56124,0.0104299,.\n</code></pre> <p>Print the last two columns</p> <pre><code>awk -v FS=',' '{print $(NF-1),$(NF)}' sample_data.csv\nP ERRCODE\n0.305961 .\n0.0104299 .\n0.0104299 .\n0.0269602 .\n0.0188466 .\n0.102694 .\n0.522847 .\n0.703856 .\n0.155079 .\n</code></pre>"},{"location":"60_awk/#awk-operators","title":"AWK operators","text":"<p>Arithmetic Operators</p> Arithmetic Operators Desciption <code>+</code> add <code>-</code> subtract <code>*</code> multiply <code>\\</code> divide <code>%</code> modulus division <code>**</code> x**y : x raised to the y-th power <p>Logical Operators</p> Logical Operators Desciption <code>\\|\\|</code> or <code>&amp;&amp;</code> and <code>!</code> not"},{"location":"60_awk/#awk-functions","title":"AWK functions","text":"<p>Numeric functions in awk</p> <ul> <li>int(x) : truncate x to integer</li> <li>log(x) : the natural logarithm of x</li> <li>exp(x) : natural exponential function</li> <li>sqrt(x) : square root of x</li> </ul> <p>Convert OR and P to BETA and -log10(P)</p> <pre><code>awk -v FS=',' -v OFS=\"\\t\" 'BEGIN{print \"SNPID\\tBETA\\tMLOG10P\"}NR&gt;1{print $3,log($10),-log($13)/log(10)}' sample_data.csv\nSNPID   BETA    MLOG10P\n1:13273:G:C     -0.287458       0.514334\n1:14599:T:A     0.593172        1.98172\n1:14604:A:G     0.593172        1.98172\n1:14930:A:G     0.531446        1.56928\n1:69897:T:C     0.457438        1.72477\n1:86331:A:G     0.385303        0.988455\n1:91581:G:A     -0.0785866      0.281625\n1:122872:T:G    0.0687142       0.152516\n1:135163:C:T    -0.339927       0.809447\n</code></pre> <p>String manipulating functions in awk</p> <ul> <li>length([string])</li> <li>split(string, array [, fieldsep [, seps ] ])</li> <li>sub(regexp, replacement [, target]) </li> <li>gsub(regexp, replacement [, target])</li> <li>substr(string, start [, length ])</li> <li>tolower(string)</li> <li>toupper(string)</li> </ul>"},{"location":"60_awk/#awk-options","title":"AWK options","text":"<pre><code>$ awk --help\nUsage: awk [POSIX or GNU style options] -f progfile [--] file ...\nUsage: awk [POSIX or GNU style options] [--] 'program' file ...\nPOSIX options:          GNU long options: (standard)\n-f progfile             --file=progfile\n        -F fs                   --field-separator=fs\n        -v var=val              --assign=var=val\nShort options:          GNU long options: (extensions)\n-b                      --characters-as-bytes\n        -c                      --traditional\n        -C                      --copyright\n        -d[file]                --dump-variables[=file]\n-D[file]                --debug[=file]\n-e 'program-text'       --source='program-text'\n-E file                 --exec=file\n        -g                      --gen-pot\n        -h                      --help\n        -i includefile          --include=includefile\n        -l library              --load=library\n        -L[fatal|invalid]       --lint[=fatal|invalid]\n-M                      --bignum\n        -N                      --use-lc-numeric\n        -n                      --non-decimal-data\n        -o[file]                --pretty-print[=file]\n-O                      --optimize\n        -p[file]                --profile[=file]\n-P                      --posix\n        -r                      --re-interval\n        -S                      --sandbox\n        -t                      --lint-old\n        -V                      --version\n\nTo report bugs, see node `Bugs' in `gawk.info', which is\nsection `Reporting Problems and Bugs' in the printed version.\n\ngawk is a pattern scanning and processing language.\nBy default it reads standard input and writes standard output.\n\nExamples:\n        gawk '{ sum += $1 }; END { print sum }' file\n        gawk -F: '{ print $1 }' /etc/passwd\n</code></pre>"},{"location":"60_awk/#reference","title":"Reference","text":"<ul> <li>https://www.gnu.org/software/gawk/manual/gawk.html</li> </ul>"},{"location":"61_sed/","title":"sed","text":"<p><code>sed</code> is also one of the most commonly used test-editing command in Linux, which is short for stream editor. <code>sed</code> command edits the text from standard input in a line-by-line approach. </p>"},{"location":"61_sed/#sed-syntax","title":"sed syntax","text":"<pre><code>sed [OPTIONS] PROCESS [FILENAME]\n</code></pre>"},{"location":"61_sed/#examples","title":"Examples","text":""},{"location":"61_sed/#sample-input","title":"sample input","text":"<pre><code>head ../02_Linux_basics/sumstats.txt\n#CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1:13273:G:C G   C   C   ADD 503 0.7461490.282904    -1.03509    0.300628    .\n1   14599   1:14599:T:A T   A   A   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14604   1:14604:A:G A   G   G   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14930   1:14930:A:G A   G   G   ADD 503 1.643590.242872 2.04585 0.0407708   .\n1   69897   1:69897:T:C T   C   T   ADD 503 1.691420.200238 2.62471 0.00867216  .\n1   86331   1:86331:A:G A   G   G   ADD 503 1.418870.238055 1.46968 0.141649    .\n1   91581   1:91581:G:A G   A   A   ADD 503 0.9313040.123644    -0.575598   0.564887    .\n1   122872  1:122872:T:G    T   G   G   ADD 503 1.048280.182036 0.259034    0.795609    .\n1   135163  1:135163:C:T    C   T   T   ADD 503 0.6766660.242611    -1.60989    0.107422    .\n</code></pre>"},{"location":"61_sed/#example-1-replacing-strings","title":"Example 1: Replacing strings","text":"<p><code>s</code> for substitute <code>g</code> for global</p> <p>Replacing strings</p> <p>\"Replace the separator from <code>:</code> to <code>_</code>\" <pre><code>head 02_Linux_basics/sumstats.txt | sed 's/:/_/g'\n#CHROM  POS ID  REF ALT A1  TEST    OBS_CT  OR  LOG(OR)_SE  Z_STAT  P   ERRCODE\n1   13273   1_13273_G_C G   C   C   ADD 503 0.7461490.282904    -1.03509    0.300628    .\n1   14599   1_14599_T_A T   A   A   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14604   1_14604_A_G A   G   G   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14930   1_14930_A_G A   G   G   ADD 503 1.643590.242872 2.04585 0.0407708   .\n1   69897   1_69897_T_C T   C   T   ADD 503 1.691420.200238 2.62471 0.00867216  .\n1   86331   1_86331_A_G A   G   G   ADD 503 1.418870.238055 1.46968 0.141649    .\n1   91581   1_91581_G_A G   A   A   ADD 503 0.9313040.123644    -0.575598   0.564887    .\n1   122872  1_122872_T_G    T   G   G   ADD 503 1.048280.182036 0.259034    0.795609    .\n1   135163  1_135163_C_T    C   T   T   ADD 503 0.6766660.242611    -1.60989    0.107422    .\n</code></pre></p>"},{"location":"61_sed/#example-2-delete-headerthe-first-line","title":"Example 2: Delete header(the first line)","text":"<p><code>-d</code> for deletion</p> <p>Delete header(the first line)</p> <pre><code>head 02_Linux_basics/sumstats.txt | sed '1d'\n1   13273   1:13273:G:C G   C   C   ADD 503 0.7461490.282904    -1.03509    0.300628    .\n1   14599   1:14599:T:A T   A   A   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14604   1:14604:A:G A   G   G   ADD 503 1.676930.240899 2.14598 0.0318742   .\n1   14930   1:14930:A:G A   G   G   ADD 503 1.643590.242872 2.04585 0.0407708   .\n1   69897   1:69897:T:C T   C   T   ADD 503 1.691420.200238 2.62471 0.00867216  .\n1   86331   1:86331:A:G A   G   G   ADD 503 1.418870.238055 1.46968 0.141649    .\n1   91581   1:91581:G:A G   A   A   ADD 503 0.9313040.123644    -0.575598   0.564887    .\n1   122872  1:122872:T:G    T   G   G   ADD 503 1.048280.182036 0.259034    0.795609    .\n1   135163  1:135163:C:T    C   T   T   ADD 503 0.6766660.242611    -1.60989    0.107422    .\n</code></pre>"},{"location":"69_resources/","title":"Resources","text":""},{"location":"69_resources/#sandbox","title":"Sandbox","text":"<p>Sandbox provides tutorials for you to learn how to use bioinformatics tools right from your browser. Everything runs in a sandbox, so you can experiment all you want.</p> <ul> <li>URL : https://sandbox.bio/</li> </ul> <p></p>"},{"location":"69_resources/#explain-shell","title":"Explain Shell","text":"<p>explainshell is a tool (with a web interface) capable of parsing man pages, extracting options and explain a given command-line by matching each argument to the relevant help text in the man page.</p> <ul> <li>URL : https://explainshell.com/</li> </ul> <p></p>"},{"location":"71_python_resources/","title":"Python Resources","text":""},{"location":"71_python_resources/#python","title":"Python\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u5165\u9580","text":"<ul> <li>Description: \u6771\u4eac\u5927\u5b66\u306b\u304a\u3051\u308b\u300cPython\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u5165\u9580\u300d\u306e\u6559\u6750\u3092\u63d0\u4f9b\u3059\u308b\u516c\u958b\u30ec\u30dd\u30b8\u30c8\u30ea</li> <li>URL: https://utokyo-ipp.github.io/</li> </ul>"},{"location":"75_R_basics/","title":"R","text":""},{"location":"75_R_basics/#installing-r","title":"Installing R","text":""},{"location":"75_R_basics/#download-r-from-cran","title":"Download R from CRAN","text":"<p>R can be downloaded from its official website CRAN (The Comprehensive R Archive Network).</p> <p>CRAN</p> <p>https://cran.r-project.org/</p>"},{"location":"75_R_basics/#install-r-using-conda","title":"Install R using conda","text":"<p>It is convenient to use conda to manage your R environment. </p> <pre><code>conda install -c conda-forge r-base=4.x.x\n</code></pre>"},{"location":"75_R_basics/#ide-for-r-positrstudio","title":"IDE for R: Posit(Rstudio)","text":"<p>Posit(Rstudio) is one of the most commonly used Integrated development environment(IDE) for R.</p> <p>https://posit.co/</p>"},{"location":"75_R_basics/#use-r-in-interactive-mode","title":"Use R in interactive mode","text":"<pre><code>R\n</code></pre>"},{"location":"75_R_basics/#run-r-script","title":"Run R script","text":"<pre><code>Rscript mycode.R\n</code></pre>"},{"location":"75_R_basics/#installing-and-using-r-packages","title":"Installing and Using R packages","text":"<pre><code>install.packages(\"package_name\")\n\nlibrary(package_name)\n</code></pre>"},{"location":"75_R_basics/#basic-syntax","title":"Basic syntax","text":""},{"location":"75_R_basics/#assignment-and-evaluation","title":"Assignment and Evaluation","text":"<pre><code>&gt; x &lt;- 1\n\n&gt; x\n[1] 1\n\n&gt; print(x)\n[1] 1\n</code></pre>"},{"location":"75_R_basics/#data-types","title":"Data types","text":""},{"location":"75_R_basics/#atomic-data-types","title":"Atomic data types","text":"<p>logical, integer, real, complex, string (or character) </p> Atomic data types Description Examples logical boolean <code>TRUE</code>, <code>FALSE</code> integer integer <code>1</code>,<code>2</code> numeric float number <code>0.01</code> complex complex number <code>1+0i</code> string string or chracter <code>abc</code>"},{"location":"75_R_basics/#vectors","title":"Vectors","text":"<pre><code>myvector &lt;- c(1,2,3)\nmyvector &lt; 1:3\n\nmyvector &lt;- c(TRUE,FALSE)\nmyvector &lt;- c(0.01, 0.02)\nmyvector &lt;- c(1+0i, 2+3i)\nmyvector &lt;- c(\"a\",\"bc\")\n</code></pre>"},{"location":"75_R_basics/#matrices","title":"Matrices","text":"<pre><code>&gt; mymatrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\n&gt; mymatrix\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n&gt; ncol(mymatrix)\n[1] 3\n&gt; nrow(mymatrix)\n[1] 2\n&gt; dim(mymatrix)\n[1] 2 3\n&gt; length(mymatrix)\n[1] 6\n</code></pre>"},{"location":"75_R_basics/#list","title":"List","text":"<p><code>list()</code> is a special vector-like data type that can contain different data types.</p> <pre><code>&gt; mylist &lt;- list(1, 0.02, \"a\", FALSE, c(1,2,3), matrix(1:6,nrow=2,ncol=3))\n&gt; mylist\n[[1]]\n[1] 1\n\n[[2]]\n[1] 0.02\n\n[[3]]\n[1] \"a\"\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] 1 2 3\n\n[[6]]\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n</code></pre>"},{"location":"75_R_basics/#dataframe","title":"Dataframe","text":"<pre><code>&gt; df &lt;- data.frame(score = c(90,80,70,60),  rank = c(\"a\", \"b\", \"c\", \"d\"))\n&gt; df\n  score rank\n1    90    a\n2    80    b\n3    70    c\n4    60    d\n</code></pre>"},{"location":"75_R_basics/#subsetting","title":"Subsetting","text":"<pre><code>myvector\n[1] 1 2 3\n&gt; myvector[0]\ninteger(0)\n&gt; myvector[1]\n[1] 1\nmyvector[1:2]\n[1] 1 2\n&gt; myvector[-1]\n[1] 2 3\n&gt; myvector[-1:-2]\n[1] 3\n</code></pre> <pre><code>&gt; mymatrix\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n&gt; mymatrix[0]\ninteger(0)\n&gt; mymatrix[1]\n[1] 1\n&gt; mymatrix[1,]\n[1] 1 3 5\n&gt; mymatrix[1,2]\n[1] 3\n&gt; mymatrix[1:2,2]\n[1] 3 4\n&gt; mymatrix[,2]\n[1] 3 4\n</code></pre> <pre><code>&gt; df\n  score rank\n1    90    a\n2    80    b\n3    70    c\n4    60    d\n&gt; df[score]\nError in `[.data.frame`(df, score) : object 'score' not found\n&gt; df[[score]]\nError in (function(x, i, exact) if (is.matrix(i)) as.matrix(x)[[i]] else .subset2(x,  :\n  object 'score' not found\n&gt; df[[\"score\"]]\n[1] 90 80 70 60\n&gt; df[\"score\"]\n  score\n1    90\n2    80\n3    70\n4    60\n&gt; df[1, \"score\"]\n[1] 90\n&gt; df[1:2, \"score\"]\n[1] 90 80\n&gt; df[1:2,2]\n[1] \"a\" \"b\"\n&gt; df[1:2,1]\n[1] 90 80\n&gt; df[,c(\"rank\",\"score\")]\n  rank score\n1    a    90\n2    b    80\n3    c    70\n4    d    60\n</code></pre>"},{"location":"75_R_basics/#data-input-and-output","title":"Data Input and Output","text":"<pre><code>mydata &lt;- read.table(\"data.txt\", header=T)\n\nwrite.table(mydata, \"data.txt\")\n</code></pre>"},{"location":"75_R_basics/#control-flow","title":"Control flow","text":""},{"location":"75_R_basics/#if","title":"if","text":"<pre><code>if (x &gt; y){\n  print (\"x\")\n} else if (x &lt; y){\n  print (\"y\")\n} else {\n  print(\"tie\")\n}\n</code></pre>"},{"location":"75_R_basics/#for","title":"for","text":"<pre><code>&gt; for (x in 1:5) {\n    print(x)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n</code></pre>"},{"location":"75_R_basics/#while","title":"while","text":"<pre><code>x&lt;-0\nwhile (x&lt;5)\n{\n    x&lt;-x+1\n    print(\"Hello world\")\n}\n\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n</code></pre>"},{"location":"75_R_basics/#functions","title":"Functions","text":"<pre><code>myfunction &lt;- function(x){\n  // actual code here\n  return(result)\n}\n\n&gt; my_add_function &lt;- function(x,y){\n  c = x + y\n  return(c)\n}\n&gt; my_add_function(1,3)\n[1] 4\n</code></pre>"},{"location":"75_R_basics/#statistical-functions","title":"Statistical functions","text":""},{"location":"75_R_basics/#normal-distribution","title":"Normal distribution","text":"Function Description dnorm(x, mean = 0, sd = 1, log = FALSE) probability density function pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) cumulative density function qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) quantile function rnorm(n, mean = 0, sd = 1) generate random values from normal distribution <pre><code>&gt; dnorm(1.96)\n[1] 0.05844094\n\n&gt; pnorm(1.96)\n[1] 0.9750021\n\n&gt; pnorm(1.96, lower.tail=FALSE)\n[1] 0.0249979\n\n&gt; qnorm(0.975)\n[1] 1.959964\n\n&gt; rnorm(10)\n [1] -0.05595019  0.83176199  0.58362601 -0.89434812  0.85722843  0.96199308\n [7]  0.47782706 -0.46322066  0.03525421 -1.00715141\n</code></pre>"},{"location":"75_R_basics/#chi-square-distribution","title":"Chi-square distribution","text":"Function Description dchisq(x, df, ncp = 0, log = FALSE) probability density function pchisq(q, df, ncp = 0, lower.tail = TRUE, log.p = FALSE) cumulative density function qchisq(p, df, ncp = 0, lower.tail = TRUE, log.p = FALSE) quantile function rchisq(n, df, ncp = 0) generate random values from normal distribution"},{"location":"75_R_basics/#regression","title":"Regression","text":"<pre><code>lm(formula, data, subset, weights, na.action,\n   method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n   singular.ok = TRUE, contrasts = NULL, offset, \u2026)\n\n# linear regression\nresults &lt;- lm(formula = y ~ x1 + x2)\n\n# logistic regression\nresults &lt;- lm(formula = y ~ x1 + x2, family = \"binomial\")\n</code></pre> <p>Reference: - https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html</p>"},{"location":"76_R_resources/","title":"R Resources","text":"<ul> <li>R Programming for Data Science : https://bookdown.org/rdpeng/rprogdatascience/</li> </ul>"},{"location":"80_anaconda/","title":"Anaconda","text":"<p>Conda is an open-source package and environment management system. </p> <p>It is a very handy tool when you need to manage python packages.</p>"},{"location":"80_anaconda/#download","title":"Download","text":"<p>https://www.anaconda.com/products/distribution</p> <p>For example, download the latest linux version:</p> <pre><code>wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh\n</code></pre> <p></p>"},{"location":"80_anaconda/#install","title":"Install","text":"<pre><code># give it permission to execute\nchmod +x Anaconda3-2021.11-Linux-x86_64.sh \n\n# install\nbash ./Anaconda3-2021.11-Linux-x86_64.sh\n</code></pre> <p>Follow the instructions on : https://docs.anaconda.com/anaconda/install/linux/</p> <p>If everything goes well, then you can see the <code>(base)</code> before the prompt, which indicate the base environment: <pre><code>(base) [heyunye@gc019 ~]$\n</code></pre></p> <p>For how to use conda, please check : https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html</p> <p>Examples: <pre><code># install a specific version of python package\nconda install pandas==1.5.2\n\n#create a new python 3.9 virtual environment with the name \"mypython39\"\nconda create -n mypython39 python=3.9\n\n#use environment.yml to create a virtual environment\nconda env create --file environment.yml\n\n# activate a virtual environment called ldsc\nconda activate ldsc\n\n# change back to base environment\nconda deactivate\n\n# list all packages in your current environment \nconda list\n\n# list all your current environments \nconda env list\n</code></pre></p>"},{"location":"81_jupyter_notebook/","title":"Jupyter notebook","text":"<p>Usyally, the conda will install the jupyter notebook (and the ipykernel) by default.</p> <p>If not, using conda to install it: <pre><code>conda install jupyter\n</code></pre></p>"},{"location":"81_jupyter_notebook/#using-jupyter-notebook-on-a-local-or-remote-server","title":"Using Jupyter notebook on a local or remote server","text":""},{"location":"81_jupyter_notebook/#using-the-default-configuration","title":"Using the default configuration","text":""},{"location":"81_jupyter_notebook/#local-machine","title":"Local machine","text":"<p>You could open it in the Anaconda interface or some other IDE.</p> <p>If using the terminal, just typing: <pre><code>jupyter-lab --port 9000 &amp;          \n</code></pre></p> <p>Then open the link in the browser. <pre><code>http://localhost:9000/lab?token=???\nhttp://127.0.0.1:9000/lab?token=???\n</code></pre></p>"},{"location":"81_jupyter_notebook/#remote-server","title":"Remote server","text":"<p>Start in the command line of the remote server, adding a port. <pre><code>jupyter-lab --ip 0.0.0.0 --port 9000 --no-browser &amp;\n</code></pre> It will generate an address the same as above.</p> <p>Then, on the local machine, using ssh to listen to the port. <pre><code>ssh -NfL localhost:9000:localhost:9000 user@host\n</code></pre> Note that the <code>localhost:9000:localhost:9000</code> is <code>localmachine:localport:remotemachine:remotehost</code> and <code>user@host</code> is the user id and address of the remote server.</p> <p>When this is finished, open the above in the browser.</p>"},{"location":"81_jupyter_notebook/#using-customized-configuration","title":"Using customized configuration","text":"<p>Steps:</p> <ul> <li>Create the configuration file</li> <li>Add the port information</li> <li>Run jupyter notebook server on remote host</li> <li>Use ssh tunnel to connect to the remote server from your local machine</li> <li>Use jupyter notebook in your browser</li> </ul>"},{"location":"81_jupyter_notebook/#create-the-configuration-file","title":"Create the configuration file","text":"<p>Create a jupyter notebook configuration file if there is no such file <pre><code>jupyter notebook --generate-config\n</code></pre></p> <p>The file is usually stored at: <pre><code>~/.jupyter/jupyter_notebook_config.py\n</code></pre></p> <p>What the first few lines of Configuration file look like:</p> <pre><code>head ~/.jupyter/jupyter_notebook_config.py\n# Configuration file for jupyter-notebook.\n\n#------------------------------------------------------------------------------\n# Application(SingletonConfigurable) configuration\n#------------------------------------------------------------------------------\n\n## This is an application.\n</code></pre>"},{"location":"81_jupyter_notebook/#add-the-port-information","title":"Add the port information","text":"<p>Simply add <code>c.NotebookApp.port =8889</code> to the configuration file and then save. Note: you can change the port you want to use. <pre><code># Configuration file for jupyter-notebook.\n\nc.NotebookApp.port = 8889\n\n#------------------------------------------------------------------------------\n# Application(SingletonConfigurable) configuration\n#------------------------------------------------------------------------------\n\n## This is an application.\n</code></pre></p>"},{"location":"81_jupyter_notebook/#run-jupyter-notebook-server-on-remote-host","title":"Run jupyter notebook server on remote host","text":"<p>On host side, set up the jupyter notebook server: <pre><code>jupyter notebook\n</code></pre></p>"},{"location":"81_jupyter_notebook/#use-ssh-tunnel-to-connect-to-the-remote-server-from-your-local-machine","title":"Use ssh tunnel to connect to the remote server from your local machine","text":"<p>On your local machine, use ssh tunnel to connect to the jupyter notebook server: </p> <pre><code>ssh -N -f -L localhost:8889:localhost:8889 username@your_remote_host_name\n</code></pre>"},{"location":"81_jupyter_notebook/#use-jupyter-notebook-in-your-browser","title":"Use jupyter notebook in your browser","text":"<p>Then you can access juptyer notebook on your local browser using the link generated by jupyter notebook server. <code>http://127.0.0.1:8889/?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</code></p>"},{"location":"82_windows_linux_subsystem/","title":"Window Linux Subsystem","text":"<p>In this section, we will briefly demostrate how to install a linux subsystem on windows.</p>"},{"location":"82_windows_linux_subsystem/#official-documents","title":"Official Documents","text":"<ul> <li>English version: https://docs.microsoft.com/en-us/windows/wsl/install</li> <li>Japanese version: https://docs.microsoft.com/ja-jp/windows/wsl/install</li> <li>Chinese version: https://docs.microsoft.com/zh-cn/windows/wsl/install</li> </ul>"},{"location":"82_windows_linux_subsystem/#prerequisites","title":"Prerequisites","text":"<p>\"You must be running Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11.\"</p>"},{"location":"82_windows_linux_subsystem/#steps","title":"Steps","text":"<ul> <li>Step 1 : Open your terminal as administrator (right click the icon and you will see the option)</li> <li>Step 2 : 'wsl.exe --install'</li> </ul> <ul> <li> <p>Step 3 : Reboot</p> </li> <li> <p>Step 4 :  Run the subsystem </p> </li> </ul> <p></p> <ul> <li>Step 5 : Setup username/password</li> </ul> <p></p> <ul> <li>Step 6 : Ready to go!</li> </ul> <p></p>"},{"location":"83_git_and_github/","title":"Git and Github","text":""},{"location":"83_git_and_github/#git","title":"Git","text":"<p>Git is very powerful version control software. Git can track the changes in all the files of your projects and allow collarboration of multiple contributors.  </p> <p>For details, please check: https://git-scm.com/</p>"},{"location":"83_git_and_github/#github","title":"Github","text":"<p>Github is an online platform, offering a cloud-based Git repository.</p> <p>https://github.com/</p>"},{"location":"83_git_and_github/#create-a-new-id","title":"Create a new id","text":"<p>Github signup page:</p> <p>https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F&amp;source=header-home</p>"},{"location":"83_git_and_github/#clone-a-repository","title":"Clone a repository","text":"<p>Syntax: <code>git colne &lt;the url you just copied&gt;</code></p> <p>Example: <code>git clone https://github.com/Cloufield/GWASTutorial.git</code></p>"},{"location":"83_git_and_github/#update-the-current-repository","title":"Update the current repository","text":"<p><code>git pull</code></p>"},{"location":"83_git_and_github/#git-setup","title":"git setup","text":"<pre><code>$ git config --global user.name \"myusername\"\n$ git config --global user.email myusername@myemail.com\n</code></pre>"},{"location":"83_git_and_github/#create-access-tokens","title":"Create access tokens","text":"<p>Please see github official documents on how to create a personal token:</p> <p>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token</p> <p>Useful Resources</p> <ul> <li>Github training: https://training.github.com/</li> <li>Learn git branching : https://learngitbranching.js.org/</li> </ul>"},{"location":"84_ssh/","title":"SSH","text":"<p>SSH stands for Secure Shell Protocol, which enables you to connect to remote server safely.</p> <p></p>"},{"location":"84_ssh/#login-to-remote-server","title":"Login to remote server","text":"<pre><code>ssh &lt;username&gt;@&lt;host&gt;\n</code></pre> <p>Before you login in, you need to generate keys for ssh connection:</p>"},{"location":"84_ssh/#keys","title":"Keys","text":"<p><pre><code>ssh-keygen -t rsa -b 4096\n</code></pre> You will get two keys, a public one and a private one.</p> <ul> <li>public key  :  <code>~/.ssh/id_rsa.pub</code></li> <li>private key :  <code>~/.ssh/id_rsa</code></li> </ul> <p>Warning</p> <p>Don't share your private key with others.    </p> <p>What you need to do is just add you local public key to <code>~/.ssh/authorized_keys</code> on host server.</p>"},{"location":"84_ssh/#file-transfer","title":"File transfer","text":"<p>Suppose you are using a local machine: </p> <p>Donwload files from remote host to local machine </p> <pre><code>scp &lt;username&gt;@&lt;host&gt;:remote_path local_path\n</code></pre> <p>Upload files from local machine to remote host</p> <pre><code>scp local_path &lt;username&gt;@&lt;host&gt;:remote_path\n</code></pre> <p>Info</p> <p><code>-r</code> : copy recursively. This option is needed when you want to transfer an entire directory. </p> <p>Example</p> <p>Copy the local work directory to remote home directory <pre><code>$ scp -r /home/gwaslab/work gwaslab@remote.com:/home/gwaslab \n</code></pre></p>"},{"location":"84_ssh/#ssh-tunneling","title":"SSH Tunneling","text":"<p>Quote</p> <p>In this forwarding type, the SSH client listens on a given port and tunnels any connection to that port to the specified port on the remote SSH server, which then connects to a port on the destination machine. The destination machine can be the remote SSH server or any other machine. https://linuxize.com/post/how-to-setup-ssh-tunneling/</p> <p><code>-L</code> : Local port forwarding</p> <pre><code>ssh -L [local_IP:]local_PORT:destination:destination_PORT &lt;username&gt;@&lt;host&gt;\n</code></pre>"},{"location":"84_ssh/#other-ssh-options","title":"other SSH options","text":"<ul> <li><code>-f</code> : send to background.</li> <li><code>-p</code>:  port for connenction (default:22).</li> <li><code>-N</code> : not to execute any commands on the remote host. (so you will not open a remote shell but just forward ports.)</li> </ul>"},{"location":"85_job_scheduler/","title":"Job scheduling system","text":"<p>(If needed) Try to use job scheduling system to run a simple script:</p> <p>Two of the most commonly used job scheduling systems:</p> <ul> <li>Slurm Workload Manager : sbatch, scancel, sq, sjobs, shosts </li> <li>Univa Grid Engine : qsub, qdel, qstat </li> </ul>"},{"location":"90_Recommended_Reading/","title":"Recommended reading","text":""},{"location":"90_Recommended_Reading/#textbooks","title":"Textbooks","text":"Year Category Reference 2020 Statistical Genetics An Introduction to Statistical Genetic Data Analysis By Melinda C. Mills, Nicola Barban and Felix C. Tropf https://mitpress.mit.edu/books/introduction-statistical-genetic-data-analysis 2019 Statistical Genetics Handbook of Statistical Genomics: Fourth Edition https://onlinelibrary.wiley.com/doi/book/10.1002/9781119487845 2009 Statistical Analysis and Machine Learning The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics)introduction-statistical-genetic-data-analysis. Trevor Hastie, Robert Tibshirani, Jerome Friedman. https://hastie.su.domains/ElemStatLearn/ (PDF book is available)"},{"location":"90_Recommended_Reading/#overview-reviews","title":"Overview Reviews","text":"Year Reference Link 2021 Uffelmann, E., Huang, Q. Q., Munung, N. S., De Vries, J., Okada, Y., Martin, A. R., \u2026 &amp; Posthuma, D. (2021). Genome-wide association studies. Nature Reviews Methods Primers, 1(1), 1-21. Pubmed 2019 Tam, V., Patel, N., Turcotte, M., Boss\u00e9, Y., Par\u00e9, G., &amp; Meyre, D. (2019). Benefits and limitations of genome-wide association studies. Nature Reviews Genetics, 20(8), 467-484. Pubmed 2017 Pasaniuc, B., &amp; Price, A. L. (2017). Dissecting the genetics of complex traits using summary association statistics. Nature reviews genetics, 18(2), 117-127. Pubmed 2023 Abdellaoui, A., Yengo, L., Verweij, K. J., &amp; Visscher, P. M. (2023). 15 years of GWAS discovery: Realizing the promise. The American Journal of Human Genetics. Pubmed 2017 Visscher, P. M., Wray, N. R., Zhang, Q., Sklar, P., McCarthy, M. I., Brown, M. A., &amp; Yang, J. (2017). 10 years of GWAS discovery: biology, function, and translation. The American Journal of Human Genetics, 101(1), 5-22. Pubmed 2005 Hirschhorn, J. N., &amp; Daly, M. J. (2005). Genome-wide association studies for common diseases and complex traits. Nature reviews genetics, 6(2), 95-108. Pubmed 2006 Balding, D. J. (2006). A tutorial on statistical methods for population association studies. Nature reviews genetics, 7(10), 781-791. Pubmed 2008 McCarthy, M. I., Abecasis, G. R., Cardon, L. R., Goldstein, D. B., Little, J., Ioannidis, J., &amp; Hirschhorn, J. N. (2008). Genome-wide association studies for complex traits: consensus, uncertainty and challenges. Nature reviews genetics, 9(5), 356-369. Pubmed 2010 Price, A. L., Zaitlen, N. A., Reich, D., &amp; Patterson, N. (2010). New approaches to population stratification in genome-wide association studies. Nature reviews genetics, 11(7), 459-463. Pubmed 2009 Ioannidis, J., Thomas, G., &amp; Daly, M. J. (2009). Validating, augmenting and refining genome-wide association signals. Nature Reviews Genetics, 10(5), 318-329. Pubmed"},{"location":"90_Recommended_Reading/#topic-specific","title":"Topic-specific","text":""},{"location":"90_Recommended_Reading/#ld","title":"LD","text":"Year Reference Link 2008 Slatkin, M. (2008). Linkage disequilibrium\u2014understanding the evolutionary past and mapping the medical future. Nature Reviews Genetics, 9(6), 477-485. Pubmed"},{"location":"90_Recommended_Reading/#imputation","title":"Imputation","text":"Year Reference Link 2010 Marchini, J., &amp; Howie, B. (2010). Genotype imputation for genome-wide association studies. Nature Reviews Genetics, 11(7), 499-511. Pubmed 2018 Das S, Abecasis GR, Browning BL. (2018). Genotype Imputation from Large Reference Panels. Annu. Rev. Genomics Hum. Genet. link"},{"location":"90_Recommended_Reading/#heritability","title":"Heritability","text":"Year Reference Link 2017 Yang, J., Zeng, J., Goddard, M. E., Wray, N. R., &amp; Visscher, P. M. (2017). Concepts, estimation and interpretation of SNP-based heritability. Nature genetics, 49(9), 1304-1310. Pubmed 2009 Manolio, T. A., Collins, F. S., Cox, N. J., Goldstein, D. B., Hindorff, L. A., Hunter, D. J., \u2026 &amp; Visscher, P. M. (2009). Finding the missing heritability of complex diseases. Nature, 461 (7265), 747-753. Pubmed"},{"location":"90_Recommended_Reading/#genetic-correlation","title":"Genetic correlation","text":"Year Reference Link 2019 Van Rheenen, W., Peyrot, W. J., Schork, A. J., Lee, S. H., &amp; Wray, N. R. (2019). Genetic correlations of polygenic disease traits: from theory to practice. Nature Reviews Genetics, 20(10), 567-581. Pubmed"},{"location":"90_Recommended_Reading/#fine-mapping","title":"Fine-mapping","text":"Year Reference Link 2019 Schaid, D. J., Chen, W., &amp; Larson, N. B. (2018). From genome-wide associations to candidate causal variants by statistical fine-mapping. Nature Reviews Genetics, 19(8), 491-504. Pubmed 2023 \u738b \u9752\u6ce2, \u30b2\u30ce\u30e0\u30ef\u30a4\u30c9\u95a2\u9023\u89e3\u6790\u306e\u305d\u306e\u5148\u3078\uff1a\u7d71\u8a08\u7684fine-mapping\u306e\u57fa\u790e\u3068\u767a\u5c55, JSBi Bioinformatics Review, 2023, 4 \u5dfb, 1 \u53f7, p. 35-51 J-STAGE ### Polygenic risk scores Year Reference Link 2022 Wang, Y., Tsuo, K., Kanai, M., Neale, B. M., &amp; Martin, A. R. (2022). Challenges and opportunities for developing more generalizable polygenic risk scores. Annual review of biomedical data science. link 2020 Choi, S. W., Mak, T. S. H., &amp; O\u2019Reilly, P. F. (2020). Tutorial: a guide to performing polygenic risk score analyses. Nature protocols, 15(9), 2759-2772. Pubmed 2019 Martin, A. R., Kanai, M., Kamatani, Y., Okada, Y., Neale, B. M., &amp; Daly, M. J. (2019). Clinical use of current polygenic risk scores may exacerbate health disparities. Nature genetics, 51(4), 584-591. Pubmed"},{"location":"90_Recommended_Reading/#rare-variants","title":"Rare variants","text":"Year Reference Link 2014 Lee, S., Abecasis, G. R., Boehnke, M., &amp; Lin, X. (2014). Rare-variant association analysis: study designs and statistical tests. The American Journal of Human Genetics, 95(1), 5-23. Pubmed 2015 Auer, P. L., &amp; Lettre, G. (2015). Rare variant association studies: considerations, challenges and opportunities. Genome medicine, 7(1), 1-11. Pubmed"},{"location":"90_Recommended_Reading/#genetic-architecture","title":"Genetic architecture","text":"Year Reference Link 2018 Timpson, N. J., Greenwood, C. M., Soranzo, N., Lawson, D. J., &amp; Richards, J. B. (2018). Genetic architecture: the shape of the genetic contribution to human traits and disease. Nature Reviews Genetics, 19(2), 110-124. Pubmed"},{"location":"90_Recommended_Reading/#useful-websites","title":"Useful Websites","text":"Description Link A Bioinformatician's UNIX Toolbox http://lh3lh3.users.sourceforge.net/biounix.shtml Osaka university, Department of Statistical Genetics Homepage http://www.sg.med.osaka-u.ac.jp/school_2021.html Genome analysis wiki (Abecasis Group Wiki) https://genome.sph.umich.edu/wiki/Main_Page EPI 511, Advanced Population and Medical Genetics(Alkes Price, Harvard School of Public Health) https://alkesgroup.broadinstitute.org/EPI511 fiveMinuteStats(Matthew Stephens, Statistics and Human Genetics at the University of Chicago) https://stephens999.github.io/fiveMinuteStats Course homepage and digital textbook for Human Genome Variation with Computational Lab https://mccoy-lab.github.io/hgv_modules/"},{"location":"90_Recommended_Reading/#_1","title":"\u548c\u6587","text":"Year Category Reference 2015 Linux \u65b0\u3057\u3044Linux\u306e\u6559\u79d1\u66f8 \u5358\u884c\u672c \u2013 2015/6/6 \u4e09\u5b85 \u82f1\u660e (\u8457), \u5927\u89d2 \u7950\u4ecb (\u8457) 2012 \u7d71\u8a08\u89e3\u6790\uff08\u3068\u5c11\u3057\u6a5f\u68b0\u5b66\u7fd2\uff09 \u306f\u3058\u3081\u3066\u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58 \u5358\u884c\u672c\uff08\u30bd\u30d5\u30c8\u30ab\u30d0\u30fc\uff09 \u2013 2012/7/31 \u5e73\u4e95 \u6709\u4e09 (\u8457) 1991 \u7d71\u8a08\u89e3\u6790\uff08\u3068\u5c11\u3057\u6a5f\u68b0\u5b66\u7fd2\uff09 \u7d71\u8a08\u5b66\u5165\u9580 (\u57fa\u790e\u7d71\u8a08\u5b66\u2160) \u5358\u884c\u672c \u2013 1991/7/9 \u6771\u4eac\u5927\u5b66\u6559\u990a\u5b66\u90e8\u7d71\u8a08\u5b66\u6559\u5ba4 (\u7de8\u96c6) 1992 \u7d71\u8a08\u89e3\u6790\uff08\u3068\u5c11\u3057\u6a5f\u68b0\u5b66\u7fd2\uff09 \u81ea\u7136\u79d1\u5b66\u306e\u7d71\u8a08\u5b66 (\u57fa\u790e\u7d71\u8a08\u5b66) \u5358\u884c\u672c \u2013 1992/8/1 \u6771\u4eac\u5927\u5b66\u6559\u990a\u5b66\u90e8\u7d71\u8a08\u5b66\u6559\u5ba4 (\u7de8\u96c6) 2012 \u7d71\u8a08\u89e3\u6790\uff08\u3068\u5c11\u3057\u6a5f\u68b0\u5b66\u7fd2\uff09 \u30c7\u30fc\u30bf\u89e3\u6790\u306e\u305f\u3081\u306e\u7d71\u8a08\u30e2\u30c7\u30ea\u30f3\u30b0\u5165\u9580\u2015\u2015\u4e00\u822c\u5316\u7dda\u5f62\u30e2\u30c7\u30eb\u30fb\u968e\u5c64\u30d9\u30a4\u30ba\u30e2\u30c7\u30eb\u30fbMCMC (\u78ba\u7387\u3068\u60c5\u5831\u306e\u79d1\u5b66) \u5358\u884c\u672c \u2013 2012/5/19 \u4e45\u4fdd \u62d3\u5f25 (\u8457) 2015 \u907a\u4f1d\u7d71\u8a08\u5b66\u5168\u822c \uff08\u57fa\u790e\u304b\u3089\u767a\u5c55\u307e\u3067\uff09 \u907a\u4f1d\u7d71\u8a08\u5b66\u5165\u9580 (\u5ca9\u6ce2\u30aa\u30f3\u30c7\u30de\u30f3\u30c9\u30d6\u30c3\u30af\u30b9) \u30aa\u30f3\u30c7\u30de\u30f3\u30c9 (\u30da\u30fc\u30d1\u30fc\u30d0\u30c3\u30af) \u2013 2015/12/10 \u938c\u8c37 \u76f4\u4e4b (\u8457) 2020 \u907a\u4f1d\u7d71\u8a08\u5b66\u5168\u822c \uff08\u57fa\u790e\u304b\u3089\u767a\u5c55\u307e\u3067\uff09 \u5b9f\u9a13\u533b\u5b66 2020\u5e743\u6708 Vol.38 No.4 GWAS\u3067\u8907\u96d1\u5f62\u8cea\u3092\u89e3\u304f\u305e! \u301c\u591a\u56e0\u5b50\u75be\u60a3\u30fb\u5f62\u8cea\u306e\u30d0\u30a4\u30aa\u30ed\u30b8\u30fc\u306b\u6311\u3080\u6b21\u4e16\u4ee3\u306e\u30b2\u30ce\u30e0\u533b\u79d1\u5b66 \u5358\u884c\u672c \u2013 2020/2/23 \u938c\u8c37 \u6d0b\u4e00\u90ce (\u8457) 2020 \u907a\u4f1d\u7d71\u8a08\u5b66\u5168\u822c \uff08\u57fa\u790e\u304b\u3089\u767a\u5c55\u307e\u3067\uff09 \u30bc\u30ed\u304b\u3089\u5b9f\u8df5\u3059\u308b \u907a\u4f1d\u7d71\u8a08\u5b66\u30bb\u30df\u30ca\u30fc\u301c\u75be\u60a3\u3068\u30b2\u30ce\u30e0\u3092\u7d50\u3073\u3064\u3051\u308b \u5358\u884c\u672c \u2013 2020/3/13 \u5ca1\u7530 \u968f\u8c61 (\u8457) ~ \u907a\u4f1d\u7d71\u8a08\u5b66\u5168\u822c \uff08\u57fa\u790e\u304b\u3089\u767a\u5c55\u307e\u3067\uff09 \u907a\u4f1d\u5b50\u533b\u5b66 \u30b7\u30ea\u30fc\u30ba\u4f01\u753b Statistical Genetics\u3000\u3008\u907a\u4f1d\u7d71\u8a08\u5b66\u306e\u57fa\u790e\u3009 - \u938c\u8c37 \u6d0b\u4e00\u90ce + \u03b1"},{"location":"95_Assignment/","title":"Self training","text":""},{"location":"95_Assignment/#pca-using-1000-genome-project-dataset","title":"PCA using 1000 Genome Project Dataset","text":"<p>In this self-learning module, we would like you to put your hands on the 1000 Genome Project data and apply the skills you have learned to this mini-project.</p> <p>Aim</p> <p>Aim:</p> <ol> <li>Download 1000 Genome VCF files. </li> <li>Perform PCA using 1000 Genome samples.</li> <li>Plot the PCs of these individuals.</li> <li>Interpret the results.</li> </ol> <p>Here is a brief overview of this mini project.</p> <p></p> <p>The ultimate goal of this assignment is simple, which is to help you get familiar with the skills and the most commonly used datasets in complex trait genomics.</p> <p>Tip</p> <p>Please pay attention to the details of each step. Understanding why and how we do certain steps is much more important than running the sample code itself. </p>"},{"location":"95_Assignment/#1-download-the-publicly-available-1000-genome-vcf","title":"1. Download the publicly available 1000 Genome VCF","text":"<p>Download the files we need from 1000 Genomes Project FTP site:</p> <ol> <li>Autosome VCF files</li> <li>Ancestry information file</li> <li>Reference genome sequence</li> <li>Strict mask</li> </ol> <p>Tip</p> <ul> <li>Autosome VCF: chr1 - chr22 files: in this directory.</li> <li>Ancestry information file: download here</li> <li>Reference genome sequence: in this directory.</li> <li>Strict mask: download here</li> </ul> <p>Note</p> <p>If it takes too long or if you are using your local laptop, you can just download the files for chr1.</p> <p>Sample shell script for downloading the files</p> <pre><code>#!/bin/bash\nfor chr in $(seq 1 22)  #Note: If it takes too long, you can download just chr1.\ndo\nwget https://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/ALL.chr${chr}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\nwget https://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/ALL.chr${chr}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz.tbi\ndone\n\nwget https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz\nwget https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.fai\n\nwget https://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\nwget https://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/supporting/accessible_genome_masks/20141020.strict_mask.whole_genome.bed\n</code></pre>"},{"location":"95_Assignment/#2-re-align-normalize-and-remove-duplication","title":"2. Re-align, normalize and remove duplication","text":"<p>We need to use bcftools to process the raw vcf files. </p> <p>Install bcftools</p> <p>http://www.htslib.org/download/</p> <p>Since the variants are not normalized and also have many duplications, we need to clean the vcf files.</p> <ul> <li>Normalize: https://samtools.github.io/bcftools/bcftools.html#norm</li> <li>Remove duplication: https://samtools.github.io/bcftools/bcftools.html#common_options</li> </ul> <p>Re-align with the reference genome, normalize variants and remove duplications</p> <pre><code>#!/bin/bash\nfor chr in $(seq 1 22)\ndo\n    bcftools norm -m-any --check-ref w -f human_g1k_v37.fasta \\\n      ALL.chr\"${chr}\".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz | \\\n      bcftools annotate -I +'%CHROM:%POS:%REF:%ALT' | \\\n        bcftools norm -Ob --rm-dup both \\\n          &gt; ALL.chr\"${chr}\".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf \n    bcftools index ALL.chr\"${chr}\".phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.bcf\ndone\n</code></pre>"},{"location":"95_Assignment/#3-convert-vcf-files-to-plink-binary-format","title":"3. Convert VCF files to plink binary format","text":"<p>Example</p> <pre><code>#!/bin/bash\nfor chr in $(seq 1 22)\ndo\nplink \\\n      --bcf ALL.chr\"${chr}\".phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.bcf \\\n      --keep-allele-order \\\n      --vcf-idspace-to _ \\\n      --const-fid \\\n      --allow-extra-chr 0 \\\n      --split-x b37 no-fail \\\n      --make-bed \\\n      --out ALL.chr\"${chr}\".phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes\ndone\n</code></pre>"},{"location":"95_Assignment/#4-using-snps-only-in-strict-masks","title":"4. Using SNPs only in strict masks","text":"<p>Strict masks are in this directory.</p> <p>Strict mask</p> <p>The overlapped region with this mask is \u201ccallable\u201d (or credible variant calls). This mask was developed in the 1KG main paper and it is well explained in https://www.biostars.org/p/219634/</p> <p>Tip</p> <p>Use <code>plink --make-set</code> option with the <code>BED</code> files to extract SNPs in the strict mask.</p>"},{"location":"95_Assignment/#5-qc-it-and-prune-it-to-100k-variants","title":"5. QC it and prune it to ~ 100K variants.","text":"<p>Tip</p> <p>Use PLINK.</p> <p>QC: only SNPs (exclude indels), MAF&gt;0.1</p> <p>Pruning: <code>plink --indep-pariwise</code></p>"},{"location":"95_Assignment/#6-perform-pca","title":"6. Perform PCA","text":"<p>Tip</p> <p><code>plink --pca</code></p>"},{"location":"95_Assignment/#7-visualization-and-interpretation","title":"7. Visualization and interpretation.","text":"<p>Draw PC1 - PC2 plot and color each individual by ancestry information (from ALL.panel file). Interpret the result.</p> <p>Tip</p> <p>You can use R, python, or any other tools you like (even Excel can do the job.)</p> <p>(If you are having trouble performing any of the steps, you can also refer to: https://www.biostars.org/p/335605/.)</p>"},{"location":"95_Assignment/#checklist","title":"Checklist","text":"<ul> <li> What does variant normalization mean and What are the two principles for variant normalization?</li> <li> For chromosome 1, what is the proportion of commom variants(MAF &gt;5%) / low-frequency-variants(1&lt;=MAF &lt;5%) / and rare variants(MAF &lt;1%) ? If possible, please draw a figure showing the distribution of MAF. (plink --freq)</li> <li> What pattern did you observe from the 1KG PCA plot? </li> </ul>"},{"location":"95_Assignment/#reference","title":"Reference","text":"<ul> <li>https://www.biostars.org/p/335605/</li> <li>1000 Genomes Project Consortium. \"A global reference for human genetic variation.\" Nature 526.7571 (2015): 68.</li> </ul>"},{"location":"96_Assignment2/","title":"The final presentation for \u57fa\u790e\u6f14\u7fd2II","text":"<p>Note</p> <ul> <li>Time limit: ~15 mins</li> <li>Number of slides: less than 10 slides</li> </ul>"},{"location":"96_Assignment2/#outline","title":"Outline","text":"<p>(Just an example, there is no need to strictly follow this.)</p> <ul> <li> Describe the general workflow of GWAS in 1 slide. (1 min)</li> <li> Summarize QC steps in 1 slide (1 min)</li> <li> Summarize PCA in 1 slide (1 min)<ul> <li> explain the aims and major steps</li> </ul> </li> <li> Show your 1000 genome PCA results (~2 slides): (~4 min)<ul> <li> explain what tool and data you use.</li> <li> explain what kind of analysis you performed.</li> <li> show your results and explain the scatter plot.</li> </ul> </li> <li> Association analysis (1-2 slides) : (2 min)<ul> <li> explain the variables in the regression and the header of your GWAS results.</li> <li> explain your manhattan plot and qq plot.</li> </ul> </li> <li> Post GWAS analysis (~6 min)<ul> <li> explain what kind of post-GWAS analysis you perform.</li> <li> explain the aim for each post-GWAS analysis.</li> <li> explain the results of LDSC.</li> <li> explain the annotated results</li> </ul> </li> </ul>"},{"location":"99_About/","title":"GWAS Tutorial - <code>Fundamental Exercise II</code>","text":"<p>This tutorial is provided by the Laboratory of Complex Trait Genomics (Kamatani Lab) in the Deparment of Computational Biology and Medical Sciences at the Univerty of Tokyo. This tutorial is designed for the graduate course <code>Fundamental Exercise II</code>.</p> <p></p>"},{"location":"99_About/#main-contributors","title":"Main Contributors","text":"<ul> <li>@kamatani-lab \uff1a https://github.com/kamatani-lab</li> <li>@koido : https://github.com/koido</li> <li>@shimaomao26 : https://github.com/shimaomao26</li> <li>@Cloufield : https://github.com/Cloufield</li> </ul>"},{"location":"99_About/#contact-us","title":"Contact Us","text":"<p>This repository is currently maintained by Yunye He. </p> <p>If you have any questions or suggestions, please feel free to contact gwaslab@gmail.com.</p> <p>Enjoy this real \"Manhattan plot\"!</p> <p></p>"},{"location":"Imputation/","title":"Imputation","text":"<p>The missing data imputation is not a task specific to genetic studies.  By comparing the genotyping array (generally 500k\u20131M markers) to the reference panel (WGSed), missing markers on the array are filled.  The tabular data imputation methods could be used to impute the genotype data. However, haplotypes are coalesced from the ancestors, and the recombination events during gametogenesis,  each individual's haplotype is a mosaic of all haplotypes in a population. Given these properties, hidden Markov model (HMM) based methods usually outperform tabular data-based ones.</p> <p>This HMM was first described in Li &amp; Stephens 2003.  Here we will not go through tools over the past 20 years.  We will introduce the concept and the usage of Minimac.</p>"},{"location":"Imputation/#figure-illustration","title":"Figure illustration","text":"<p>In the figure, each row in the above panel represents a reference haplotype. The middle panel shows the genotyping array.  Genotyped markers are squared and WGS-only markers are circled. The two colors represent the ref and alt alleles.  You could also think they represent different haplotype fragments. The red triangles indicate the recombination hot spots,  which a crossover between the reference haplotypes is more likely to happen.</p> <p>Given the genotyped marker, matching probabilities are calculated for all potential paths through reference haplotypes.  Then, in this example (the real case is not this simple), we assumed at each recombination hotspot, there is a free recombination.  You will see that all paths chained by dark blue match 2 of the 4 genotyped markers. So these paths have equal probability.</p> <p>Finally, missing markers are filled with the probability-weighted alleles on each path. For the left three circles,  two paths are cyan and one path is orange, the imputation result will be 1/3 orange and 2/3 cyan.</p>"},{"location":"Imputation/#how-to-do-imputation","title":"How to do imputation","text":"<p>The simplest way is to use the Michigan or TOPMed imputation server,  if you don't have resources of WGS data. Just make your vcf, submit it to the server, and select the favored reference panel. There are built-in phasing, liftover, and QC on the server,  but we would strongly suggest checking the data and doing these steps by yourself. For example:</p> <ul> <li>Liftover between hg19 and hg38, properly flip the alleles, and exclude the ambiguous variants.</li> <li>Phasing the data locally and storing it. The phased data can be used for imputation against any reference panel.</li> <li>Check the ancestry information and select the proper reference panel.</li> </ul> <p>Another way is to run the job locally. Recent tools are memory and computation efficient, you may run it in a small in-house server or even PC.</p> <p>A typical workflow of Minimac is:</p> <p>Parameter estimation (this step will create a m3vcf reference panel file):</p> <pre><code>Minimac3 \\\n--refHaps ./phased_reference.vcf.gz \\\n--processReference \\\n--prefix ./phased_reference \\\n--log\n</code></pre> <p>Imputation:</p> <pre><code>minimac4 \\\n--refHaps ./phased_reference.m3vcf \\\n--haps ./phased_target.vcf.gz \\\n--prefix ./result \\\n--format GT,DS,HDS,GP,SD \\\n--meta \\\n--log \\\n--cpus 10\n</code></pre> <p>Details of the options.</p>"},{"location":"Imputation/#after-imputation","title":"After imputation","text":"<p>The output is a vcf file. First, we need to examine the imputation quality.  It can be a long long story and I will not explain it in detail. Most of the time,  when the following criteria meet, </p> <ul> <li>Genotyping array contains &gt; 500k markers</li> <li>Reference panel is 1KG or ancestry matched, or at least the major ancestry in the panel matches the target</li> </ul> <p>The standard imputation quality metric, named <code>Rsq</code>, efficiently discriminates the well-imputed variants at a threshold 0.7  (may loosen it to 0.3 to allow more variants in the GWAS).</p>"},{"location":"Imputation/#before-gwas","title":"Before GWAS","text":"<p>Three types of genotypes are widely used in GWAS -- best-guess genotype, allelic dosage, and genotype probability.  Using Dosage (DS) keeps the dataset smallest while most association test software only requires this information.</p>"},{"location":"PRS_evaluation/","title":"Polygenic risk scores evaluation","text":""},{"location":"PRS_evaluation/#regressions-for-evaluation-of-prs","title":"Regressions for evaluation of PRS","text":"\\[Phenotype \\sim PRS_{phenotype} + Covariates\\] \\[logit(P) \\sim PRS_{phenotype} + Covariates\\] <p>Covariates usually include sex, age and top 10 PCs.</p>"},{"location":"PRS_evaluation/#evaluation","title":"Evaluation","text":""},{"location":"PRS_evaluation/#roc-aic-auc-and-c-index","title":"ROC, AIC, AUC, and C-index","text":"<p>ROC : receiver operating characteristic curve shows the performance of a classification model at all thresholds.</p> <ul> <li>\"y\" : True Positive rate. \\({{TP}\\over{TP + FN}}\\)</li> <li>\"x\" : False Positive rate. \\({{FP}\\over{FP + TN}}\\)</li> </ul> <p>AUC: area under the ROC Curve, a common measure for the performance of a classification model.</p> <p>AIC</p> <p>Akaike Information Criterion (AIC): a measure for comparison of different statistical models.</p> \\[AIC = 2k - 2ln(\\hat{L})\\] <ul> <li>\\(k\\) : number of estimated parameters</li> <li>\\(\\hat{L}\\) : maximum value of the model likelihood function</li> </ul> <p>C-index</p> <p>C-index: concordance index, which is a metric to evaluate the predictive performance of models and is commonly used in survival analysis. It is a measure of the probability that the predicted scores \\(M_i\\) and $ M_j$ by a model of two randomly selected individuals \\(i\\) and \\(j\\), have the reverse relative order as their true event times \\(T_i, T_j\\).</p> \\[ C = Pr (M_j &gt; M_i | T_j &lt; T_i) \\] <p>Interpretation: Individuals with higher scores should have higher risk of the disease events</p> <p>Reference: Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L., &amp; Rosati, R. A. (1982). Evaluating the yield of medical tests. Jama, 247(18), 2543-2546. Reference: Longato, E., Vettoretti, M., &amp; Di Camillo, B. (2020). A practical perspective on the concordance index for the evaluation and selection of prognostic time-to-event models. Journal of Biomedical Informatics, 108, 103496.</p>"},{"location":"PRS_evaluation/#r2-and-pseudo-r2","title":"R2 and pseudo-R2","text":"<p>Coefficient of determination</p> <p>\\(R^2\\) : coefficient of determination, which measures the amount of variance explained by the regression model.</p> <p>In linear regression:</p> \\[ R^2 = 1 - {{RSS}\\over{TSS}} \\] <ul> <li>\\(RSS\\) : sum of squares of residuals</li> <li>\\(TSS\\) : total sum of squares</li> </ul> <p>Pseudo-R2 (Nagelkerke)</p> <p>In logistic regression, </p> <p>One of the most commonly used Pseudo-R2 for PRS analysis is Nagelkerke's \\(R^2\\)</p> \\[R^2_{Nagelkerke} = {{1 - ({{L_0}\\over{L_M}})^{2/n}}\\over{1 - L_0^{2/n}}}\\] <ul> <li>\\(L_0\\) : Likelihood of the null model</li> <li>\\(L_full\\) : Likelihood of the full model</li> </ul>"},{"location":"PRS_evaluation/#r2-on-the-liability-scale-lee","title":"R2 on the liability scale (Lee)","text":"<p>R2 on liability scale</p> <p>\\(R^2\\) on the liability scale for ascertained case-control studies</p> \\[ R^2_l = {{R_o^2 C}\\over{1 + R_o^2 \\theta C }} \\] <ul> <li>\\(C\\) and \\(\\theta\\) are correcting factors for ascertainment</li> <li>\\(C = {{K(1-K)}\\over{Z^2}}{{K(1-K)}\\over{P(1-P)}}\\) </li> <li> <p>\\(\\theta = m {{P-K}\\over{1-K}} ( m{{P-K}\\over{1-K}} - t)\\) </p> </li> <li> <p>\\(K\\) : population disease prevalence</p> </li> <li>\\(P\\) : sample disease prevalence</li> <li>\\(t\\): the threshold on the normal distribution truncating the proportion of disease prevalence K</li> <li>\\(m = z / K\\) : mean liability for cases</li> <li>\\(z = f(t)\\) : the value of probability density function of standard normal distribution at t </li> </ul> <p>Reference : Lee, S. H., Goddard, M. E., Wray, N. R., &amp; Visscher, P. M. (2012). A better coefficient of determination for genetic profile analysis. Genetic epidemiology, 36(3), 214-224.</p> <p>The authors also provided R codes for calculation (removed unrelated codes for simplicity)</p> <pre><code># R2 on the liability scale using the transformation\n\nnt = total number of the sample\nncase = number of cases\nncont = number of controls\nthd = the threshold on the normal distribution which truncates the proportion of disease prevalence\nK = population prevalence\nP = proportion of cases in the case-control samples\n\n#threshold\nthd = -qnorm(K,0,1)\n\n#value of standard normal density function at thd\nzv = dnorm(thd) #mean liability for case\nmv = zv/K #linear model\nlmv = lm(y\u223cg) #R20 : R2 on the observed scale\nR2O = var(lmv$fitted.values)/(ncase/nt*ncont/nt)\n\n# calculate correction factors\ntheta = mv*(P-K)/(1-K)*(mv*(P-K)/(1-K)-thd) cv = K*(1-K)/zv^2*K*(1-K)/(P*(1-P)) # convert to R2 on the liability scale\nR2 = R2O*cv/(1+R2O*theta*cv)\n</code></pre>"},{"location":"PRS_evaluation/#bootstrap-confidence-interval-methods-for-r2","title":"Bootstrap Confidence Interval Methods for R2","text":"<p>Bootstrap is a commonly used resampling method to generate a sampling distribution from the known sample dataset. It repeatedly takes random samples with replacement from the known sample dataset.</p> <p>Steps:</p> <ul> <li>Sample with replacement B times. (B should be large.)</li> <li>Estimate the parameter using the bootstrp sample. </li> <li>Obtain the approximate distribution of the parameter.</li> </ul> <p>The percentile bootstrap interval is then defined as the interval between \\(100 \\times \\alpha /2\\) and \\(100 \\times (1 - \\alpha /2)\\) percentiles of the parameters estimated by bootstrapping. We can use this method to estimate the  bootstrap interval for \\(R^2\\).</p>"},{"location":"PRS_evaluation/#reference","title":"Reference","text":"<ul> <li>R2 on liability scale: Lee, S. H., Goddard, M. E., Wray, N. R., &amp; Visscher, P. M. (2012). A better coefficient of determination for genetic profile analysis. Genetic epidemiology, 36(3), 214-224.</li> <li>C-index 1: Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L., &amp; Rosati, R. A. (1982). Evaluating the yield of medical tests. Jama, 247(18), 2543-2546.</li> <li>C-index 2: Longato, E., Vettoretti, M., &amp; Di Camillo, B. (2020). A practical perspective on the concordance index for the evaluation and selection of prognostic time-to-event models. Journal of Biomedical Informatics, 108, 103496.</li> </ul>"},{"location":"Phasing/","title":"Phasing","text":"<p>Human genome is diploid. Distribution of variants between homologous chromosomes can affect the interpretation of genotype data, such as allele specific expression,  context-informed annotation, loss-of-function compound heterozygous events. </p> <p>Example</p> <p></p> <p>( SHAPEIT5 )</p> <p>In the above illustration, when LoF variants are on both copies of a gene, the gene is thought knocked out</p> <p>Trio data and long read sequencing can solve the haplotyping problem. That is not always possible. Statistical phasing is based on the Li &amp; Stephens Markov model.  The haploid version of this model (see Imputation) is easier to understand. Because the maternal and paternal haplotypes are independent,  unphased genotype could be constructed by the addition of two haplotypes.</p> <p>Recent methods had incopoorates long IBD sharing, local haplotypes, etc, to make it tractable for large datasets.  You could read the following methods if you are interested.</p> <ul> <li>PHASE</li> <li>MaCH</li> <li>EAGLE2</li> <li>SHAPEIT5</li> <li>BEAGLE3</li> </ul>"},{"location":"Phasing/#how-to-do-phasing","title":"How to do phasing","text":"<p>In most of the cases, phasing is just a pre-step of imputation, and we do not care about how the phasing goes.  But there are several considerations, like reference-based or reference-free, large and small sample size, rare variants cutoff.  There is no single method that could best fit all cases.</p> <p>Here I show one example using EAGLE2.</p> <pre><code>eagle \\\n--vcf=target.vcf.gz \\\n--geneticMapFile=genetic_map_hg19_withX.txt.gz \\\n--chrom=19 \\\n--outPrefix=target.eagle \\\n--numThreads=10\n</code></pre>"},{"location":"TwoSampleMR/","title":"TwoSampleMR Tutorial","text":"In\u00a0[1]: Copied! <pre>library(data.table)\nlibrary(TwoSampleMR)\n</pre> library(data.table) library(TwoSampleMR) <pre>TwoSampleMR version 0.5.6 \n[&gt;] New: Option to use non-European LD reference panels for clumping etc\n[&gt;] Some studies temporarily quarantined to verify effect allele\n[&gt;] See news(package='TwoSampleMR') and https://gwas.mrcieu.ac.uk for further details\n\n\n</pre> In\u00a0[2]: Copied! <pre>exp_raw &lt;- fread(\"koges_bmi.txt.gz\")\n\nexp_raw &lt;- subset(exp_raw,exp_raw$pval&lt;5e-8)\n\nexp_raw$phenotype &lt;- \"BMI\"\n\nexp_raw$n &lt;- 72282\n\nexp_dat &lt;- format_data( exp_raw,\ntype = \"exposure\",\nsnp_col = \"rsids\",\nbeta_col = \"beta\",\nse_col = \"sebeta\",\neffect_allele_col = \"alt\",\nother_allele_col = \"ref\",\neaf_col = \"af\",\npval_col = \"pval\",\nphenotype_col = \"phenotype\",\nsamplesize_col= \"n\"\n)\nclumped_exp &lt;- clump_data(exp_dat,clump_r2=0.01,pop=\"EAS\")\n</pre> exp_raw &lt;- fread(\"koges_bmi.txt.gz\")  exp_raw &lt;- subset(exp_raw,exp_raw$pval&lt;5e-8)  exp_raw$phenotype &lt;- \"BMI\"  exp_raw$n &lt;- 72282  exp_dat &lt;- format_data( exp_raw,     type = \"exposure\",     snp_col = \"rsids\",     beta_col = \"beta\",     se_col = \"sebeta\",     effect_allele_col = \"alt\",     other_allele_col = \"ref\",     eaf_col = \"af\",     pval_col = \"pval\",     phenotype_col = \"phenotype\",     samplesize_col= \"n\" ) clumped_exp &lt;- clump_data(exp_dat,clump_r2=0.01,pop=\"EAS\")  <pre>Warning message in .fun(piece, ...):\n\u201cDuplicated SNPs present in exposure data for phenotype 'BMI. Just keeping the first instance:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrs4665740\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrs7201608\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u201d\nAPI: public: http://gwas-api.mrcieu.ac.uk/\n\nPlease look at vignettes for options on running this locally if you need to run many instances of this command.\n\nClumping rvi6Om, 2452 variants, using EAS population reference\n\nRemoving 2420 of 2452 variants due to LD with other variants or absence from LD reference panel\n\n</pre> In\u00a0[16]: Copied! <pre>out_raw &lt;- fread(\"hum0197.v3.BBJ.T2D.v1/GWASsummary_T2D_Japanese_SakaueKanai2020.auto.txt.gz\",\nselect=c(\"SNPID\",\"Allele1\",\"Allele2\",\"BETA\",\"SE\",\"p.value\",\"N\",\"AF_Allele2\"))\n\nout_raw$phenotype &lt;- \"T2D\"\n\nout_dat &lt;- format_data( out_raw,\ntype = \"outcome\",\nsnp_col = \"SNPID\",\nbeta_col = \"BETA\",\nse_col = \"SE\",\neffect_allele_col = \"Allele2\",\nother_allele_col = \"Allele1\",\npval_col = \"p.value\",\nphenotype_col = \"phenotype\",\nsamplesize_col= \"n\",\neaf_col=\"AF_Allele2\"\n)\n</pre> out_raw &lt;- fread(\"hum0197.v3.BBJ.T2D.v1/GWASsummary_T2D_Japanese_SakaueKanai2020.auto.txt.gz\",                     select=c(\"SNPID\",\"Allele1\",\"Allele2\",\"BETA\",\"SE\",\"p.value\",\"N\",\"AF_Allele2\"))  out_raw$phenotype &lt;- \"T2D\"  out_dat &lt;- format_data( out_raw,     type = \"outcome\",     snp_col = \"SNPID\",     beta_col = \"BETA\",     se_col = \"SE\",     effect_allele_col = \"Allele2\",     other_allele_col = \"Allele1\",     pval_col = \"p.value\",     phenotype_col = \"phenotype\",     samplesize_col= \"n\",     eaf_col=\"AF_Allele2\" ) <pre>Warning message in format_data(out_raw, type = \"outcome\", snp_col = \"SNPID\", beta_col = \"BETA\", :\n\u201ceffect_allele column has some values that are not A/C/T/G or an indel comprising only these characters or D/I. These SNPs will be excluded.\u201d\nWarning message in format_data(out_raw, type = \"outcome\", snp_col = \"SNPID\", beta_col = \"BETA\", :\n\u201cThe following SNP(s) are missing required information for the MR tests and will be excluded\n1:1142714:t:&lt;cn0&gt;\n1:4288465:t:&lt;ins:me:alu&gt;\n1:4882232:t:&lt;cn0&gt;\n1:5172414:g:&lt;cn0&gt;\n1:5173809:t:&lt;cn0&gt;\n1:5934301:g:&lt;ins:me:alu&gt;\n1:6814818:a:&lt;ins:me:alu&gt;\n1:7921468:c:&lt;cn2&gt;\n1:8502010:t:&lt;ins:me:alu&gt;\n1:8924066:c:&lt;cn0&gt;\n1:9171841:c:&lt;cn0&gt;\n1:9403667:a:&lt;cn2&gt;\n1:9595360:a:&lt;cn0&gt;\n1:9846036:c:&lt;cn0&gt;\n1:10067190:g:&lt;cn0&gt;\n1:10482499:g:&lt;cn0&gt;\n1:11682873:t:&lt;cn0&gt;\n1:11830220:t:&lt;ins:me:sva&gt;\n1:11988599:c:&lt;cn0&gt;\n1:12475666:t:&lt;ins:me:sva&gt;\n1:12737575:a:&lt;ins:me:alu&gt;\n1:12842004:a:&lt;cn0&gt;\n1:14437074:t:&lt;cn0&gt;\n1:14437868:a:&lt;cn0&gt;\n1:14713511:t:&lt;cn2&gt;\n1:14735732:g:&lt;cn0&gt;\n1:15343948:g:&lt;cn0&gt;\n1:16151682:c:&lt;cn0&gt;\n1:16329336:t:&lt;ins:me:sva&gt;\n1:16358741:g:&lt;cn0&gt;\n1:17676165:a:&lt;cn0&gt;\n1:19486410:c:&lt;ins:me:alu&gt;\n1:19855608:a:&lt;cn2&gt;\n1:20257109:t:&lt;ins:me:alu&gt;\n1:20310746:g:&lt;cn0&gt;\n1:20496899:c:&lt;cn0&gt;\n1:20497183:c:&lt;cn0&gt;\n1:20864015:t:&lt;cn0&gt;\n1:20944751:c:&lt;ins:me:alu&gt;\n1:21346279:a:&lt;cn0&gt;\n1:21492591:c:&lt;ins:me:alu&gt;\n1:21786418:t:&lt;cn0&gt;\n1:22302473:t:&lt;cn0&gt;\n1:22901908:t:&lt;ins:me:alu&gt;\n1:23908383:g:&lt;cn0&gt;\n1:24223580:g:&lt;cn0&gt;\n1:24520350:g:&lt;cn0&gt;\n1:24804603:c:&lt;cn0&gt;\n1:25055152:g:&lt;cn0&gt;\n1:26460095:a:&lt;cn0&gt;\n1:26961278:g:&lt;cn0&gt;\n1:29373390:t:&lt;ins:me:alu&gt;\n1:31090520:t:&lt;ins:me:alu&gt;\n1:31316259:t:&lt;cn0&gt;\n1:31720009:a:&lt;cn0&gt;\n1:32535965:g:&lt;cn0&gt;\n1:32544371:a:&lt;cn0&gt;\n1:33785116:c:&lt;cn0&gt;\n1:35101427:c:&lt;cn0&gt;\n1:35177287:g:&lt;cn0&gt;\n1:35627104:t:&lt;cn0&gt;\n1:36474694:t:&lt;ins:me:alu&gt;\n1:36733282:t:&lt;cn0&gt;\n1:37215810:a:&lt;ins:me:alu&gt;\n1:37816478:a:&lt;cn0&gt;\n1:38132306:t:&lt;cn0&gt;\n1:39084231:a:&lt;cn0&gt;\n1:39677675:t:&lt;ins:me:alu&gt;\n1:40524704:t:&lt;ins:me:alu&gt;\n1:40552356:a:&lt;cn0&gt;\n1:40976681:g:&lt;cn0&gt;\n1:41021684:a:&lt;cn0&gt;\n1:41785500:a:&lt;ins:me:line1&gt;\n1:42390318:c:&lt;ins:me:alu&gt;\n1:43694061:t:&lt;cn0&gt;\n1:44059290:a:&lt;inv&gt;\n1:45021223:t:&lt;cn0&gt;\n1:45708588:a:&lt;cn0&gt;\n1:45822649:t:&lt;cn0&gt;\n1:46333195:a:&lt;ins:me:alu&gt;\n1:46794814:t:&lt;ins:me:alu&gt;\n1:47267517:t:&lt;cn0&gt;\n1:47346571:a:&lt;cn0&gt;\n1:47623401:a:&lt;cn0&gt;\n1:47913001:t:&lt;cn0&gt;\n1:48820285:t:&lt;ins:me:alu&gt;\n1:48972537:g:&lt;ins:me:alu&gt;\n1:49357693:t:&lt;ins:me:alu&gt;\n1:49428756:t:&lt;ins:me:line1&gt;\n1:49861993:g:&lt;ins:me:alu&gt;\n1:50912662:c:&lt;ins:me:alu&gt;\n1:51102445:t:&lt;cn0&gt;\n1:52146313:a:&lt;cn0&gt;\n1:53594175:t:&lt;cn0&gt;\n1:53595112:c:&lt;cn0&gt;\n1:55092043:g:&lt;cn0&gt;\n1:55341923:c:&lt;cn0&gt;\n1:55342224:g:&lt;cn0&gt;\n1:55927718:a:&lt;cn0&gt;\n1:56268665:t:&lt;ins:me:line1&gt;\n1:56405404:t:&lt;ins:me:line1&gt;\n1:56879062:t:&lt;ins:me:alu&gt;\n1:57100960:t:&lt;ins:me:sva&gt;\n1:57208746:a:&lt;cn0&gt;\n1:58722032:t:&lt;cn2&gt;\n1:58743910:a:&lt;cn0&gt;\n1:58795378:a:&lt;cn0&gt;\n1:59205317:t:&lt;ins:me:alu&gt;\n1:59591483:t:&lt;ins:me:alu&gt;\n1:59871876:t:&lt;ins:me:alu&gt;\n1:60046725:a:&lt;cn0&gt;\n1:60048628:c:&lt;cn0&gt;\n1:60470604:t:&lt;ins:me:alu&gt;\n1:60487912:t:&lt;cn0&gt;\n1:60715714:t:&lt;ins:me:line1&gt;\n1:61144594:c:&lt;ins:me:alu&gt;\n1:62082822:a:&lt;cn0&gt;\n1:62113386:c:&lt;cn0&gt;\n1:62479250:t:&lt;cn0&gt;\n1:62622902:g:&lt;cn0&gt;\n1:62654739:c:&lt;cn0&gt;\n1:63841704:c:&lt;ins:me:alu&gt;\n1:64720497:a:&lt;cn0&gt;\n1:64850193:a:&lt;ins:me:sva&gt;\n1:65346960:t:&lt;ins:me:alu&gt;\n1:65412505:a:&lt;cn0&gt;\n1:68375746:a:&lt;cn0&gt;\n1:70061670:g:&lt;ins:me:alu&gt;\n1:70091056:t:&lt;ins:me:alu&gt;\n1:70093557:c:&lt;ins:me:alu&gt;\n1:70412360:t:&lt;ins:me:alu&gt;\n1:70424730:t:&lt;cn2&gt;\n1:70820401:t:&lt;cn0&gt;\n1:70912433:g:&lt;ins:me:alu&gt;\n1:72449620:a:&lt;cn0&gt;\n1:72755694:t:&lt;cn0&gt;\n1:72766343:t:&lt;cn0&gt;\n1:72778537:g:&lt;cn0&gt;\n1:73092779:c:&lt;cn2&gt;\n1:74312425:a:&lt;cn0&gt;\n1:75148055:t:&lt;ins:me:alu&gt;\n1:75192907:c:&lt;ins:me:line1&gt;\n1:75301685:t:&lt;ins:me:alu&gt;\n1:75557174:c:&lt;ins:me:alu&gt;\n1:76392967:t:&lt;ins:me:alu&gt;\n1:76416074:a:&lt;ins:me:alu&gt;\n1:76900598:c:&lt;cn0&gt;\n1:77577928:t:&lt;ins:me:alu&gt;\n1:77634327:a:&lt;ins:me:alu&gt;\n1:77764994:t:&lt;ins:me:alu&gt;\n1:77830614:t:&lt;cn0&gt;\n1:78446240:c:&lt;ins:me:sva&gt;\n1:78607067:t:&lt;ins:me:alu&gt;\n1:78649157:a:&lt;cn0&gt;\n1:78800902:t:&lt;ins:me:line1&gt;\n1:79108845:t:&lt;ins:me:alu&gt;\n1:79331208:c:&lt;ins:me:alu&gt;\n1:79582082:t:&lt;ins:me:alu&gt;\n1:79855600:c:&lt;cn0&gt;\n1:80221781:t:&lt;cn0&gt;\n1:80299106:t:&lt;ins:me:alu&gt;\n1:80504615:t:&lt;cn0&gt;\n1:80554065:t:&lt;cn0&gt;\n1:80955976:t:&lt;ins:me:line1&gt;\n1:81422415:c:&lt;cn0&gt;\n1:82312054:g:&lt;ins:me:alu&gt;\n1:82850409:g:&lt;ins:me:alu&gt;\n1:83041946:t:&lt;cn0&gt;\n1:84056670:a:&lt;cn0&gt;\n1:84388330:g:&lt;cn0&gt;\n1:84517858:a:&lt;cn0&gt;\n1:84712009:g:&lt;cn0&gt;\n1:84913274:c:&lt;ins:me:alu&gt;\n1:85293152:g:&lt;ins:me:alu&gt;\n1:85620127:t:&lt;ins:me:alu&gt;\n1:85910957:g:&lt;cn0&gt;\n1:86400829:t:&lt;cn0&gt;\n1:86696940:a:&lt;ins:me:alu&gt;\n1:87064962:c:&lt;cn2&gt;\n1:87096974:c:&lt;cn0&gt;\n1:87096990:t:&lt;cn0&gt;\n1:88813625:t:&lt;ins:me:alu&gt;\n1:89209563:t:&lt;ins:me:alu&gt;\n1:89733616:t:&lt;ins:me:line1&gt;\n1:89811425:g:&lt;cn0&gt;\n1:90370569:t:&lt;ins:me:alu&gt;\n1:90914512:g:&lt;ins:me:line1&gt;\n1:91878937:g:&lt;cn0&gt;\n1:92131841:g:&lt;inv&gt;\n1:92232051:t:&lt;cn0&gt;\n1:93291972:c:&lt;cn0&gt;\n1:93498232:t:&lt;ins:me:alu&gt;\n1:94288372:c:&lt;cn0&gt;\n1:95192010:a:&lt;ins:me:line1&gt;\n1:95342701:g:&lt;ins:me:alu&gt;\n1:95522242:t:&lt;cn0&gt;\n1:97458273:t:&lt;inv&gt;\n1:98605297:t:&lt;ins:me:alu&gt;\n1:99610528:a:&lt;ins:me:alu&gt;\n1:99698454:g:&lt;ins:me:alu&gt;\n1:100355940:a:&lt;ins:me:alu&gt;\n1:100645536:g:&lt;ins:me:alu&gt;\n1:100994221:g:&lt;ins:me:alu&gt;\n1:101693230:t:&lt;cn0&gt;\n1:101695346:a:&lt;cn0&gt;\n1:101770067:g:&lt;ins:me:alu&gt;\n1:101978980:t:&lt;ins:me:line1&gt;\n1:102568923:g:&lt;ins:me:line1&gt;\n1:102920544:t:&lt;ins:me:alu&gt;\n1:103054499:t:&lt;ins:me:alu&gt;\n1:104359763:g:&lt;cn0&gt;\n1:104443176:t:&lt;cn0&gt;\n1:104574487:t:&lt;ins:me:alu&gt;\n1:105054083:t:&lt;ins:me:alu&gt;\n1:105070244:c:&lt;ins:me:alu&gt;\n1:105138650:t:&lt;ins:me:alu&gt;\n1:105231111:t:&lt;ins:me:alu&gt;\n1:105832823:g:&lt;cn0&gt;\n1:106015797:t:&lt;cn0&gt;\n1:106978443:t:&lt;cn0&gt;\n1:107896853:g:&lt;cn0&gt;\n1:107949843:t:&lt;ins:me:alu&gt;\n1:108142479:t:&lt;ins:me:alu&gt;\n1:108369370:a:&lt;cn0&gt;\n1:108402972:a:&lt;cn0&gt;\n1:109366972:g:&lt;cn0&gt;\n1:109573240:a:&lt;cn0&gt;\n1:110187159:a:&lt;cn0&gt;\n1:110225019:c:&lt;cn0&gt;\n1:111013750:a:&lt;cn0&gt;\n1:111472607:g:&lt;cn0&gt;\n1:111802597:g:&lt;ins:me:sva&gt;\n1:111827762:a:&lt;cn0&gt;\n1:111896187:c:&lt;ins:me:sva&gt;\n1:112032284:t:&lt;ins:me:alu&gt;\n1:112123691:t:&lt;ins:me:alu&gt;\n1:112691740:a:&lt;cn0&gt;\n1:112736007:a:&lt;ins:me:alu&gt;\n1:112992009:t:&lt;ins:me:alu&gt;\n1:113799625:g:&lt;cn0&gt;\n1:114925678:t:&lt;cn0&gt;\n1:115178042:c:&lt;cn0&gt;\n1:116229468:c:&lt;cn0&gt;\n1:116983571:t:&lt;ins:me:alu&gt;\n1:117593370:a:&lt;cn0&gt;\n1:119526940:a:&lt;cn0&gt;\n1:119553366:c:&lt;ins:me:line1&gt;\n1:120012853:a:&lt;cn0&gt;\n1:152555495:g:&lt;cn0&gt;\n1:152643788:a:&lt;cn0&gt;\n1:152760084:c:&lt;cn0&gt;\n1:153133703:a:&lt;cn0&gt;\n1:154123770:t:&lt;ins:me:alu&gt;\n1:154324167:g:&lt;cn0&gt;\n1:154865017:g:&lt;ins:me:alu&gt;\n1:157173860:t:&lt;cn0&gt;\n1:157363502:t:&lt;ins:me:alu&gt;\n1:157540655:g:&lt;cn0&gt;\n1:157887236:t:&lt;inv&gt;\n1:158371473:a:&lt;ins:me:alu&gt;\n1:158488410:a:&lt;cn0&gt;\n1:158726918:a:&lt;cn0&gt;\n1:160979498:c:&lt;cn0&gt;\n1:162263027:t:&lt;ins:me:alu&gt;\n1:163088865:t:&lt;ins:me:alu&gt;\n1:163314443:g:&lt;ins:me:alu&gt;\n1:163639693:t:&lt;ins:me:alu&gt;\n1:165553149:t:&lt;ins:me:line1&gt;\n1:165861400:t:&lt;ins:me:sva&gt;\n1:166189445:t:&lt;ins:me:alu&gt;\n1:167506110:g:&lt;ins:me:alu&gt;\n1:167712862:g:&lt;ins:me:alu&gt;\n1:168926083:a:&lt;ins:me:sva&gt;\n1:169004356:c:&lt;cn0&gt;\n1:169042039:c:&lt;cn0&gt;\n1:169225213:t:&lt;cn0&gt;\n1:169524859:t:&lt;ins:me:line1&gt;\n1:170603451:a:&lt;ins:me:alu&gt;\n1:170991168:c:&lt;ins:me:alu&gt;\n1:171358314:t:&lt;ins:me:alu&gt;\n1:172177959:g:&lt;cn0&gt;\n1:172825753:g:&lt;cn0&gt;\n1:173811663:a:&lt;cn0&gt;\n1:174654509:g:&lt;cn0&gt;\n1:174796517:t:&lt;cn0&gt;\n1:174894014:g:&lt;cn0&gt;\n1:175152408:g:&lt;cn0&gt;\n1:177509016:g:&lt;cn0&gt;\n1:177544393:g:&lt;cn0&gt;\n1:177946159:a:&lt;cn0&gt;\n1:178397612:t:&lt;ins:me:alu&gt;\n1:178495321:a:&lt;cn0&gt;\n1:178692798:t:&lt;ins:me:alu&gt;\n1:179491966:t:&lt;ins:me:alu&gt;\n1:179607260:a:&lt;cn0&gt;\n1:180272299:a:&lt;cn0&gt;\n1:180857564:c:&lt;ins:me:alu&gt;\n1:181043348:a:&lt;cn0&gt;\n1:181588360:t:&lt;ins:me:alu&gt;\n1:181601286:t:&lt;ins:me:alu&gt;\n1:181853551:g:&lt;ins:me:alu&gt;\n1:182420857:t:&lt;ins:me:alu&gt;\n1:183308627:a:&lt;cn0&gt;\n1:185009806:t:&lt;cn0&gt;\n1:185504717:c:&lt;ins:me:alu&gt;\n1:185584799:t:&lt;ins:me:alu&gt;\n1:185857064:a:&lt;cn0&gt;\n1:187464747:t:&lt;cn0&gt;\n1:187522081:g:&lt;ins:me:alu&gt;\n1:187609013:t:&lt;cn0&gt;\n1:187716053:g:&lt;cn0&gt;\n1:187932575:t:&lt;cn0&gt;\n1:187955397:c:&lt;ins:me:alu&gt;\n1:188174657:t:&lt;ins:me:alu&gt;\n1:188186464:t:&lt;ins:me:alu&gt;\n1:188438213:t:&lt;ins:me:alu&gt;\n1:188615934:g:&lt;ins:me:alu&gt;\n1:189247039:a:&lt;ins:me:alu&gt;\n1:190052658:t:&lt;cn0&gt;\n1:190309695:t:&lt;cn0&gt;\n1:190773296:t:&lt;ins:me:alu&gt;\n1:190874469:t:&lt;ins:me:alu&gt;\n1:191466954:t:&lt;ins:me:line1&gt;\n1:191580781:a:&lt;ins:me:alu&gt;\n1:191817437:c:&lt;ins:me:alu&gt;\n1:191916438:t:&lt;cn0&gt;\n1:192008678:t:&lt;ins:me:line1&gt;\n1:192262268:a:&lt;ins:me:line1&gt;\n1:193549655:c:&lt;ins:me:line1&gt;\n1:193675125:t:&lt;ins:me:alu&gt;\n1:193999047:t:&lt;cn0&gt;\n1:194067859:t:&lt;ins:me:alu&gt;\n1:194575585:t:&lt;cn0&gt;\n1:194675140:c:&lt;ins:me:alu&gt;\n1:195146820:c:&lt;ins:me:alu&gt;\n1:195746415:a:&lt;ins:me:line1&gt;\n1:195885406:g:&lt;cn0&gt;\n1:195904499:g:&lt;cn0&gt;\n1:196464453:a:&lt;ins:me:line1&gt;\n1:196602664:a:&lt;cn0&gt;\n1:196728877:g:&lt;cn0&gt;\n1:196734744:a:&lt;cn0&gt;\n1:196761370:t:&lt;ins:me:alu&gt;\n1:197756784:c:&lt;inv&gt;\n1:197894025:c:&lt;cn0&gt;\n1:198093872:c:&lt;ins:me:alu&gt;\n1:198243300:t:&lt;ins:me:alu&gt;\n1:198529696:t:&lt;ins:me:line1&gt;\n1:198757296:t:&lt;cn0&gt;\n1:198773749:t:&lt;cn0&gt;\n1:198815313:a:&lt;ins:me:alu&gt;\n1:202961159:t:&lt;ins:me:alu&gt;\n1:203684252:t:&lt;cn0&gt;\n1:204238474:c:&lt;ins:me:alu&gt;\n1:204345055:t:&lt;ins:me:alu&gt;\n1:204381864:c:&lt;cn0&gt;\n1:205178526:t:&lt;inv&gt;\u201d\n</pre> In\u00a0[17]: Copied! <pre>harmonized_data &lt;- harmonise_data(clumped_exp,out_dat,action=1)\n</pre> harmonized_data &lt;- harmonise_data(clumped_exp,out_dat,action=1) <pre>Harmonising BMI (rvi6Om) and T2D (ETcv15)\n\n</pre> In\u00a0[18]: Copied! <pre>harmonized_data\n</pre> harmonized_data A data.frame: 28 \u00d7 29 SNPeffect_allele.exposureother_allele.exposureeffect_allele.outcomeother_allele.outcomebeta.exposurebeta.outcomeeaf.exposureeaf.outcomeremove\u22efpval.exposurese.exposuresamplesize.exposureexposuremr_keep.exposurepval_origin.exposureid.exposureactionmr_keepsamplesize.outcome &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;lgl&gt;\u22ef&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;lgl&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;lgl&gt;&lt;lgl&gt; 1rs10198356GAGA 0.044 0.0278218160.4500.46949841FALSE\u22ef1.5e-170.005172282BMITRUEreportedrvi6Om1TRUENA 2rs10209994CACA 0.030 0.0284334240.6400.65770918FALSE\u22ef2.0e-080.005472282BMITRUEreportedrvi6Om1TRUENA 3rs10824329AGAG 0.029 0.0182171190.5100.56240335FALSE\u22ef1.7e-080.005172282BMITRUEreportedrvi6Om1TRUENA 4rs10938397GAGA 0.036 0.0445547360.2800.29915686FALSE\u22ef1.0e-100.005672282BMITRUEreportedrvi6Om1TRUENA 5rs11066132TCTC-0.053-0.0319288060.1600.24197159FALSE\u22ef1.0e-130.007172282BMITRUEreportedrvi6Om1TRUENA 6rs12522139GTGT-0.037-0.0107492430.2700.24543922FALSE\u22ef1.8e-100.005772282BMITRUEreportedrvi6Om1TRUENA 7rs12591730AGAG 0.037 0.0330428120.2200.25367536FALSE\u22ef1.5e-080.006572282BMITRUEreportedrvi6Om1TRUENA 8rs13013021TCTC 0.070 0.1040752230.9070.90195307FALSE\u22ef1.9e-150.008872282BMITRUEreportedrvi6Om1TRUENA 9rs1955337 TGTG 0.036 0.0195935030.3000.24112816FALSE\u22ef7.4e-110.005672282BMITRUEreportedrvi6Om1TRUENA 10rs2076308 CGCG 0.037 0.0413520380.3100.31562874FALSE\u22ef3.4e-110.005572282BMITRUEreportedrvi6Om1TRUENA 11rs2278557 GCGC 0.034 0.0212111960.3200.29052039FALSE\u22ef7.4e-100.005572282BMITRUEreportedrvi6Om1TRUENA 12rs2304608 ACAC 0.031 0.0466695150.4700.44287320FALSE\u22ef1.1e-090.005172282BMITRUEreportedrvi6Om1TRUENA 13rs2531995 TCTC 0.031 0.0433160150.3700.33584772FALSE\u22ef5.2e-090.005372282BMITRUEreportedrvi6Om1TRUENA 14rs261967  CACA 0.032 0.0489708280.4400.39718313FALSE\u22ef3.5e-100.005172282BMITRUEreportedrvi6Om1TRUENA 15rs35332469CTCT-0.035 0.0080755980.2200.17678428FALSE\u22ef3.6e-080.006372282BMITRUEreportedrvi6Om1TRUENA 16rs35560038TATA-0.047 0.0739350890.5900.61936434FALSE\u22ef1.4e-190.005272282BMITRUEreportedrvi6Om1TRUENA 17rs3755804 TCTC 0.043 0.0228541340.2800.30750660FALSE\u22ef1.5e-140.005672282BMITRUEreportedrvi6Om1TRUENA 18rs4470425 ACAC-0.030-0.0208441370.4500.44152032FALSE\u22ef4.9e-090.005172282BMITRUEreportedrvi6Om1TRUENA 19rs476828  CTCT 0.067 0.0786518590.2700.25309742FALSE\u22ef2.8e-310.005772282BMITRUEreportedrvi6Om1TRUENA 20rs4883723 AGAG 0.039 0.0213709100.2800.22189601FALSE\u22ef8.3e-120.005772282BMITRUEreportedrvi6Om1TRUENA 21rs509325  GTGT 0.065 0.0356917590.2800.26816326FALSE\u22ef7.8e-310.005772282BMITRUEreportedrvi6Om1TRUENA 22rs55872725TCTC 0.090 0.1215170230.1200.20355108FALSE\u22ef1.8e-310.007772282BMITRUEreportedrvi6Om1TRUENA 23rs6089309 CTCT-0.033-0.0186698330.7000.65803267FALSE\u22ef3.5e-090.005672282BMITRUEreportedrvi6Om1TRUENA 24rs6265    TCTC-0.049-0.0316426960.4600.40541994FALSE\u22ef6.1e-220.005172282BMITRUEreportedrvi6Om1TRUENA 25rs6736712 GCGC-0.053-0.0297168990.9170.93023505FALSE\u22ef2.1e-080.009572282BMITRUEreportedrvi6Om1TRUENA 26rs7560832 CACA-0.150-0.0904811950.0120.01129784FALSE\u22ef2.0e-090.025072282BMITRUEreportedrvi6Om1TRUENA 27rs825486  TCTC-0.031 0.0190735540.6900.75485104FALSE\u22ef3.1e-080.005672282BMITRUEreportedrvi6Om1TRUENA 28rs9348441 ATAT-0.036 0.1792307940.4700.42502848FALSE\u22ef1.3e-120.005172282BMITRUEreportedrvi6Om1TRUENA In\u00a0[6]: Copied! <pre>res &lt;- mr(harmonized_data)\n</pre> res &lt;- mr(harmonized_data) <pre>Analysing 'rvi6Om' on 'hff6sO'\n\n</pre> In\u00a0[7]: Copied! <pre>res\n</pre> res A data.frame: 5 \u00d7 9 id.exposureid.outcomeoutcomeexposuremethodnsnpbsepval &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; rvi6Omhff6sOT2DBMIMR Egger                 281.33375800.694852606.596064e-02 rvi6Omhff6sOT2DBMIWeighted median          280.62989800.085163151.399605e-13 rvi6Omhff6sOT2DBMIInverse variance weighted280.55989560.232258061.592361e-02 rvi6Omhff6sOT2DBMISimple mode              280.60978420.133054299.340189e-05 rvi6Omhff6sOT2DBMIWeighted mode            280.59467780.126803557.011481e-05 In\u00a0[8]: Copied! <pre>mr_heterogeneity(harmonized_data)\n</pre> mr_heterogeneity(harmonized_data) A data.frame: 2 \u00d7 8 id.exposureid.outcomeoutcomeexposuremethodQQ_dfQ_pval &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; rvi6Omhff6sOT2DBMIMR Egger                 670.7022261.000684e-124 rvi6Omhff6sOT2DBMIInverse variance weighted706.6579271.534239e-131 In\u00a0[9]: Copied! <pre>mr_pleiotropy_test(harmonized_data)\n</pre> mr_pleiotropy_test(harmonized_data) A data.frame: 1 \u00d7 7 id.exposureid.outcomeoutcomeexposureegger_interceptsepval &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; rvi6Omhff6sOT2DBMI-0.036036970.03052410.2484472 In\u00a0[10]: Copied! <pre>res_single &lt;- mr_singlesnp(harmonized_data)\n</pre> res_single &lt;- mr_singlesnp(harmonized_data) In\u00a0[11]: Copied! <pre>res_single\n</pre> res_single A data.frame: 30 \u00d7 9 exposureoutcomeid.exposureid.outcomesamplesizeSNPbsep &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;lgl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; 1BMIT2Drvi6Omhff6sONArs10198356                      0.63231400.20828372.398742e-03 2BMIT2Drvi6Omhff6sONArs10209994                      0.94778080.32258143.302164e-03 3BMIT2Drvi6Omhff6sONArs10824329                      0.62817650.32462145.297739e-02 4BMIT2Drvi6Omhff6sONArs10938397                      1.23763160.27758548.251150e-06 5BMIT2Drvi6Omhff6sONArs11066132                      0.60243030.22324016.963693e-03 6BMIT2Drvi6Omhff6sONArs12522139                      0.29052010.28902403.148119e-01 7BMIT2Drvi6Omhff6sONArs12591730                      0.89304900.30766873.700413e-03 8BMIT2Drvi6Omhff6sONArs13013021                      1.48678890.22077771.646925e-11 9BMIT2Drvi6Omhff6sONArs1955337                       0.54426400.29941466.910079e-02 10BMIT2Drvi6Omhff6sONArs2076308                       1.11762260.26579692.613132e-05 11BMIT2Drvi6Omhff6sONArs2278557                       0.62385870.29681843.556906e-02 12BMIT2Drvi6Omhff6sONArs2304608                       1.50546820.29689053.961740e-07 13BMIT2Drvi6Omhff6sONArs2531995                       1.39729080.31301578.045689e-06 14BMIT2Drvi6Omhff6sONArs261967                        1.53033840.29211921.616714e-07 15BMIT2Drvi6Omhff6sONArs35332469                     -0.23073140.34792195.072217e-01 16BMIT2Drvi6Omhff6sONArs35560038                     -1.57308700.20189686.619637e-15 17BMIT2Drvi6Omhff6sONArs3755804                       0.53149150.23250732.225933e-02 18BMIT2Drvi6Omhff6sONArs4470425                       0.69480460.30799442.407689e-02 19BMIT2Drvi6Omhff6sONArs476828                        1.17390830.15685507.207355e-14 20BMIT2Drvi6Omhff6sONArs4883723                       0.54797210.28550045.494141e-02 21BMIT2Drvi6Omhff6sONArs509325                        0.54910400.15981965.908641e-04 22BMIT2Drvi6Omhff6sONArs55872725                      1.35018910.12597918.419325e-27 23BMIT2Drvi6Omhff6sONArs6089309                       0.56575250.33470099.096620e-02 24BMIT2Drvi6Omhff6sONArs6265                          0.64576930.19018716.851804e-04 25BMIT2Drvi6Omhff6sONArs6736712                       0.56069620.34487841.039966e-01 26BMIT2Drvi6Omhff6sONArs7560832                       0.60320800.29049723.785077e-02 27BMIT2Drvi6Omhff6sONArs825486                       -0.61527590.35003347.878772e-02 28BMIT2Drvi6Omhff6sONArs9348441                      -4.97863320.25727821.992909e-83 29BMIT2Drvi6Omhff6sONAAll - Inverse variance weighted 0.55989560.23225811.592361e-02 30BMIT2Drvi6Omhff6sONAAll - MR Egger                  1.33375800.69485266.596064e-02 In\u00a0[12]: Copied! <pre>res_loo &lt;- mr_leaveoneout(harmonized_data)\nres_loo\n</pre> res_loo &lt;- mr_leaveoneout(harmonized_data) res_loo A data.frame: 29 \u00d7 9 exposureoutcomeid.exposureid.outcomesamplesizeSNPbsep &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;lgl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; 1BMIT2Drvi6Omhff6sONArs101983560.55628340.24249172.178871e-02 2BMIT2Drvi6Omhff6sONArs102099940.55205760.23881222.079526e-02 3BMIT2Drvi6Omhff6sONArs108243290.55853350.23902391.945341e-02 4BMIT2Drvi6Omhff6sONArs109383970.54126880.23887092.345460e-02 5BMIT2Drvi6Omhff6sONArs110661320.55806060.24172752.096381e-02 6BMIT2Drvi6Omhff6sONArs125221390.56671020.23950641.797373e-02 7BMIT2Drvi6Omhff6sONArs125917300.55248020.23909902.085075e-02 8BMIT2Drvi6Omhff6sONArs130130210.51897150.23868082.968017e-02 9BMIT2Drvi6Omhff6sONArs1955337 0.56026350.23945051.929468e-02 10BMIT2Drvi6Omhff6sONArs2076308 0.54313550.23944032.330758e-02 11BMIT2Drvi6Omhff6sONArs2278557 0.55836340.23949241.972992e-02 12BMIT2Drvi6Omhff6sONArs2304608 0.53725570.23773252.382639e-02 13BMIT2Drvi6Omhff6sONArs2531995 0.54190160.23797122.277590e-02 14BMIT2Drvi6Omhff6sONArs261967  0.53587610.23766862.415093e-02 15BMIT2Drvi6Omhff6sONArs353324690.57359070.23783451.587739e-02 16BMIT2Drvi6Omhff6sONArs355600380.67349060.22178042.391474e-03 17BMIT2Drvi6Omhff6sONArs3755804 0.56102150.24132492.008503e-02 18BMIT2Drvi6Omhff6sONArs4470425 0.55689930.23926321.993549e-02 19BMIT2Drvi6Omhff6sONArs476828  0.50375550.24432243.922224e-02 20BMIT2Drvi6Omhff6sONArs4883723 0.56020500.23973251.945000e-02 21BMIT2Drvi6Omhff6sONArs509325  0.56084290.24685062.308693e-02 22BMIT2Drvi6Omhff6sONArs558727250.44194460.24547717.180543e-02 23BMIT2Drvi6Omhff6sONArs6089309 0.55978590.23889021.911519e-02 24BMIT2Drvi6Omhff6sONArs6265    0.55470680.24369102.282978e-02 25BMIT2Drvi6Omhff6sONArs6736712 0.55988150.23876021.902944e-02 26BMIT2Drvi6Omhff6sONArs7560832 0.55881130.23962291.969836e-02 27BMIT2Drvi6Omhff6sONArs825486  0.58000260.23675451.429330e-02 28BMIT2Drvi6Omhff6sONArs9348441 0.73789670.13668386.717515e-08 29BMIT2Drvi6Omhff6sONAAll       0.55989560.23225811.592361e-02 In\u00a0[29]: Copied! <pre>harmonized_data$\"r.outcome\" &lt;- get_r_from_lor(\nharmonized_data$\"beta.outcome\",\nharmonized_data$\"eaf.outcome\",\n45383,\n132032,\n0.26,\nmodel = \"logit\",\ncorrection = FALSE\n)\n</pre> harmonized_data$\"r.outcome\" &lt;- get_r_from_lor(   harmonized_data$\"beta.outcome\",   harmonized_data$\"eaf.outcome\",   45383,   132032,   0.26,   model = \"logit\",   correction = FALSE ) In\u00a0[34]: Copied! <pre>out &lt;- directionality_test(harmonized_data)\nout\n</pre> out &lt;- directionality_test(harmonized_data) out <pre>r.exposure and/or r.outcome not present.\n\nCalculating approximate SNP-exposure and/or SNP-outcome correlations, assuming all are quantitative traits. Please pre-calculate r.exposure and/or r.outcome using get_r_from_lor() for any binary traits\n\n</pre> A data.frame: 1 \u00d7 8 id.exposureid.outcomeexposureoutcomesnp_r2.exposuresnp_r2.outcomecorrect_causal_directionsteiger_pval &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;lgl&gt;&lt;dbl&gt; rvi6OmETcv15BMIT2D0.021254530.005496427TRUENA In\u00a0[\u00a0]: Copied! <pre>res &lt;- mr(harmonized_data)\np1 &lt;- mr_scatter_plot(res, harmonized_data)\np1[[1]]\n</pre> res &lt;- mr(harmonized_data) p1 &lt;- mr_scatter_plot(res, harmonized_data) p1[[1]] In\u00a0[\u00a0]: Copied! <pre>res_single &lt;- mr_singlesnp(harmonized_data)\np2 &lt;- mr_forest_plot(res_single)\np2[[1]]\n</pre> res_single &lt;- mr_singlesnp(harmonized_data) p2 &lt;- mr_forest_plot(res_single) p2[[1]] In\u00a0[\u00a0]: Copied! <pre>res_loo &lt;- mr_leaveoneout(harmonized_data)\np3 &lt;- mr_leaveoneout_plot(res_loo)\np3[[1]]\n</pre> res_loo &lt;- mr_leaveoneout(harmonized_data) p3 &lt;- mr_leaveoneout_plot(res_loo) p3[[1]] In\u00a0[\u00a0]: Copied! <pre>res_single &lt;- mr_singlesnp(harmonized_data)\np4 &lt;- mr_funnel_plot(res_single)\np4[[1]]\n</pre> res_single &lt;- mr_singlesnp(harmonized_data) p4 &lt;- mr_funnel_plot(res_single) p4[[1]] In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Visualization/","title":"Visualization by gwaslab","text":"In\u00a0[1]: Copied! <pre>import gwaslab as gl\n</pre> import gwaslab as gl In\u00a0[2]: Copied! <pre>sumstats = gl.Sumstats(\"1kgeas.B1.glm.firth\",fmt=\"plink2\")\n</pre> sumstats = gl.Sumstats(\"1kgeas.B1.glm.firth\",fmt=\"plink2\") <pre>Tue Dec 27 23:08:04 2022 Start to load format from formatbook....\nTue Dec 27 23:08:04 2022  -plink2 format meta info:\nTue Dec 27 23:08:04 2022   - format_name  : PLINK2 .glm.firth, .glm.logistic,.glm.linear\nTue Dec 27 23:08:04 2022   - format_source  : https://www.cog-genomics.org/plink/2.0/formats\nTue Dec 27 23:08:04 2022   - format_version  : Alpha 3.3 final (3 Jun)\nTue Dec 27 23:08:04 2022   - last_check_date  :  20220806\nTue Dec 27 23:08:04 2022  -plink2 format dictionary:\nTue Dec 27 23:08:04 2022   - plink2 keys: ID,#CHROM,POS,REF,ALT,A1,OBS_CT,A1_FREQ,BETA,LOG(OR)_SE,SE,Z_STAT,P,LOG10_P,MACH_R2,OR,L95,U95\nTue Dec 27 23:08:04 2022   - gwaslab values: SNPID,CHR,POS,REF,ALT,EA,N,EAF,BETA,SE,SE,Z,P,MLOG10P,INFO,OR,OR_95L,OR_95U\nTue Dec 27 23:08:04 2022 Start to initiate from file :1kgeas.B1.glm.firth\nTue Dec 27 23:08:05 2022  -Reading columns          : P,ID,OR,POS,REF,A1,OBS_CT,ALT,#CHROM,LOG(OR)_SE,Z_STAT\nTue Dec 27 23:08:05 2022  -Renaming columns to      : P,SNPID,OR,POS,REF,EA,N,ALT,CHR,SE,Z\nTue Dec 27 23:08:05 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:05 2022  -Initiating a status column: STATUS ...\nTue Dec 27 23:08:06 2022  -EA,REF and ALT columns are available: assigning NEA...\nTue Dec 27 23:08:06 2022 Start to reorder the columns...\nTue Dec 27 23:08:06 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:06 2022  -Reordering columns to    : SNPID,CHR,POS,EA,NEA,SE,Z,P,OR,N,STATUS\nTue Dec 27 23:08:06 2022 Finished sorting columns successfully!\nTue Dec 27 23:08:06 2022 Finished loading data successfully!\n</pre> In\u00a0[3]: Copied! <pre>sumstats.data\n</pre> sumstats.data Out[3]: SNPID CHR POS EA NEA SE Z P OR N STATUS 0 1:13273:G:C 1 13273 C G 0.282904 -1.035090 0.300628 0.746149 503 9999999 1 1:14599:T:A 1 14599 A T 0.240899 2.145980 0.031874 1.676930 503 9999999 2 1:14604:A:G 1 14604 G A 0.240899 2.145980 0.031874 1.676930 503 9999999 3 1:14930:A:G 1 14930 G A 0.242872 2.045850 0.040771 1.643590 503 9999999 4 1:69897:T:C 1 69897 T C 0.200238 2.624710 0.008672 1.691420 503 9999999 ... ... ... ... ... ... ... ... ... ... ... ... 1122294 22:51213613:C:T 22 51213613 T C 0.263823 -0.992284 0.321059 0.769674 503 9999999 1122295 22:51222965:G:A 22 51222965 A G 0.142966 -1.204630 0.228347 0.841793 503 9999999 1122296 22:51228910:G:A 22 51228910 A G 0.144489 -1.012670 0.311216 0.863881 503 9999999 1122297 22:51233666:C:T 22 51233666 T C 0.162193 1.159120 0.246406 1.206840 503 9999999 1122298 22:51234343:G:T 22 51234343 T G 0.162193 1.159120 0.246406 1.206840 503 9999999 <p>1122299 rows \u00d7 11 columns</p> In\u00a0[4]: Copied! <pre>sumstats.get_lead(sig_level=5e-6)\n</pre> sumstats.get_lead(sig_level=5e-6) <pre>Tue Dec 27 23:08:08 2022 Start to extract lead variants...\nTue Dec 27 23:08:08 2022  -Processing 1122299 variants...\nTue Dec 27 23:08:08 2022  -Significance threshold : 5e-06\nTue Dec 27 23:08:08 2022  -Sliding window size: 500  kb\nTue Dec 27 23:08:08 2022  -Found 114 significant variants in total...\nTue Dec 27 23:08:08 2022  -Identified 7 lead variants!\nTue Dec 27 23:08:08 2022 Finished extracting lead variants successfully!\n</pre> Out[4]: SNPID CHR POS EA NEA SE Z P OR N STATUS 71958 1:217437563:C:T 1 217437563 C T 0.151157 -5.22793 1.714210e-07 0.453736 503 9999999 110723 2:55574452:G:C 2 55574452 C G 0.160948 -5.98392 2.178320e-09 0.381707 503 9999999 248686 3:176524872:C:T 3 176524872 T C 0.248418 4.92774 8.318440e-07 3.401240 503 9999999 255388 3:193128900:G:A 3 193128900 A G 0.153788 4.70811 2.500290e-06 2.062770 503 9999999 424615 6:29919659:T:C 6 29919659 T C 0.155457 -5.89341 3.782970e-09 0.400048 503 9999999 635128 9:36660672:A:G 9 36660672 G A 0.160275 5.63422 1.758540e-08 2.467060 503 9999999 747517 11:56249438:A:G 11 56249438 G A 0.188891 -4.77836 1.767350e-06 0.405518 503 9999999 In\u00a0[5]: Copied! <pre>sumstats.plot_mqq(skip=2,anno=True)\n</pre> sumstats.plot_mqq(skip=2,anno=True) <pre>Tue Dec 27 23:08:11 2022 Start to plot manhattan/qq plot with the following basic settings:\nTue Dec 27 23:08:11 2022  -Genome-wide significance level is set to 5e-08 ...\nTue Dec 27 23:08:11 2022  -Raw input contains 1122299 variants...\nTue Dec 27 23:08:11 2022  -Plot layout mode is : mqq\nTue Dec 27 23:08:11 2022 Finished loading specified columns from the sumstats.\nTue Dec 27 23:08:11 2022 Start conversion and sanity check:\nTue Dec 27 23:08:11 2022  -Removed 0 variants with nan in CHR or POS column ...\nTue Dec 27 23:08:11 2022  -Removed 14 variants with nan in P column ...\nTue Dec 27 23:08:11 2022  -P values are being converted to -log10(P)...\nTue Dec 27 23:08:11 2022  -Sanity check after conversion: 0 variants with P value outside of (0,1] will be removed...\nTue Dec 27 23:08:11 2022  -Sanity check: 0 na/inf/-inf variants will be removed...\nTue Dec 27 23:08:11 2022  -Maximum -log10(P) values is 8.661878321078913 .\nTue Dec 27 23:08:11 2022 Finished data conversion and sanity check.\nTue Dec 27 23:08:12 2022 Start to create manhattan plot with 11806 variants:\nTue Dec 27 23:08:12 2022  -Found 3 significant variants with a sliding window size of 500 kb...\nTue Dec 27 23:08:12 2022 Finished creating Manhattan plot successfully!\nTue Dec 27 23:08:12 2022  -Annotating using column CHR:POS...\nTue Dec 27 23:08:12 2022 Start to create QQ plot with 11806 variants:\nTue Dec 27 23:08:12 2022  -Calculating lambda GC: 1.0136534522123153\nTue Dec 27 23:08:12 2022 Finished creating QQ plot successfully!\n</pre> Out[5]: <pre>(&lt;Figure size 3000x1000 with 2 Axes&gt;, &lt;gwaslab.Log.Log at 0x7f3b96d223d0&gt;)</pre> In\u00a0[6]: Copied! <pre>sumstats.basic_check()\n</pre> sumstats.basic_check() <pre>Tue Dec 27 23:08:13 2022 Start to check IDs...\nTue Dec 27 23:08:13 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:13 2022  -Checking if SNPID is chr:pos:ref:alt...(separator: - ,: , _)\nTue Dec 27 23:08:14 2022 Finished checking IDs successfully!\nTue Dec 27 23:08:14 2022 Start to fix chromosome notation...\nTue Dec 27 23:08:14 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:17 2022  -Vairants with standardized chromosome notation: 1122299\nTue Dec 27 23:08:19 2022  -All CHR are already fixed...\nTue Dec 27 23:08:21 2022 Finished fixing chromosome notation successfully!\nTue Dec 27 23:08:21 2022 Start to fix basepair positions...\nTue Dec 27 23:08:21 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:21 2022  -Converting to Int64 data type ...\nTue Dec 27 23:08:22 2022  -Position upper_bound is: 250,000,000\nTue Dec 27 23:08:24 2022  -Remove outliers: 0\nTue Dec 27 23:08:24 2022  -Converted all position to datatype Int64.\nTue Dec 27 23:08:24 2022 Finished fixing basepair position successfully!\nTue Dec 27 23:08:24 2022 Start to fix alleles...\nTue Dec 27 23:08:24 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:25 2022  -Detected 0 variants with alleles that contain bases other than A/C/T/G .\nTue Dec 27 23:08:25 2022  -Converted all bases to string datatype and UPPERCASE.\nTue Dec 27 23:08:27 2022 Finished fixing allele successfully!\nTue Dec 27 23:08:27 2022 Start sanity check for statistics ...\nTue Dec 27 23:08:27 2022  -Current Dataframe shape : 1122299  x  11\nTue Dec 27 23:08:27 2022  -Checking if  0 &lt;=N&lt;= inf  ...\nTue Dec 27 23:08:27 2022  -Removed 0 variants with bad N.\nTue Dec 27 23:08:27 2022  -Checking if  -37.5 &lt;Z&lt; 37.5  ...\nTue Dec 27 23:08:27 2022  -Removed 14 variants with bad Z.\nTue Dec 27 23:08:27 2022  -Checking if  5e-300 &lt;= P &lt;= 1  ...\nTue Dec 27 23:08:27 2022  -Removed 0 variants with bad P.\nTue Dec 27 23:08:27 2022  -Checking if  0 &lt;SE&lt; inf  ...\nTue Dec 27 23:08:27 2022  -Removed 0 variants with bad SE.\nTue Dec 27 23:08:27 2022  -Checking if  -10 &lt;log(OR)&lt; 10  ...\nTue Dec 27 23:08:27 2022  -Removed 0 variants with bad OR.\nTue Dec 27 23:08:27 2022  -Checking STATUS...\nTue Dec 27 23:08:28 2022  -Coverting STAUTUS to interger.\nTue Dec 27 23:08:28 2022  -Removed 14 variants with bad statistics in total.\nTue Dec 27 23:08:28 2022 Finished sanity check successfully!\nTue Dec 27 23:08:28 2022 Start to normalize variants...\nTue Dec 27 23:08:28 2022  -Current Dataframe shape : 1122285  x  11\nTue Dec 27 23:08:29 2022  -No available variants to normalize..\nTue Dec 27 23:08:29 2022 Finished normalizing variants successfully!\n</pre> In\u00a0[7]: Copied! <pre>sumstats.plot_mqq(mode=\"r\",anno=True,region=(2,54531536,56731536),region_grid=True)\n#2:\n</pre> sumstats.plot_mqq(mode=\"r\",anno=True,region=(2,54531536,56731536),region_grid=True) #2: <pre>Tue Dec 27 23:08:29 2022 Start to plot manhattan/qq plot with the following basic settings:\nTue Dec 27 23:08:29 2022  -Genome-wide significance level is set to 5e-08 ...\nTue Dec 27 23:08:29 2022  -Raw input contains 1122285 variants...\nTue Dec 27 23:08:29 2022  -Plot layout mode is : r\nTue Dec 27 23:08:29 2022  -Region to plot : chr2:54531536-56731536.\nTue Dec 27 23:08:29 2022  -Extract SNPs in region : chr2:54531536-56731536...\nTue Dec 27 23:08:31 2022  -Extract SNPs in specified regions: 898\nTue Dec 27 23:08:31 2022 Finished loading specified columns from the sumstats.\nTue Dec 27 23:08:31 2022 Start conversion and sanity check:\nTue Dec 27 23:08:31 2022  -Removed 0 variants with nan in CHR or POS column ...\nTue Dec 27 23:08:31 2022  -Removed 0 variants with nan in P column ...\nTue Dec 27 23:08:31 2022  -P values are being converted to -log10(P)...\nTue Dec 27 23:08:31 2022  -Sanity check after conversion: 0 variants with P value outside of (0,1] will be removed...\nTue Dec 27 23:08:31 2022  -Sanity check: 0 na/inf/-inf variants will be removed...\nTue Dec 27 23:08:31 2022  -Maximum -log10(P) values is 8.661878321078913 .\nTue Dec 27 23:08:31 2022 Finished data conversion and sanity check.\nTue Dec 27 23:08:31 2022 Start to create manhattan plot with 898 variants:\nTue Dec 27 23:08:31 2022  -Loading gtf files from:default\nTue Dec 27 23:08:45 2022  -plotting gene track..\nTue Dec 27 23:08:45 2022  -Finished plotting gene track..\nTue Dec 27 23:08:45 2022  -Found 1 significant variants with a sliding window size of 500 kb...\nTue Dec 27 23:08:45 2022 Finished creating Manhattan plot successfully!\nTue Dec 27 23:08:45 2022  -Annotating using column CHR:POS...\n</pre> Out[7]: <pre>(&lt;Figure size 3000x2000 with 3 Axes&gt;, &lt;gwaslab.Log.Log at 0x7f3b96d223d0&gt;)</pre> In\u00a0[8]: Copied! <pre>gl.download_ref(\"1kg_eas_hg19\")\n</pre> gl.download_ref(\"1kg_eas_hg19\") <pre>Tue Dec 27 22:44:52 2022 Start to download  1kg_eas_hg19  ...\nTue Dec 27 22:44:52 2022  -Downloading to: /home/he/anaconda3/envs/py38/lib/python3.8/site-packages/gwaslab/data/EAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz\nTue Dec 27 22:52:33 2022  -Updating record in config file...\nTue Dec 27 22:52:35 2022  -Updating record in config file...\nTue Dec 27 22:52:35 2022  -Downloading to: /home/he/anaconda3/envs/py38/lib/python3.8/site-packages/gwaslab/data/EAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi\nTue Dec 27 22:52:35 2022 Downloaded  1kg_eas_hg19  successfully!\n</pre> In\u00a0[8]: Copied! <pre>sumstats.plot_mqq(mode=\"r\",anno=True,region=(2,54531536,56731536),region_grid=True,vcf_path=gl.get_path(\"1kg_eas_hg19\"))\n</pre> sumstats.plot_mqq(mode=\"r\",anno=True,region=(2,54531536,56731536),region_grid=True,vcf_path=gl.get_path(\"1kg_eas_hg19\")) <pre>Tue Dec 27 23:08:48 2022 Start to plot manhattan/qq plot with the following basic settings:\nTue Dec 27 23:08:48 2022  -Genome-wide significance level is set to 5e-08 ...\nTue Dec 27 23:08:48 2022  -Raw input contains 1122285 variants...\nTue Dec 27 23:08:48 2022  -Plot layout mode is : r\nTue Dec 27 23:08:48 2022  -Region to plot : chr2:54531536-56731536.\nTue Dec 27 23:08:48 2022  -Extract SNPs in region : chr2:54531536-56731536...\nTue Dec 27 23:08:50 2022  -Extract SNPs in specified regions: 898\nTue Dec 27 23:08:50 2022 Finished loading specified columns from the sumstats.\nTue Dec 27 23:08:50 2022 Start conversion and sanity check:\nTue Dec 27 23:08:50 2022  -Removed 0 variants with nan in CHR or POS column ...\nTue Dec 27 23:08:50 2022  -Removed 0 variants with nan in P column ...\nTue Dec 27 23:08:50 2022  -P values are being converted to -log10(P)...\nTue Dec 27 23:08:50 2022  -Sanity check after conversion: 0 variants with P value outside of (0,1] will be removed...\nTue Dec 27 23:08:50 2022  -Sanity check: 0 na/inf/-inf variants will be removed...\nTue Dec 27 23:08:50 2022  -Maximum -log10(P) values is 8.661878321078913 .\nTue Dec 27 23:08:50 2022 Finished data conversion and sanity check.\nTue Dec 27 23:08:50 2022 Start to load reference genotype...\nTue Dec 27 23:08:50 2022  -reference vcf path : /home/he/anaconda3/envs/py38/lib/python3.8/site-packages/gwaslab/data/EAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz\nTue Dec 27 23:08:53 2022  -Retrieving index...\nTue Dec 27 23:08:53 2022  -Ref variants in the region: 71908\nTue Dec 27 23:08:53 2022  -Calculating Rsq...\nTue Dec 27 23:08:53 2022 Finished loading reference genotype successfully!\nTue Dec 27 23:08:53 2022 Start to create manhattan plot with 898 variants:\nTue Dec 27 23:08:54 2022  -Loading gtf files from:default\nTue Dec 27 23:09:06 2022  -plotting gene track..\nTue Dec 27 23:09:07 2022  -Finished plotting gene track..\nTue Dec 27 23:09:07 2022  -Found 1 significant variants with a sliding window size of 500 kb...\nTue Dec 27 23:09:07 2022 Finished creating Manhattan plot successfully!\nTue Dec 27 23:09:07 2022  -Annotating using column CHR:POS...\n</pre> Out[8]: <pre>(&lt;Figure size 3000x2000 with 4 Axes&gt;, &lt;gwaslab.Log.Log at 0x7f3b96d223d0&gt;)</pre>"},{"location":"Visualization/#visualization-by-gwaslab","title":"Visualization by gwaslab\u00b6","text":""},{"location":"Visualization/#import-gwaslab-package","title":"Import gwaslab package\u00b6","text":""},{"location":"Visualization/#load-sumstats","title":"Load sumstats\u00b6","text":""},{"location":"Visualization/#check-the-lead-variants-in-significant-loci","title":"Check the lead variants in significant loci\u00b6","text":""},{"location":"Visualization/#create-mahattan-plot","title":"Create mahattan plot\u00b6","text":""},{"location":"Visualization/#qc-check","title":"QC check\u00b6","text":""},{"location":"Visualization/#create-regional-plot","title":"Create regional plot\u00b6","text":""},{"location":"Visualization/#create-regional-plot-with-ld-information","title":"Create regional plot with LD information\u00b6","text":""},{"location":"finemapping_susie/","title":"Finemapping using susieR","text":"In\u00a0[1]: Copied! <pre>import gwaslab as gl\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</pre> import gwaslab as gl import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>sumstats = gl.Sumstats(\"../06_Association_tests/1kgeas.B1.glm.firth\",fmt=\"plink2\")\n</pre> sumstats = gl.Sumstats(\"../06_Association_tests/1kgeas.B1.glm.firth\",fmt=\"plink2\") <pre>Mon Jan 16 12:24:25 2023 Start to load format from formatbook....\nMon Jan 16 12:24:25 2023  -plink2 format meta info:\nMon Jan 16 12:24:25 2023   - format_name  : PLINK2 .glm.firth, .glm.logistic,.glm.linear\nMon Jan 16 12:24:25 2023   - format_source  : https://www.cog-genomics.org/plink/2.0/formats\nMon Jan 16 12:24:25 2023   - format_version  : Alpha 3.3 final (3 Jun)\nMon Jan 16 12:24:25 2023   - last_check_date  :  20220806\nMon Jan 16 12:24:25 2023  -plink2 format dictionary:\nMon Jan 16 12:24:25 2023   - plink2 keys: ID,#CHROM,POS,REF,ALT,A1,OBS_CT,A1_FREQ,BETA,LOG(OR)_SE,SE,Z_STAT,P,LOG10_P,MACH_R2,OR,L95,U95\nMon Jan 16 12:24:25 2023   - gwaslab values: SNPID,CHR,POS,REF,ALT,EA,N,EAF,BETA,SE,SE,Z,P,MLOG10P,INFO,OR,OR_95L,OR_95U\nMon Jan 16 12:24:25 2023 Start to initiate from file :../06_Association_tests/1kgeas.B1.glm.firth\nMon Jan 16 12:24:27 2023  -Reading columns          : REF,ID,POS,#CHROM,A1,P,LOG(OR)_SE,OR,ALT,Z_STAT,OBS_CT\nMon Jan 16 12:24:27 2023  -Renaming columns to      : REF,SNPID,POS,CHR,EA,P,SE,OR,ALT,Z,N\nMon Jan 16 12:24:27 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:27 2023  -Initiating a status column: STATUS ...\nMon Jan 16 12:24:28 2023  -EA,REF and ALT columns are available: assigning NEA...\nMon Jan 16 12:24:29 2023 Start to reorder the columns...\nMon Jan 16 12:24:29 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:29 2023  -Reordering columns to    : SNPID,CHR,POS,EA,NEA,SE,Z,P,OR,N,STATUS\nMon Jan 16 12:24:29 2023 Finished sorting columns successfully!\nMon Jan 16 12:24:29 2023 Finished loading data successfully!\n</pre> In\u00a0[3]: Copied! <pre>sumstats.basic_check()\n</pre> sumstats.basic_check() <pre>Mon Jan 16 12:24:29 2023 Start to check IDs...\nMon Jan 16 12:24:29 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:29 2023  -Checking if SNPID is chr:pos:ref:alt...(separator: - ,: , _)\nMon Jan 16 12:24:31 2023 Finished checking IDs successfully!\nMon Jan 16 12:24:31 2023 Start to fix chromosome notation...\nMon Jan 16 12:24:31 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:35 2023  -Vairants with standardized chromosome notation: 1122299\nMon Jan 16 12:24:38 2023  -All CHR are already fixed...\nMon Jan 16 12:24:40 2023 Finished fixing chromosome notation successfully!\nMon Jan 16 12:24:40 2023 Start to fix basepair positions...\nMon Jan 16 12:24:40 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:40 2023  -Converting to Int64 data type ...\nMon Jan 16 12:24:42 2023  -Position upper_bound is: 250,000,000\nMon Jan 16 12:24:45 2023  -Remove outliers: 0\nMon Jan 16 12:24:46 2023  -Converted all position to datatype Int64.\nMon Jan 16 12:24:46 2023 Finished fixing basepair position successfully!\nMon Jan 16 12:24:46 2023 Start to fix alleles...\nMon Jan 16 12:24:46 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:46 2023  -Detected 0 variants with alleles that contain bases other than A/C/T/G .\nMon Jan 16 12:24:46 2023  -Converted all bases to string datatype and UPPERCASE.\nMon Jan 16 12:24:49 2023 Finished fixing allele successfully!\nMon Jan 16 12:24:49 2023 Start sanity check for statistics ...\nMon Jan 16 12:24:49 2023  -Current Dataframe shape : 1122299  x  11\nMon Jan 16 12:24:49 2023  -Checking if  0 &lt;=N&lt;= inf  ...\nMon Jan 16 12:24:49 2023  -Removed 0 variants with bad N.\nMon Jan 16 12:24:49 2023  -Checking if  -37.5 &lt;Z&lt; 37.5  ...\nMon Jan 16 12:24:49 2023  -Removed 14 variants with bad Z.\nMon Jan 16 12:24:49 2023  -Checking if  5e-300 &lt;= P &lt;= 1  ...\nMon Jan 16 12:24:49 2023  -Removed 0 variants with bad P.\nMon Jan 16 12:24:49 2023  -Checking if  0 &lt;SE&lt; inf  ...\nMon Jan 16 12:24:49 2023  -Removed 0 variants with bad SE.\nMon Jan 16 12:24:49 2023  -Checking if  -10 &lt;log(OR)&lt; 10  ...\nMon Jan 16 12:24:49 2023  -Removed 0 variants with bad OR.\nMon Jan 16 12:24:49 2023  -Checking STATUS...\nMon Jan 16 12:24:50 2023  -Coverting STAUTUS to interger.\nMon Jan 16 12:24:50 2023  -Removed 14 variants with bad statistics in total.\nMon Jan 16 12:24:50 2023 Finished sanity check successfully!\nMon Jan 16 12:24:50 2023 Start to normalize variants...\nMon Jan 16 12:24:50 2023  -Current Dataframe shape : 1122285  x  11\nMon Jan 16 12:24:51 2023  -No available variants to normalize..\nMon Jan 16 12:24:51 2023 Finished normalizing variants successfully!\n</pre> In\u00a0[4]: Copied! <pre>sumstats.get_lead()\n</pre> sumstats.get_lead() <pre>Mon Jan 16 12:24:51 2023 Start to extract lead variants...\nMon Jan 16 12:24:51 2023  -Processing 1122285 variants...\nMon Jan 16 12:24:51 2023  -Significance threshold : 5e-08\nMon Jan 16 12:24:51 2023  -Sliding window size: 500  kb\nMon Jan 16 12:24:52 2023  -Found 59 significant variants in total...\nMon Jan 16 12:24:52 2023  -Identified 3 lead variants!\nMon Jan 16 12:24:52 2023 Finished extracting lead variants successfully!\n</pre> Out[4]: SNPID CHR POS EA NEA SE Z P OR N STATUS 110723 2:55574452:G:C 2 55574452 C G 0.160813 -5.97102 2.357800e-09 0.382809 503 9960099 424615 6:29919659:T:C 6 29919659 T C 0.155341 -5.88600 3.956520e-09 0.400783 503 9960099 635128 9:36660672:A:G 9 36660672 G A 0.160260 5.63617 1.738760e-08 2.467620 503 9960099 In\u00a0[5]: Copied! <pre>sumstats.plot_mqq()\n</pre> sumstats.plot_mqq() <pre>Mon Jan 16 12:24:52 2023 Start to plot manhattan/qq plot with the following basic settings:\nMon Jan 16 12:24:52 2023  -Genome-wide significance level is set to 5e-08 ...\nMon Jan 16 12:24:52 2023  -Raw input contains 1122285 variants...\nMon Jan 16 12:24:52 2023  -Plot layout mode is : mqq\nMon Jan 16 12:24:52 2023 Finished loading specified columns from the sumstats.\nMon Jan 16 12:24:52 2023 Start conversion and sanity check:\nMon Jan 16 12:24:52 2023  -Removed 0 variants with nan in CHR or POS column ...\nMon Jan 16 12:24:53 2023  -Removed 0 variants with nan in P column ...\nMon Jan 16 12:24:53 2023  -P values are being converted to -log10(P)...\nMon Jan 16 12:24:53 2023  -Sanity check after conversion: 0 variants with P value outside of (0,1] will be removed...\nMon Jan 16 12:24:53 2023  -Sanity check: 0 na/inf/-inf variants will be removed...\nMon Jan 16 12:24:53 2023  -Maximum -log10(P) values is 8.627493036637102 .\nMon Jan 16 12:24:53 2023 Finished data conversion and sanity check.\nMon Jan 16 12:24:55 2023 Start to create manhattan plot with 1122285 variants:\nMon Jan 16 12:25:00 2023  -Found 3 significant variants with a sliding window size of 500 kb...\nMon Jan 16 12:25:00 2023 Finished creating Manhattan plot successfully!\nMon Jan 16 12:25:00 2023  -Skip annotating\nMon Jan 16 12:25:00 2023 Start to create QQ plot with 1122285 variants:\nMon Jan 16 12:25:01 2023  -Calculating lambda GC: 1.011958203677465\nMon Jan 16 12:25:01 2023 Finished creating QQ plot successfully!\n</pre> Out[5]: <pre>(&lt;Figure size 3000x1000 with 2 Axes&gt;, &lt;gwaslab.Log.Log at 0x7fcc2f677310&gt;)</pre> In\u00a0[6]: Copied! <pre>locus = sumstats.filter_value('CHR==2 &amp; POS&gt;55074452 &amp; POS&lt;56074452')\n</pre> locus = sumstats.filter_value('CHR==2 &amp; POS&gt;55074452 &amp; POS&lt;56074452') <pre>Mon Jan 16 12:25:18 2023 Start filtering values by condition: CHR==2 &amp; POS&gt;55074452 &amp; POS&lt;56074452\nMon Jan 16 12:25:18 2023  -Removing 1121871 variants not meeting the conditions: CHR==2 &amp; POS&gt;55074452 &amp; POS&lt;56074452\nMon Jan 16 12:25:18 2023 Finished filtering values.\n</pre> In\u00a0[7]: Copied! <pre>locus.fill_data(to_fill=[\"BETA\"])\n</pre> locus.fill_data(to_fill=[\"BETA\"]) <pre>Mon Jan 16 12:25:18 2023 Start filling data using existing columns...\nMon Jan 16 12:25:18 2023  -Raw input columns:  ['SNPID', 'CHR', 'POS', 'EA', 'NEA', 'SE', 'Z', 'P', 'OR', 'N', 'STATUS']\nMon Jan 16 12:25:18 2023  -Overwrite mode:  False\nMon Jan 16 12:25:18 2023   -Skipping columns:  []\nMon Jan 16 12:25:18 2023  -Filling columns:  ['BETA']\nMon Jan 16 12:25:18 2023   - Filling BETA value using OR column...\nMon Jan 16 12:25:18 2023 Start to reorder the columns...\nMon Jan 16 12:25:18 2023  -Current Dataframe shape : 414  x  12\nMon Jan 16 12:25:18 2023  -Reordering columns to    : SNPID,CHR,POS,EA,NEA,BETA,SE,Z,P,OR,N,STATUS\nMon Jan 16 12:25:18 2023 Finished sorting columns successfully!\nMon Jan 16 12:25:18 2023 Finished filling data using existing columns.\n</pre> In\u00a0[8]: Copied! <pre>locus.data\n</pre> locus.data Out[8]: SNPID CHR POS EA NEA BETA SE Z P OR N STATUS 110525 2:55079388:C:T 2 55079388 T C 0.199523 0.136600 1.460650 0.144112 1.220820 503 9960099 110526 2:55079654:A:T 2 55079654 A T 0.126765 0.134100 0.945302 0.344505 1.135150 503 9960099 110527 2:55080703:A:C 2 55080703 C A 0.073548 0.146541 0.501892 0.615744 1.076320 503 9960099 110528 2:55086992:G:A 2 55086992 A G -0.216742 0.310447 -0.698158 0.485078 0.805138 503 9960099 110529 2:55087887:C:T 2 55087887 T C -0.059573 0.156956 -0.379552 0.704278 0.942167 503 9960099 ... ... ... ... ... ... ... ... ... ... ... ... ... 110934 2:56046438:G:A 2 56046438 G A 0.312831 0.211863 1.476560 0.139794 1.367290 503 9960099 110935 2:56047038:G:A 2 56047038 G A 0.209061 0.171011 1.222500 0.221519 1.232520 503 9960099 110936 2:56048107:G:C 2 56048107 G C 0.240677 0.169593 1.419120 0.155864 1.272110 503 9960099 110937 2:56074410:C:A 2 56074410 C A 0.238922 0.203308 1.175160 0.239932 1.269880 503 9960099 110938 2:56074415:C:A 2 56074415 C A 0.385188 0.214410 1.796500 0.072415 1.469890 503 9960099 <p>414 rows \u00d7 12 columns</p> In\u00a0[43]: Copied! <pre>locus.harmonize(basic_check=False, ref_seq=\"/Users/he/mydata/Reference/Genome/human_g1k_v37.fasta\")\n</pre> locus.harmonize(basic_check=False, ref_seq=\"/Users/he/mydata/Reference/Genome/human_g1k_v37.fasta\") <pre>Mon Jan 16 12:41:05 2023 Start to check if NEA is aligned with reference sequence...\nMon Jan 16 12:41:05 2023  -Current Dataframe shape : 414  x  12\nMon Jan 16 12:41:05 2023  -Reference genome fasta file: /Users/he/mydata/Reference/Genome/human_g1k_v37.fasta\nMon Jan 16 12:41:05 2023  -Checking records: 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  X  Y  MT  \nMon Jan 16 12:41:36 2023  -Variants allele on given reference sequence :  237\nMon Jan 16 12:41:36 2023  -Variants flipped :  177\nMon Jan 16 12:41:36 2023   -Raw Matching rate :  100.00%\nMon Jan 16 12:41:36 2023  -Variants inferred reverse_complement :  0\nMon Jan 16 12:41:36 2023  -Variants inferred reverse_complement_flipped :  0\nMon Jan 16 12:41:36 2023  -Both allele on genome + unable to distinguish :  0\nMon Jan 16 12:41:36 2023  -Variants not on given reference sequence :  0\nMon Jan 16 12:41:37 2023  -Current Dataframe shape : 414  x  12\nMon Jan 16 12:41:37 2023 Start to flip allele-specific stats for SNPs with status xxxxx[35]x: alt-&gt;ea , ref-&gt;nea ... \nMon Jan 16 12:41:37 2023  -Flipping 177 variants...\nMon Jan 16 12:41:37 2023  -Swapping column: NEA &lt;=&gt; EA...\nMon Jan 16 12:41:37 2023  -Flipping column: BETA = - BETA...\nMon Jan 16 12:41:37 2023  -Flipping column: OR = 1 / OR...\nMon Jan 16 12:41:37 2023  -Changed the status for flipped variants : xxxxx[35]x -&gt; xxxxx[12]x\nMon Jan 16 12:41:37 2023 Finished converting successfully!\nMon Jan 16 12:41:37 2023 Start to sort the genome coordinates...\nMon Jan 16 12:41:37 2023  -Current Dataframe shape : 414  x  12\nMon Jan 16 12:41:37 2023  -Sorting genome coordinates...\nMon Jan 16 12:41:37 2023 Finished sorting genome coordinates successfully!\nMon Jan 16 12:41:37 2023 Start to reorder the columns...\nMon Jan 16 12:41:37 2023  -Current Dataframe shape : 414  x  12\nMon Jan 16 12:41:37 2023  -Reordering columns to    : SNPID,CHR,POS,EA,NEA,BETA,SE,Z,P,OR,N,STATUS\nMon Jan 16 12:41:37 2023 Finished sorting columns successfully!\n</pre> Out[43]: <pre>&lt;gwaslab.Sumstats.Sumstats at 0x7fcbdb6194f0&gt;</pre> In\u00a0[44]: Copied! <pre>locus.data.to_csv(\"sig_locus.tsv\",sep=\"\\t\",index=None)\nlocus.data[\"SNPID\"].to_csv(\"sig_locus.snplist\",sep=\"\\t\",index=None,header=None)\n</pre> locus.data.to_csv(\"sig_locus.tsv\",sep=\"\\t\",index=None) locus.data[\"SNPID\"].to_csv(\"sig_locus.snplist\",sep=\"\\t\",index=None,header=None) In\u00a0[10]: Copied! <pre>import rpy2\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects.numpy2ri as numpy2ri\nnumpy2ri.activate()\n</pre> import rpy2 import rpy2.robjects as ro from rpy2.robjects.packages import importr import rpy2.robjects.numpy2ri as numpy2ri numpy2ri.activate() <pre>INFO:rpy2.situation:cffi mode is CFFI_MODE.ANY\nINFO:rpy2.situation:R home found: /Library/Frameworks/R.framework/Resources\nINFO:rpy2.situation:R library path: \nINFO:rpy2.situation:LD_LIBRARY_PATH: \nINFO:rpy2.situation:R exec path: /Library/Frameworks/R.framework/Resources/bin/R\nINFO:rpy2.rinterface_lib.embedded:Default options to initialize R: rpy2, --quiet, --no-save\nINFO:rpy2.rinterface_lib.embedded:R is already initialized. No need to initialize.\n</pre> In\u00a0[45]: Copied! <pre>df = pd.read_csv(\"sig_locus.tsv\",sep=\"\\t\")\ndf\n</pre> df = pd.read_csv(\"sig_locus.tsv\",sep=\"\\t\") df  Out[45]: SNPID CHR POS EA NEA BETA SE Z P OR N STATUS 0 2:55079388:C:T 2 55079388 T C 0.199523 0.136600 1.460650 0.144112 1.220820 503 9960009 1 2:55079654:A:T 2 55079654 T A -0.126765 0.134100 0.945302 0.344505 0.880941 503 9960019 2 2:55080703:A:C 2 55080703 C A 0.073548 0.146541 0.501892 0.615744 1.076320 503 9960009 3 2:55086992:G:A 2 55086992 A G -0.216742 0.310447 -0.698158 0.485078 0.805138 503 9960009 4 2:55087887:C:T 2 55087887 T C -0.059573 0.156956 -0.379552 0.704278 0.942167 503 9960009 ... ... ... ... ... ... ... ... ... ... ... ... ... 409 2:56046438:G:A 2 56046438 A G -0.312831 0.211863 1.476560 0.139794 0.731374 503 9960019 410 2:56047038:G:A 2 56047038 A G -0.209061 0.171011 1.222500 0.221519 0.811346 503 9960019 411 2:56048107:G:C 2 56048107 C G -0.240677 0.169593 1.419120 0.155864 0.786096 503 9960019 412 2:56074410:C:A 2 56074410 A C -0.238922 0.203308 1.175160 0.239932 0.787476 503 9960019 413 2:56074415:C:A 2 56074415 A C -0.385188 0.214410 1.796500 0.072415 0.680323 503 9960019 <p>414 rows \u00d7 12 columns</p> In\u00a0[46]: Copied! <pre># import susieR as object\nsusieR = importr('susieR')\n</pre> # import susieR as object susieR = importr('susieR') In\u00a0[65]: Copied! <pre># convert pd.DataFrame to numpy\nld = pd.read_csv(\"sig_locus_mt.ld\",sep=\"\\t\",header=None)\nR_df = ld.values\nld2 = pd.read_csv(\"sig_locus_mt_r2.ld\",sep=\"\\t\",header=None)\nR_df2 = ld2.values\n</pre> # convert pd.DataFrame to numpy ld = pd.read_csv(\"sig_locus_mt.ld\",sep=\"\\t\",header=None) R_df = ld.values ld2 = pd.read_csv(\"sig_locus_mt_r2.ld\",sep=\"\\t\",header=None) R_df2 = ld2.values In\u00a0[66]: Copied! <pre>R_df\n</pre> R_df Out[66]: <pre>array([[ 1.        , -0.766746  ,  0.850204  , ...,  0.016145  ,\n        -0.0543754 , -0.0664124 ],\n       [-0.766746  ,  1.        , -0.646211  , ..., -0.00602064,\n         0.0680547 ,  0.0639558 ],\n       [ 0.850204  , -0.646211  ,  1.        , ...,  0.034304  ,\n        -0.0402253 , -0.0526333 ],\n       ...,\n       [ 0.016145  , -0.00602064,  0.034304  , ...,  1.        ,\n         0.536946  ,  0.586261  ],\n       [-0.0543754 ,  0.0680547 , -0.0402253 , ...,  0.536946  ,\n         1.        ,  0.91945   ],\n       [-0.0664124 ,  0.0639558 , -0.0526333 , ...,  0.586261  ,\n         0.91945   ,  1.        ]])</pre> In\u00a0[67]: Copied! <pre>plt.figure(figsize=(10,10),dpi=200)\nfig, ax = plt.subplots(ncols=2,figsize=(20,10))\nsns.heatmap(data=R_df,cmap=\"Spectral\",ax=ax[0])\nsns.heatmap(data=R_df2,ax=ax[1])\nax[0].set_title(\"LD r matrix\")\nax[1].set_title(\"LD r2 matrix\")\n</pre> plt.figure(figsize=(10,10),dpi=200) fig, ax = plt.subplots(ncols=2,figsize=(20,10)) sns.heatmap(data=R_df,cmap=\"Spectral\",ax=ax[0]) sns.heatmap(data=R_df2,ax=ax[1]) ax[0].set_title(\"LD r matrix\") ax[1].set_title(\"LD r2 matrix\") Out[67]: <pre>Text(0.5, 1.0, 'LD r2 matrix')</pre> <pre>&lt;Figure size 2000x2000 with 0 Axes&gt;</pre> <p>https://stephenslab.github.io/susieR/articles/finemapping_summary_statistics.html#fine-mapping-with-susier-using-summary-statistics</p> In\u00a0[52]: Copied! <pre>ro.r('set.seed(2932)')\nfit = susieR.susie_rss(\n    bhat = df[\"BETA\"].values.reshape((len(R_df),1)),\n    shat = df[\"SE\"].values.reshape((len(R_df),1)),\n    R = R_df,\n    L = 10,\n    n = 503\n)\n</pre> ro.r('set.seed(2932)') fit = susieR.susie_rss(     bhat = df[\"BETA\"].values.reshape((len(R_df),1)),     shat = df[\"SE\"].values.reshape((len(R_df),1)),     R = R_df,     L = 10,     n = 503 ) In\u00a0[53]: Copied! <pre>fit\n</pre> fit Out[53]: ListVector with 18 elements.              alpha                           [[2.09031576e-08 1.14917130e-08 8.42180573e-09 ... 1.97346018e-08   1.45441089e-08 3.53510267e-08]  [2.41545894e-03 2.41545894e-03 2.41545894e-03 ... 2.41545894e-03   2.41545894e-03 2.41545894e-03]  [2.41545894e-03 2.41545894e-03 2.41545894e-03 ... 2.41545894e-03   2.41545894e-03 2.41545894e-03]  ...  [2.41545894e-03 2.41545894e-03 2.41545894e-03 ... 2.41545894e-03   2.41545894e-03 2.41545894e-03]  [2.41545894e-03 2.41545894e-03 2.41545894e-03 ... 2.41545894e-03   2.41545894e-03 2.41545894e-03]  [2.41545894e-03 2.41545894e-03 2.41545894e-03 ... 2.41545894e-03   2.41545894e-03 2.41545894e-03]]                           mu                           [[ 0.06309725 -0.04088597  0.02172167 ... -0.06131219 -0.05080381   -0.07752188]  [ 0.         -0.         -0.         ... -0.         -0.   -0.        ]  [ 0.         -0.         -0.         ... -0.         -0.   -0.        ]  ...  [ 0.         -0.         -0.         ... -0.         -0.   -0.        ]  [ 0.         -0.         -0.         ... -0.         -0.   -0.        ]  [ 0.         -0.         -0.         ... -0.         -0.   -0.        ]]                           mu2                           [[0.00591148 0.00360188 0.00240205 ... 0.0056894  0.00451125 0.00793986]  [0.         0.         0.         ... 0.         0.         0.        ]  [0.         0.         0.         ... 0.         0.         0.        ]  ...  [0.         0.         0.         ... 0.         0.         0.        ]  [0.         0.         0.         ... 0.         0.         0.        ]  [0.         0.         0.         ... 0.         0.         0.        ]]                           ...                           ...                           intercept                           [nan]                           sets              ListVector with 5 elements.              cs               [RTYPES.VECSXP]                           purity               [RTYPES.VECSXP]                           cs_index               [RTYPES.INTSXP]                           coverage               [RTYPES.REALSXP]                           requested_coverage               [RTYPES.REALSXP]                           pip                           [2.09031575e-08 1.14917130e-08 8.42180570e-09 9.44036416e-09  7.99259614e-09 7.82761989e-09 7.99259614e-09 7.60946095e-09  9.67108293e-09 1.04038267e-08 7.96309008e-09 7.76430453e-09  8.36936176e-09 7.50647877e-09 8.73110317e-09 7.60175667e-09  1.01546580e-08 7.56616325e-09 7.71511066e-09 1.07942909e-08  1.26330332e-08 7.97594291e-09 7.45979867e-09 1.30970170e-08  7.46701023e-09 7.83804932e-09 9.34796740e-09 7.54673457e-09  1.88440308e-08 1.53151037e-08 7.73068876e-09 7.92250443e-09  9.46189005e-09 9.11702858e-09 7.45895190e-09 8.94437802e-09  7.62546359e-09 7.56105401e-09 7.56714669e-09 8.46597004e-09  9.99148286e-09 7.47299889e-09 1.69038357e-08 1.02058041e-08  1.00159782e-08 8.27364843e-09 1.03048376e-08 9.90422766e-09  8.36507497e-09 7.51431339e-09 7.62189023e-09 2.35054238e-08  7.45601891e-09 7.52632778e-09 7.62567443e-09 7.60398222e-09  2.35054238e-08 2.35054238e-08 1.15541452e-08 7.45763573e-09  1.38177028e-08 1.39423916e-08 7.69093889e-09 7.75432651e-09  1.34753771e-08 1.27900914e-08 9.91085158e-09 9.42588507e-09  7.86883614e-09 1.06865728e-08 1.10165844e-08 3.03261281e-08  7.69979580e-09 7.70838060e-09 1.08052649e-08 7.80567844e-09  7.81748444e-09 1.42343403e-08 7.80567844e-09 7.73845910e-09  8.06207379e-09 1.06285312e-08 8.49009518e-09 7.67162622e-09  7.75772591e-09 1.13849075e-08 1.04807192e-08 9.82374915e-09  2.46783600e-08 1.05266895e-08 7.47087914e-09 1.58215204e-08  1.67008889e-08 7.45417039e-09 7.46643858e-09 7.48290085e-09  7.81491294e-09 7.45752360e-09 7.51961071e-09 7.46156803e-09  1.62131901e-08 1.47665666e-08 7.47263984e-09 1.41537722e-08  2.20997779e-08 1.41537722e-08 7.73217224e-09 9.30909783e-09  7.48321916e-09 7.71818576e-09 1.14764277e-08 1.44414303e-08  1.82531505e-08 7.92305588e-09 1.25367983e-08 4.60026702e-08  8.57377402e-09 7.85984455e-09 2.64984137e-08 1.88249148e-08  9.54209978e-09 7.81151299e-09 4.21824182e-08 3.86133887e-08  1.27432430e-08 1.95360467e-08 2.65250873e-08 8.83275342e-09  8.51156001e-09 8.51156001e-09 1.50101219e-08 8.34762637e-09  7.89815358e-09 8.34762637e-09 2.45558158e-08 3.11809560e-08  4.25112319e-08 4.68950988e-08 1.26244293e-08 1.26244293e-08  1.17859971e-08 1.17859971e-08 1.17859971e-08 1.17859971e-08  3.24974035e-07 1.35901978e-06 8.05918032e-09 3.48543503e-07  2.73603853e-07 7.61463126e-09 8.39069492e-09 9.25320593e-07  2.23811632e-06 3.10686932e-07 5.75885050e-08 8.03355837e-07  7.89365251e-09 3.90455674e-06 7.58969454e-09 3.41527936e-07  7.69917863e-09 8.25207469e-09 8.55467241e-09 8.62167793e-09  2.33702433e-06 1.54176013e-07 1.54176013e-07 8.59720994e-09  7.42201848e-07 1.35592885e-06 9.77435732e-09 9.77435732e-09  1.91960041e-06 3.26570809e-07 4.11204704e-07 1.00452685e-07  1.05150425e-08 1.12440724e-06 3.39028849e-07 4.02941241e-07  3.10817701e-05 3.29185001e-06 3.07697664e-05 1.25121970e-07  6.63953239e-08 2.53330077e-03 4.24011647e-06 9.99998067e-08  9.99998067e-08 1.23857016e-07 1.02761442e-07 5.84713353e-02  4.74592175e-02 1.26053028e-05 1.26053028e-05 1.23857016e-07  2.73793332e-06 9.80164150e-09 7.75660186e-02 1.30575619e-07  4.59694239e-02 4.59694239e-02 1.24032304e-05 4.59694239e-02  5.24027902e-02 1.30575619e-07 1.30575619e-07 5.24027902e-02  9.11239850e-09 5.24027902e-02 1.58466333e-07 4.59694239e-02  1.30575619e-07 1.04597350e-05 6.65720442e-02 5.24027902e-02  5.37640361e-06 2.21300765e-06 4.65276696e-02 5.24027902e-02  1.86326095e-05 6.84298901e-08 1.86326095e-05 1.86326095e-05  4.59694239e-02 1.63399065e-05 1.86326095e-05 8.09477714e-08  5.92608748e-02 8.09477714e-08 8.09477714e-08 6.84298901e-08  1.94605588e-02 3.07575456e-02 2.02954636e-02 2.02954636e-02  2.02954636e-02 2.28749668e-02 3.61878248e-03 2.58902058e-03  2.58902058e-03 2.35225857e-03 5.78676321e-04 5.16266949e-04  5.78676321e-04 6.13950909e-06 3.17349826e-04 8.51080534e-07  2.15205228e-03 2.58215504e-06 1.06545459e-06 2.08598613e-04  9.26327770e-09 9.51081582e-08 2.11638251e-08 4.54121996e-07  2.12352469e-08 2.47941976e-08 1.95006642e-08 1.17904978e-07  1.25162721e-08 2.37077117e-06 3.83354452e-06 2.24825814e-07  1.25857980e-08 6.64878314e-07 1.40140653e-06 1.45532895e-06  1.38147536e-07 9.81574493e-08 2.00304374e-07 1.37819820e-08  8.38701530e-09 3.17553720e-07 1.33016749e-08 3.17553720e-07  1.48563556e-08 2.70823296e-08 3.94867256e-08 3.31275155e-08  5.02038067e-08 3.03998018e-08 1.04184873e-08 5.21020123e-08  8.30852330e-08 8.31666203e-09 3.62558695e-08 9.24424726e-09  2.94505917e-08 3.71229630e-08 3.35893682e-08 1.86677489e-08  2.86545185e-08 4.19476687e-08 4.19476687e-08 4.19476687e-08  7.49728746e-09 1.44842853e-08 1.37215267e-08 1.07384306e-08  3.08292305e-08 7.34302839e-08 4.64974772e-08 8.62741434e-09  3.46599717e-08 4.64974772e-08 2.49323749e-08 7.49728746e-09  3.23670774e-08 7.65532548e-09 3.52397800e-08 6.32024237e-08  9.68198433e-09 8.83393891e-09 8.83393891e-09 3.46599717e-08  7.54065299e-09 3.46599717e-08 6.49189673e-08 8.83393891e-09  9.53293400e-09 2.39698416e-08 1.80489245e-08 7.45291329e-09  8.98235641e-09 8.15624146e-09 8.88278040e-09 7.45492612e-09  3.52029177e-08 8.66947303e-09 3.85237134e-08 5.85299860e-08  1.77752624e-08 3.52814989e-08 9.50864520e-09 8.98235641e-09  3.52814989e-08 5.27248724e-08 2.34112576e-08 3.85653312e-08  8.35897651e-09 8.35897651e-09 3.10870285e-08 9.50864520e-09  3.10870285e-08 8.66947303e-09 2.22525851e-08 3.71312863e-08  3.71312863e-08 7.49585505e-09 8.66714789e-09 3.07878297e-08  3.07878297e-08 3.49187574e-08 2.33273370e-08 8.15624146e-09  8.14460199e-09 8.14460199e-09 2.18897992e-08 2.36189536e-08  2.29131760e-08 2.03908631e-08 1.08495642e-08 2.04002493e-08  8.28861091e-09 3.16764461e-08 1.80735789e-08 9.32555699e-09  7.46850193e-09 7.53406826e-09 7.45299356e-09 1.29965121e-08  2.13225541e-08 7.46850193e-09 1.30041372e-08 2.40550460e-08  2.11257709e-08 7.46320772e-09 7.46850193e-09 1.44811875e-08  1.33454933e-08 7.46850193e-09 3.78714380e-08 3.26581555e-08  2.11451148e-08 8.00030475e-09 4.43934026e-08 2.49577152e-08  2.63517824e-08 2.03125987e-08 3.10894386e-08 2.69933466e-08  2.69933466e-08 3.10894386e-08 3.10894386e-08 2.13794114e-08  2.31339645e-08 2.13794114e-08 2.13794114e-08 2.10187687e-08  2.31339645e-08 2.13794114e-08 1.78675994e-08 1.93158107e-08  1.78675994e-08 2.13794114e-08 7.53473628e-09 2.13794114e-08  1.97346018e-08 2.13794114e-08 1.53628896e-08 1.97346018e-08  1.45441089e-08 3.53510268e-08]              In\u00a0[54]: Copied! <pre>df[\"MLOG10P\"] = -np.log10(df[\"P\"])\n</pre> df[\"MLOG10P\"] = -np.log10(df[\"P\"]) In\u00a0[55]: Copied! <pre>df[\"cs\"] = 0\nn_cs=len(susieR.susie_get_cs(fit, coverage = 0.95,min_abs_corr = 0.5,Xcorr = R_df)[0])\nfor i in range(n_cs):\n    cs_index = susieR.susie_get_cs(fit,coverage = 0.95,min_abs_corr = 0.5,Xcorr = R_df)[0][i]\n    df.loc[cs_index-1,\"cs\"] = i + 1\ndf[\"pip\"] = np.array(susieR.susie_get_pip(fit))\n</pre> df[\"cs\"] = 0 n_cs=len(susieR.susie_get_cs(fit, coverage = 0.95,min_abs_corr = 0.5,Xcorr = R_df)[0]) for i in range(n_cs):     cs_index = susieR.susie_get_cs(fit,coverage = 0.95,min_abs_corr = 0.5,Xcorr = R_df)[0][i]     df.loc[cs_index-1,\"cs\"] = i + 1 df[\"pip\"] = np.array(susieR.susie_get_pip(fit)) In\u00a0[68]: Copied! <pre>fig ,axes = plt.subplots(nrows=2,sharex=True,figsize=(15,7),height_ratios=(4,1))\n\ncol_to_plot = \"MLOG10P\"\np=axes[0].scatter(df[\"POS\"],df[col_to_plot],c=ld[df[\"P\"].idxmin()]**2)\n\naxes[0].scatter(df.loc[df[\"cs\"]==1,\"POS\"],df.loc[df[\"cs\"]==1,col_to_plot],\n           marker='o',s=40,c=\"None\",edgecolors='black',label=\"Variants in credible set 1\")\n\naxes[0].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),col_to_plot],\n           marker='x',s=40,c=\"red\",edgecolors='black',label=\"Causal\")\n\nplt.colorbar( p , label=\"Rsq with the lead variant\")\naxes[0].set_xlabel(\"position\")\naxes[0].set_xlim((55400000, 55800000))\naxes[0].set_ylabel(col_to_plot)\naxes[0].legend()\n\np=axes[1].scatter(df[\"POS\"],df[\"pip\"],c=ld[df[\"P\"].idxmin()]**2)\n\naxes[1].scatter(df.loc[df[\"cs\"]==1,\"POS\"],df.loc[df[\"cs\"]==1,\"pip\"],\n           marker='o',s=40,c=\"None\",edgecolors='black',label=\"Variants in credible set 1\")\n\naxes[1].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"pip\"],\n           marker='x',s=40,c=\"red\",edgecolors='black',label=\"Causal\")\n\nplt.colorbar( p , label=\"Rsq with the lead variant\")\naxes[1].set_xlabel(\"position\")\naxes[1].set_xlim((55400000, 55800000))\naxes[1].set_ylabel(\"PIP\")\naxes[1].legend()\n</pre> fig ,axes = plt.subplots(nrows=2,sharex=True,figsize=(15,7),height_ratios=(4,1))  col_to_plot = \"MLOG10P\" p=axes[0].scatter(df[\"POS\"],df[col_to_plot],c=ld[df[\"P\"].idxmin()]**2)  axes[0].scatter(df.loc[df[\"cs\"]==1,\"POS\"],df.loc[df[\"cs\"]==1,col_to_plot],            marker='o',s=40,c=\"None\",edgecolors='black',label=\"Variants in credible set 1\")  axes[0].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),col_to_plot],            marker='x',s=40,c=\"red\",edgecolors='black',label=\"Causal\")  plt.colorbar( p , label=\"Rsq with the lead variant\") axes[0].set_xlabel(\"position\") axes[0].set_xlim((55400000, 55800000)) axes[0].set_ylabel(col_to_plot) axes[0].legend()  p=axes[1].scatter(df[\"POS\"],df[\"pip\"],c=ld[df[\"P\"].idxmin()]**2)  axes[1].scatter(df.loc[df[\"cs\"]==1,\"POS\"],df.loc[df[\"cs\"]==1,\"pip\"],            marker='o',s=40,c=\"None\",edgecolors='black',label=\"Variants in credible set 1\")  axes[1].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"pip\"],            marker='x',s=40,c=\"red\",edgecolors='black',label=\"Causal\")  plt.colorbar( p , label=\"Rsq with the lead variant\") axes[1].set_xlabel(\"position\") axes[1].set_xlim((55400000, 55800000)) axes[1].set_ylabel(\"PIP\") axes[1].legend()   <pre>&lt;ipython-input-68-47ec57c63437&gt;:9: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  axes[0].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),col_to_plot],\n&lt;ipython-input-68-47ec57c63437&gt;:23: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  axes[1].scatter(df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"POS\"],df.loc[(df[\"CHR\"]==2)&amp;(df[\"POS\"]==55620927),\"pip\"],\n</pre> Out[68]: <pre>&lt;matplotlib.legend.Legend at 0x7fcb87870280&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"finemapping_susie/#finemapping-using-susier","title":"Finemapping using susieR\u00b6","text":""},{"location":"finemapping_susie/#data-preparation","title":"Data preparation\u00b6","text":""},{"location":"finemapping_susie/#finemapping","title":"Finemapping\u00b6","text":""},{"location":"plot_PCA/","title":"Plotting PCA","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns In\u00a0[2]: Copied! <pre>pca = pd.read_table(\"../05_PCA/plink_results_projected.sscore\",sep=\"\\t\")\npca\n</pre> pca = pd.read_table(\"../05_PCA/plink_results_projected.sscore\",sep=\"\\t\") pca  Out[2]: #FID IID ALLELE_CT NAMED_ALLELE_DOSAGE_SUM PC1_AVG PC2_AVG PC3_AVG PC4_AVG PC5_AVG PC6_AVG PC7_AVG PC8_AVG PC9_AVG PC10_AVG 0 0 HG00403 219504 219504 0.000644 -0.029750 -0.015150 -0.012238 0.022915 0.023541 -0.033705 -0.007513 -0.012540 0.002717 1 0 HG00404 219504 219504 -0.000492 -0.031018 -0.007642 -0.020500 0.028407 -0.008724 0.012335 -0.004921 -0.005570 0.024897 2 0 HG00406 219504 219504 0.006210 -0.034375 -0.008986 -0.003351 -0.021756 -0.018243 0.003339 -0.007606 -0.034002 0.006411 3 0 HG00407 219504 219504 0.006786 -0.023931 -0.007044 -0.004661 0.009854 0.000890 0.006796 -0.020050 -0.013187 0.035033 4 0 HG00409 219504 219504 -0.002363 -0.023160 0.032066 0.014556 0.023677 0.007048 0.012859 0.031961 -0.013063 0.011022 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 0 NA19087 219504 219504 -0.075155 0.030773 -0.012063 -0.020874 -0.009061 0.016212 -0.014142 -0.009302 -0.033492 -0.018796 500 0 NA19088 219504 219504 -0.079479 0.034983 -0.023635 -0.018341 -0.006770 -0.019187 -0.020756 -0.020069 -0.000052 0.010113 501 0 NA19089 219504 219504 -0.083754 0.037050 -0.021951 0.030121 -0.041074 -0.019957 0.002440 -0.010965 0.014069 0.021758 502 0 NA19090 219504 219504 -0.069719 0.028421 -0.014476 0.012602 -0.032481 -0.032686 0.023443 0.004380 0.006470 -0.018362 503 0 NA19091 219504 219504 -0.077385 0.038104 -0.017811 0.021642 -0.035877 -0.028448 -0.002518 -0.011106 0.003426 -0.003905 <p>504 rows \u00d7 14 columns</p> In\u00a0[3]: Copied! <pre>ped = pd.read_table(\"../01_Dataset/integrated_call_samples_v3.20130502.ALL.panel\",sep=\"\\t\")\nped\n</pre> ped = pd.read_table(\"../01_Dataset/integrated_call_samples_v3.20130502.ALL.panel\",sep=\"\\t\") ped  Out[3]: sample pop super_pop gender Unnamed: 4 Unnamed: 5 0 HG00096 GBR EUR male NaN NaN 1 HG00097 GBR EUR female NaN NaN 2 HG00099 GBR EUR female NaN NaN 3 HG00100 GBR EUR female NaN NaN 4 HG00101 GBR EUR male NaN NaN ... ... ... ... ... ... ... 2499 NA21137 GIH SAS female NaN NaN 2500 NA21141 GIH SAS female NaN NaN 2501 NA21142 GIH SAS female NaN NaN 2502 NA21143 GIH SAS female NaN NaN 2503 NA21144 GIH SAS female NaN NaN <p>2504 rows \u00d7 6 columns</p> In\u00a0[4]: Copied! <pre>pcaped=pd.merge(pca,ped,right_on=\"sample\",left_on=\"IID\",how=\"inner\")\npcaped\n</pre> pcaped=pd.merge(pca,ped,right_on=\"sample\",left_on=\"IID\",how=\"inner\") pcaped Out[4]: #FID IID ALLELE_CT NAMED_ALLELE_DOSAGE_SUM PC1_AVG PC2_AVG PC3_AVG PC4_AVG PC5_AVG PC6_AVG PC7_AVG PC8_AVG PC9_AVG PC10_AVG sample pop super_pop gender Unnamed: 4 Unnamed: 5 0 0 HG00403 219504 219504 0.000644 -0.029750 -0.015150 -0.012238 0.022915 0.023541 -0.033705 -0.007513 -0.012540 0.002717 HG00403 CHS EAS male NaN NaN 1 0 HG00404 219504 219504 -0.000492 -0.031018 -0.007642 -0.020500 0.028407 -0.008724 0.012335 -0.004921 -0.005570 0.024897 HG00404 CHS EAS female NaN NaN 2 0 HG00406 219504 219504 0.006210 -0.034375 -0.008986 -0.003351 -0.021756 -0.018243 0.003339 -0.007606 -0.034002 0.006411 HG00406 CHS EAS male NaN NaN 3 0 HG00407 219504 219504 0.006786 -0.023931 -0.007044 -0.004661 0.009854 0.000890 0.006796 -0.020050 -0.013187 0.035033 HG00407 CHS EAS female NaN NaN 4 0 HG00409 219504 219504 -0.002363 -0.023160 0.032066 0.014556 0.023677 0.007048 0.012859 0.031961 -0.013063 0.011022 HG00409 CHS EAS male NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 0 NA19087 219504 219504 -0.075155 0.030773 -0.012063 -0.020874 -0.009061 0.016212 -0.014142 -0.009302 -0.033492 -0.018796 NA19087 JPT EAS female NaN NaN 500 0 NA19088 219504 219504 -0.079479 0.034983 -0.023635 -0.018341 -0.006770 -0.019187 -0.020756 -0.020069 -0.000052 0.010113 NA19088 JPT EAS male NaN NaN 501 0 NA19089 219504 219504 -0.083754 0.037050 -0.021951 0.030121 -0.041074 -0.019957 0.002440 -0.010965 0.014069 0.021758 NA19089 JPT EAS male NaN NaN 502 0 NA19090 219504 219504 -0.069719 0.028421 -0.014476 0.012602 -0.032481 -0.032686 0.023443 0.004380 0.006470 -0.018362 NA19090 JPT EAS female NaN NaN 503 0 NA19091 219504 219504 -0.077385 0.038104 -0.017811 0.021642 -0.035877 -0.028448 -0.002518 -0.011106 0.003426 -0.003905 NA19091 JPT EAS male NaN NaN <p>504 rows \u00d7 20 columns</p> In\u00a0[5]: Copied! <pre>plt.figure(figsize=(10,10))\nsns.scatterplot(data=pcaped,x=\"PC1_AVG\",y=\"PC2_AVG\",hue=\"pop\",s=50)\n</pre> plt.figure(figsize=(10,10)) sns.scatterplot(data=pcaped,x=\"PC1_AVG\",y=\"PC2_AVG\",hue=\"pop\",s=50) Out[5]: <pre>&lt;AxesSubplot: xlabel='PC1_AVG', ylabel='PC2_AVG'&gt;</pre>"},{"location":"plot_PCA/#plotting-pca","title":"Plotting PCA\u00b6","text":""},{"location":"plot_PCA/#loading-files","title":"loading files\u00b6","text":""},{"location":"plot_PCA/#merge-pca-and-population-information","title":"Merge PCA and population information\u00b6","text":""},{"location":"plot_PCA/#plotting","title":"Plotting\u00b6","text":""},{"location":"prs_tutorial/","title":"PRS Tutorial","text":"In\u00a0[1]: Copied! <pre>import sys\nsys.path.insert(0,\"/Users/he/work/PRSlink/src\")\nimport prslink as pl\n</pre> import sys sys.path.insert(0,\"/Users/he/work/PRSlink/src\") import prslink as pl In\u00a0[2]: Copied! <pre>a= pl.PRS()\n</pre> a= pl.PRS() In\u00a0[3]: Copied! <pre>a.add_score(\"./1kgeas.0.1.profile\",  \"IID\",[\"SCORE\"],[\"0.1\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.05.profile\", \"IID\",[\"SCORE\"],[\"0.05\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.2.profile\",  \"IID\",[\"SCORE\"],[\"0.2\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.3.profile\",  \"IID\",[\"SCORE\"],[\"0.3\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.4.profile\",  \"IID\",[\"SCORE\"],[\"0.4\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.5.profile\",  \"IID\",[\"SCORE\"],[\"0.5\"],sep=\"\\s+\")\na.add_score(\"./1kgeas.0.001.profile\",\"IID\",[\"SCORE\"],[\"0.01\"],sep=\"\\s+\")\n</pre> a.add_score(\"./1kgeas.0.1.profile\",  \"IID\",[\"SCORE\"],[\"0.1\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.05.profile\", \"IID\",[\"SCORE\"],[\"0.05\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.2.profile\",  \"IID\",[\"SCORE\"],[\"0.2\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.3.profile\",  \"IID\",[\"SCORE\"],[\"0.3\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.4.profile\",  \"IID\",[\"SCORE\"],[\"0.4\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.5.profile\",  \"IID\",[\"SCORE\"],[\"0.5\"],sep=\"\\s+\") a.add_score(\"./1kgeas.0.001.profile\",\"IID\",[\"SCORE\"],[\"0.01\"],sep=\"\\s+\") <pre>- Dataset shape before loading : (0, 1)\n- Loading score data from file: ./1kgeas.0.1.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.1\n  - Overlapping IDs:0\n- Loading finished successfully!\n- Dataset shape after loading : (504, 2)\n- Dataset shape before loading : (504, 2)\n- Loading score data from file: ./1kgeas.0.05.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.05\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 3)\n- Dataset shape before loading : (504, 3)\n- Loading score data from file: ./1kgeas.0.2.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.2\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 4)\n- Dataset shape before loading : (504, 4)\n- Loading score data from file: ./1kgeas.0.3.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.3\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 5)\n- Dataset shape before loading : (504, 5)\n- Loading score data from file: ./1kgeas.0.4.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.4\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 6)\n- Dataset shape before loading : (504, 6)\n- Loading score data from file: ./1kgeas.0.5.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.5\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 7)\n- Dataset shape before loading : (504, 7)\n- Loading score data from file: ./1kgeas.0.001.profile\n  - Setting ID:IID\n  - Loading score:SCORE\n  - Loaded columns: 0.01\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 8)\n</pre> In\u00a0[4]: Copied! <pre>a.add_pheno(\"../01_Dataset/t2d/1kgeas_t2d.txt\",\"IID\",[\"T2D\"],types=\"B\",sep=\"\\s+\")\n</pre> a.add_pheno(\"../01_Dataset/t2d/1kgeas_t2d.txt\",\"IID\",[\"T2D\"],types=\"B\",sep=\"\\s+\") <pre>- Dataset shape before loading : (504, 8)\n- Loading pheno data from file: ../01_Dataset/t2d/1kgeas_t2d.txt\n  - Setting ID:IID\n  - Loading pheno:T2D\n  - Loaded columns: T2D\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 9)\n</pre> In\u00a0[5]: Copied! <pre>a.add_covar(\"./1kgeas.eigenvec\",\"IID\",[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],sep=\"\\s+\")\n</pre> a.add_covar(\"./1kgeas.eigenvec\",\"IID\",[\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\"],sep=\"\\s+\")  <pre>- Dataset shape before loading : (504, 9)\n- Loading covar data from file: ./1kgeas.eigenvec\n  - Setting ID:IID\n  - Loading covar:PC1 PC2 PC3 PC4 PC5\n  - Loaded columns: PC1 PC2 PC3 PC4 PC5\n  - Overlapping IDs:504\n- Loading finished successfully!\n- Dataset shape after loading : (504, 14)\n</pre> In\u00a0[6]: Copied! <pre>a.data[\"T2D\"] = a.data[\"T2D\"]-1\n</pre> a.data[\"T2D\"] = a.data[\"T2D\"]-1 In\u00a0[7]: Copied! <pre>a.data\n</pre> a.data Out[7]: IID 0.1 0.05 0.2 0.3 0.4 0.5 0.01 T2D PC1 PC2 PC3 PC4 PC5 0 HG00403 -0.000061 -2.812450e-05 -0.000019 -2.131690e-05 -0.000024 -0.000022 0.000073 0 0.000107 0.039080 0.021048 0.016633 0.063373 1 HG00404 0.000025 4.460810e-07 0.000041 4.370760e-05 0.000024 0.000018 0.000156 1 -0.001216 0.045148 0.009013 0.028122 0.041474 2 HG00406 0.000011 2.369040e-05 -0.000009 2.928090e-07 -0.000010 -0.000008 -0.000188 0 0.005020 0.044668 0.016583 0.020077 -0.031782 3 HG00407 -0.000133 -1.326670e-04 -0.000069 -5.677710e-05 -0.000062 -0.000057 -0.000744 1 0.005408 0.034132 0.014955 0.003872 0.009794 4 HG00409 0.000010 -3.120730e-07 -0.000012 -1.873660e-05 -0.000025 -0.000023 -0.000367 1 -0.002121 0.031752 -0.048352 -0.043185 0.064674 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 NA19087 -0.000042 -6.215880e-05 -0.000038 -1.116230e-05 -0.000019 -0.000018 -0.000397 0 -0.067583 -0.040340 0.015038 0.039039 -0.010774 500 NA19088 0.000085 9.058670e-05 0.000047 2.666260e-05 0.000016 0.000014 0.000723 0 -0.069752 -0.047710 0.028578 0.036714 -0.000906 501 NA19089 -0.000067 -4.767610e-05 -0.000011 -1.393760e-05 -0.000019 -0.000016 -0.000126 0 -0.073989 -0.046706 0.040089 -0.034719 -0.062692 502 NA19090 0.000064 3.989030e-05 0.000022 7.445850e-06 0.000010 0.000003 -0.000149 0 -0.061156 -0.034606 0.032674 -0.016363 -0.065390 503 NA19091 0.000051 4.469220e-05 0.000043 3.089720e-05 0.000019 0.000016 0.000028 0 -0.067749 -0.052950 0.036908 -0.023856 -0.058515 <p>504 rows \u00d7 14 columns</p> In\u00a0[13]: Copied! <pre>a.set_k({\"T2D\":0.2})\n</pre> a.set_k({\"T2D\":0.2}) In\u00a0[14]: Copied! <pre>a.evaluate(a.pheno_cols, a.score_cols, a.covar_cols,r2_lia=True)\n</pre> a.evaluate(a.pheno_cols, a.score_cols, a.covar_cols,r2_lia=True) <pre> - Binary trait: fitting logistic regression...\n - Binary trait: using records with phenotype being 0 or 1...\nOptimization terminated successfully.\n         Current function value: 0.668348\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.653338\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.657903\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.654492\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.654413\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.653085\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.654681\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.661290\n         Iterations 5\n</pre> Out[14]: PHENO TYPE PRS N_CASE N BETA CI_L CI_U P R2_null R2_full Delta_R2 AUC_null AUC_full Delta_AUC R2_lia_null R2_lia_full Delta_R2_lia SE 0 T2D B 0.01 200 502 0.250643 0.064512 0.436773 0.008308 0.010809 0.029616 0.018808 0.536921 0.586821 0.049901 0.010729 0.029826 0.019096 NaN 1 T2D B 0.05 200 502 0.310895 0.119814 0.501976 0.001428 0.010809 0.038545 0.027736 0.536921 0.601987 0.065066 0.010729 0.038925 0.028196 NaN 2 T2D B 0.5 200 502 0.367803 0.169184 0.566421 0.000284 0.010809 0.046985 0.036176 0.536921 0.605397 0.068477 0.010729 0.047553 0.036824 NaN 3 T2D B 0.2 200 502 0.365641 0.169678 0.561604 0.000255 0.010809 0.047479 0.036670 0.536921 0.607318 0.070397 0.010729 0.048079 0.037349 NaN 4 T2D B 0.3 200 502 0.367788 0.171062 0.564515 0.000248 0.010809 0.047686 0.036877 0.536921 0.608493 0.071573 0.010729 0.048315 0.037585 NaN 5 T2D B 0.1 200 502 0.374750 0.181520 0.567979 0.000144 0.010809 0.050488 0.039679 0.536921 0.613957 0.077036 0.010729 0.051270 0.040540 NaN 6 T2D B 0.4 200 502 0.389232 0.189866 0.588597 0.000130 0.010809 0.051145 0.040336 0.536921 0.609238 0.072318 0.010729 0.051845 0.041116 NaN In\u00a0[15]: Copied! <pre>a.plot_roc(a.pheno_cols, a.score_cols, a.covar_cols)\n</pre> a.plot_roc(a.pheno_cols, a.score_cols, a.covar_cols) <pre>Optimization terminated successfully.\n         Current function value: 0.668348\n         Iterations 5\n</pre> In\u00a0[16]: Copied! <pre>a.plot_prs(a.score_cols)\n</pre> a.plot_prs(a.score_cols) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}